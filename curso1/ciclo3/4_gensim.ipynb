{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso1/ciclo3/4_gensim.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbb4c409",
      "metadata": {
        "id": "cbb4c409"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1e7ctPi8O3bTQoLZaO9ZZjwGr2r8Z93RS\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e2f1bbd",
      "metadata": {
        "id": "3e2f1bbd"
      },
      "source": [
        "# Gensim y Embeddings\n",
        "---\n",
        "\n",
        "En este notebook presentaremos algunas generalidades de la librería `gensim` y la estrategia de extracción de características conocida como _embeddings_. Comenzamos instalando e importando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19a9d959",
      "metadata": {
        "id": "19a9d959"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode gensim==4.2.0 scipy==1.10.1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5a234e1",
      "metadata": {
        "id": "a5a234e1"
      },
      "source": [
        "> **NOTA**: Después de la instalación, es posible que Google Colaboratory le pida reiniciar el entorno de ejecución (*RUNTIME*). Puede hacerlo haciendo clic en el botón `RESTART RUNTIME` antes de continuar.\n",
        " <img src=\"https://drive.google.com/uc?export=view&id=1x9WLH8bLR5i6yV-NoOheOmlVX2C9l07F\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f13f8e53",
      "metadata": {
        "id": "f13f8e53"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from unidecode import unidecode\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "73b25810",
      "metadata": {
        "id": "73b25810"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"es_core_news_lg\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "424b2f63",
      "metadata": {
        "id": "424b2f63"
      },
      "source": [
        "## **1. Generalidades de Gensim**\n",
        "---\n",
        "\n",
        "`gensim` es una librería de código abierto para _Python_ que ofrece múltiples estrategias de _embedding_, modelos de tópicos y corpus típicos para pruebas con modelos de NLP.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1puqgEoMoHEJTyLKb4-KP6Hc9po3Whekv\" width=\"80%\">\n",
        "\n",
        "Comenzamos instalando `gensim`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17495720",
      "metadata": {
        "id": "17495720"
      },
      "outputs": [],
      "source": [
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2721b26d",
      "metadata": {
        "id": "2721b26d"
      },
      "source": [
        "`gensim` dispone de los siguientes módulos:\n",
        "\n",
        "- `utils`: módulo con funcionalidades generales de `gensim` como persistencia y carga de modelos.\n",
        "- `matutils`: módulo con funcionalidades matemáticas e interacción con librerías científicas como `numpy` o `scipy`.\n",
        "- `downloader`: permite la descarga de modelos preentrenados.\n",
        "- `corpora`: se usa para la carga y manipulación de distintos corpus.\n",
        "- `models`: contiene distintos modelos y tipos de embeddings.\n",
        "- `similarities`: contiene distintas medidas de similitud textual y semántica.\n",
        "- `topic_coherence`: módulo para el análisis de coherencia en modelos no supervisados.\n",
        "- `scripts`: contiene algunos algoritmos para usar `gensim` como un `cli`.\n",
        "- `parsing`: provee métodos para extracción de información a partir de textos.\n",
        "\n",
        "Normalmente, muchos de estos módulos no son requeridos para una aplicación típica de NLP, solamente llegan a ser necesarios cuando necesitamos hacer algo muy específico y personalizado. Los módulos con los que generalmente estaremos trabajando son `downloader`, `models`, `corpora` y `similarities`. Veamos el uso de `gensim` en aplicaciones relacionadas a _embeddings_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "447034a4",
      "metadata": {
        "id": "447034a4"
      },
      "source": [
        "## **2. Conjunto de Datos**\n",
        "---\n",
        "\n",
        "En este caso, trabajaremos como corpus el antiguo testamento de la Biblia, ya que es un conjunto de datos en español extenso con palabras muy típicas del lenguaje. Primero lo descargamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69aad43c",
      "metadata": {
        "id": "69aad43c"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u3/biblia.txt' -O 'biblia.txt'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13d1edc9",
      "metadata": {
        "id": "13d1edc9"
      },
      "source": [
        "Ahora, cargamos el dataset, especificamos como encoding `latin_1` para tomar caracteres especiales de lenguajes derivados del latín:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0122a657",
      "metadata": {
        "id": "0122a657"
      },
      "outputs": [],
      "source": [
        "with open(\"biblia.txt\", encoding=\"latin_1\") as f:\n",
        "    text = f.readlines()\n",
        "display(text[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cd448e7",
      "metadata": {
        "id": "5cd448e7"
      },
      "source": [
        "Definimos una función para preprocesar el texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dbfd2cc",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5dbfd2cc"
      },
      "outputs": [],
      "source": [
        "pat = re.compile(r\"[^a-z ]\")\n",
        "spaces = re.compile(r\"\\s{2,}\")\n",
        "def preprocess(text, min_len=1, max_len=23):\n",
        "    # Normalizamos el texto\n",
        "    norm_text = unidecode(text).lower()\n",
        "\n",
        "    # Extraemos tokens\n",
        "    tokens = norm_text.split()\n",
        "\n",
        "    # Filtramos palabras por longitud\n",
        "    filtered_tokens = filter(\n",
        "            lambda token: (\n",
        "                len(token) >= min_len and\n",
        "                len(token) <= max_len\n",
        "                ),\n",
        "            tokens\n",
        "        )\n",
        "    filtered_text = \" \".join(filtered_tokens)\n",
        "    # Eliminamos caracteres especiales\n",
        "    clean_text = re.sub(pat, \"\", filtered_text)\n",
        "    # Eliminamos espacios duplicados\n",
        "    spaces_text = re.sub(spaces, \" \", clean_text)\n",
        "    return spaces_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f5ab685",
      "metadata": {
        "id": "5f5ab685"
      },
      "source": [
        "Limpiamos el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b55d88e",
      "metadata": {
        "id": "9b55d88e"
      },
      "outputs": [],
      "source": [
        "corpus = map(preprocess, text)\n",
        "corpus = list(filter(lambda doc: len(doc), corpus))\n",
        "display(corpus[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af8285aa",
      "metadata": {
        "id": "af8285aa"
      },
      "source": [
        "## **3. Embeddings**\n",
        "---\n",
        "\n",
        "Un embedding es una representación vectorial de un elemento en un espacio dimensional reducido. Por lo general, se utilizan embeddings en el campo del aprendizaje automático y en el procesamiento del lenguaje natural. Los embeddings se pueden entrenar para capturar las relaciones entre los elementos de un conjunto de datos, lo que puede ser útil en aplicaciones como la clasificación de texto o la recomendación de productos. En general, un embedding es una forma de representar un elemento de un conjunto de datos en un espacio vectorial de menor dimensión, lo que permite que los algoritmos de aprendizaje automático puedan manejar mejor esos datos.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1m9pQizY35Tonc86nG5-Tmhcx8gKWj8yX\" width=\"80%\">\n",
        "\n",
        "Existen distintos tipos de _embedding_, por ejemplo, `spacy` calcula un atributo `vector` para cada `Token` en sus documentos, veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e8d79ce",
      "metadata": {
        "id": "8e8d79ce"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_lg\")\n",
        "doc = nlp(corpus[10])\n",
        "display(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "645f9238",
      "metadata": {
        "id": "645f9238"
      },
      "source": [
        "Veamos la representación vectorial del `Token` número 5:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52765724",
      "metadata": {
        "id": "52765724"
      },
      "outputs": [],
      "source": [
        "display(doc[5].vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "17c592ad",
      "metadata": {
        "id": "17c592ad"
      },
      "source": [
        "No obstante, esta representación es general (codifica información del conjunto de noticias en español donde el pipeline fue entrenado), con `gensim` podemos entrenar nuestros propios _embeddings_ usando los modelos más típicos de representación:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d8381f0",
      "metadata": {
        "id": "8d8381f0"
      },
      "source": [
        "### **3.1. Word2Vec**\n",
        "---\n",
        "\n",
        "Se trata de un método creado por Google en 2013 que está basado en redes neuronales y busca transformar las palabras en vectores numéricos dentro de un espacio vectorial que capture información _contextual_ y _semántica_.\n",
        "\n",
        "El enfoque de _Word2Vec_ busca que una palabra guarde información de su contexto, es decir, una palabra viene a representar información de las palabras que la rodean (vecindario). Esto se puede ver desde dos enfoques como se muestra en la siguiente figura:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1jIfs_cQKSytMoOsim_sGHX7xstJFPzvT\" width=\"70%\">\n",
        "\n",
        "* **Continuous Bag-of-words (CBOW)**: se trata de un modelo que toma como entrada el contexto de una palabra (vecindario) y trata de predecir la palabra en cuestión.\n",
        "* **Skip-gram**: se trata de un modelo que toma como entrada una palabra y trata de predecir el contexto (vecindario).\n",
        "\n",
        "Por ejemplo, de la siguiente frase:\n",
        "\n",
        "> `\"el procesamiento de lenguaje natural en Python\"`\n",
        "\n",
        "Siguiendo el modelo de _Skip-Gram_, tendríamos lo siguiente para codificar la palabra `\"lenguaje\"`.\n",
        "\n",
        "* **Entradas**: `\"lenguaje\"`\n",
        "* **Salidas**: `[\"el\", \"procesamiento\", \"de\", \"natural\", \"en\", \"Python\"]`\n",
        "\n",
        "El proceso general que describe un modelo de _Word2Vec_ es el siguiente:\n",
        "\n",
        "1. Construcción del vocabulario a nivel de palabra.\n",
        "2. Extracción de secuencias de un tamaño dado (contexto).\n",
        "3. Codificación de cada palabra como un vector one-hot o variables _dummy_.\n",
        "4. Paso de las codificaciones por la arquitectura de la red neuronal.\n",
        "\n",
        "Desde `gensim` podemos utilizar este modelo con la clase `Word2Vec`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "61606982",
      "metadata": {
        "id": "61606982"
      },
      "outputs": [],
      "source": [
        "from gensim.models.word2vec import Word2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcf40584",
      "metadata": {
        "id": "dcf40584"
      },
      "source": [
        "Dentro de los parámetros más relevantes tenemos:\n",
        "\n",
        "- `sentences`: corpus tokenizado.\n",
        "- `vector_size`: tamaño de los vectores que se aprenderán por palabra.\n",
        "- `window`: tamaño del contexto.\n",
        "- `min_count`: ignora palabras que tengan una frecuencia de documento menor a este valor.\n",
        "- `workers`: específica cuántos procesos se usan para el entrenamiento del modelo (se realiza de forma distribuida).\n",
        "- `sg`: 1 para modelo de tipo _Skip-Gram_, 0 para _CBOW_.\n",
        "- `alpha`: taza de aprendizaje del modelo (hiper-parámetro).\n",
        "- `min_alpha`: taza de aprendizaje mínima del modelo.\n",
        "- `seed`: semilla de números aleatorios para reproducibilidad del modelo.\n",
        "- `max_vocab_size`: limita el número máximo de palabras a codificar.\n",
        "- `epochs`: número de iteraciones para el entrenamiento del modelo.\n",
        "\n",
        "Para entrenar el modelo, necesitamos el corpus tokenizado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e11ffce7",
      "metadata": {
        "id": "e11ffce7"
      },
      "outputs": [],
      "source": [
        "tokens = list(map(lambda doc: doc.split(), corpus))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f12601bf",
      "metadata": {
        "id": "f12601bf"
      },
      "source": [
        "Ahora, entrenamos el modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3171a2ca",
      "metadata": {
        "id": "3171a2ca"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec(\n",
        "        sentences = tokens,\n",
        "        vector_size = 100,\n",
        "        epochs = 20,\n",
        "        workers = -1 # específica que se debe usar el número máximo de procesos.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45999fd2",
      "metadata": {
        "id": "45999fd2"
      },
      "source": [
        "Veamos cómo podemos extraer el vector de una palabra en específico con el atributo `wv`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0988380b",
      "metadata": {
        "id": "0988380b"
      },
      "outputs": [],
      "source": [
        "vect = model.wv[\"tierra\"]\n",
        "display(vect)\n",
        "display(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "61a39f1f",
      "metadata": {
        "id": "61a39f1f"
      },
      "source": [
        "Como podemos ver, la palabra `\"tierra\"` se codifica como un vector de tamaño `100`. También podemos extraer una representación vectorial de todo un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a1bc44c",
      "metadata": {
        "id": "0a1bc44c"
      },
      "outputs": [],
      "source": [
        "display(tokens[10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d9f1ee7",
      "metadata": {
        "id": "1d9f1ee7"
      },
      "source": [
        "Veamos las representaciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18421c51",
      "metadata": {
        "id": "18421c51"
      },
      "outputs": [],
      "source": [
        "vects = model.wv[tokens[10]]\n",
        "display(vects)\n",
        "display(vects.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bd4ea771",
      "metadata": {
        "id": "bd4ea771"
      },
      "source": [
        "Como puede ver, obtuvimos `12` vectores de tamaño `100`, correspondientes a las 12 palabras del documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0a69811",
      "metadata": {
        "id": "f0a69811"
      },
      "outputs": [],
      "source": [
        "display(len(tokens[10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3842e3f3",
      "metadata": {
        "id": "3842e3f3"
      },
      "source": [
        "El atributo `wv` de un modelo de `gensim` contiene vectores anotados `KeyedVectors`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3b49f082",
      "metadata": {
        "id": "3b49f082"
      },
      "outputs": [],
      "source": [
        "display(type(model.wv))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96859506",
      "metadata": {
        "id": "96859506"
      },
      "source": [
        "Estos vectores anotados son el resultado final del modelo (un vector por cada palabra) y en muchas oportunidades es lo que necesitamos para extraer características de un texto. No obstante, los `KeyedVectors` no almacenan información de los estados internos del modelo ni de su arquitectura (_Skip-Gram_ o _CBOW_).\n",
        "\n",
        "Con esto, podemos ver las dos formas de exportar modelos de `gensim`:\n",
        "\n",
        "- **Modelo completo**: podemos almacenar un modelo completo con el método `save`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ad8fc64",
      "metadata": {
        "id": "4ad8fc64"
      },
      "outputs": [],
      "source": [
        "model.save(\"model.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55a461a9",
      "metadata": {
        "id": "55a461a9"
      },
      "source": [
        "Para cargarlo, podemos usar el método `load` de `Word2Vec`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f8d3a94",
      "metadata": {
        "id": "3f8d3a94"
      },
      "outputs": [],
      "source": [
        "model = Word2Vec.load(\"model.bin\")\n",
        "display(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eebc1288",
      "metadata": {
        "id": "eebc1288"
      },
      "source": [
        "De esta forma, almacenamos el modelo con sus estados y parámetros.\n",
        "\n",
        "- **Vectores**: podemos almacenar únicamente los vectores del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d89915e",
      "metadata": {
        "id": "1d89915e"
      },
      "outputs": [],
      "source": [
        "model.wv.save(\"model.vec\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91b5c2a1",
      "metadata": {
        "id": "91b5c2a1"
      },
      "source": [
        "Para cargarlo, usamos la utilidad de `KeyedVectors` del modelo correspondiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96df7b1c",
      "metadata": {
        "id": "96df7b1c"
      },
      "outputs": [],
      "source": [
        "from gensim.models.word2vec import KeyedVectors\n",
        "model = KeyedVectors.load(\"model.vec\")\n",
        "display(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d93c9560",
      "metadata": {
        "id": "d93c9560"
      },
      "source": [
        "Guardar un modelo completo da más flexibilidad, no obstante, ocupa mucho más espacio en disco y en memoria a diferencia de guardar únicamente los vectores. Veamos una comparación con `os`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ceb9239b",
      "metadata": {
        "id": "ceb9239b"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bdc69da",
      "metadata": {
        "id": "1bdc69da"
      },
      "source": [
        "Veamos el tamaño del modelo completo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec69becc",
      "metadata": {
        "id": "ec69becc"
      },
      "outputs": [],
      "source": [
        "display(f\"{os.stat('model.bin').st_size / 1024 ** 2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90174bff",
      "metadata": {
        "id": "90174bff"
      },
      "source": [
        "Veamos el tamaño de los vectores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fb21992",
      "metadata": {
        "id": "3fb21992"
      },
      "outputs": [],
      "source": [
        "display(f\"{os.stat('model.vec').st_size / 1024 ** 2:.2f} MB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "80b2c05f",
      "metadata": {
        "id": "80b2c05f"
      },
      "source": [
        "### **3.2. FastText**\n",
        "---\n",
        "\n",
        "El modelo _FastText_ es una versión mejorada del modelo _Word2Vec_. Este modelo fue propuesto por Facebook en el año 2015.\n",
        "\n",
        "La intención de _FastText_ es representar palabras que no están dentro del vocabulario, veamos lo que ocurre con el modelo que teníamos entrenado anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3dfa5f19",
      "metadata": {
        "id": "3dfa5f19"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    vect = model.wv[\"pepe\"]\n",
        "    display(vect)\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f33e63",
      "metadata": {
        "id": "b6f33e63"
      },
      "source": [
        "Como podemos ver, es una palabra que no se encuentra en el vocabulario y por ello no tiene un vector asociado.\n",
        "\n",
        "_FastText_ soluciona este problema al manipular secuencias de N-Grams a nivel de caracter en lugar de secuencias de N-Grams a nivel de palabra. De esta forma, la mayoría de texto bien escrito en un idioma se puede codificar e incluso extrapolar para obtener una representación vectorial. Este modelo a nivel de arquitectura es idéntico al modelo de _Word2Vec_, con la diferencia que el contexto y la codificación se realiza a nivel N-gram como se muestra a continuación:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Fjo1lnpnreXLF0tEY5c6SlsmEH2RvsPb\" width=\"100%\">\n",
        "\n",
        "Desde `gensim` podemos usar esta estrategia de representación con la clase `FastText`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e6fd37",
      "metadata": {
        "id": "43e6fd37"
      },
      "outputs": [],
      "source": [
        "from gensim.models.fasttext import FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da01a8e",
      "metadata": {
        "id": "6da01a8e"
      },
      "source": [
        "El modelo es prácticamente idéntico a _Word2Vec_ con la única diferencia que podemos especificar características de los N-grams:\n",
        "\n",
        "- `min_n`: longitud mínima de _N-Grams_ a considerar.\n",
        "- `max_n`: longitud máxima de _N-Grams_ a considerar.\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b47a17c7",
      "metadata": {
        "id": "b47a17c7"
      },
      "outputs": [],
      "source": [
        "model = FastText(\n",
        "        sentences = tokens,\n",
        "        vector_size = 100,\n",
        "        epochs = 20,\n",
        "        workers = -1, # específica que se debe usar el número máximo de procesos.\n",
        "        min_n = 2,\n",
        "        max_n = 4\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f173c1f",
      "metadata": {
        "id": "7f173c1f"
      },
      "source": [
        "Veamos cómo codificar una palabra:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "811b1be9",
      "metadata": {
        "id": "811b1be9"
      },
      "outputs": [],
      "source": [
        "vect = model.wv[\"tierra\"]\n",
        "display(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39f5cff0",
      "metadata": {
        "id": "39f5cff0"
      },
      "source": [
        "También podemos codificar palabras fuera del vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d65ddde",
      "metadata": {
        "id": "2d65ddde"
      },
      "outputs": [],
      "source": [
        "vect = model.wv[\"pepe\"]\n",
        "display(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6bbbb928",
      "metadata": {
        "id": "6bbbb928"
      },
      "source": [
        "### **3.3. Doc2Vec**\n",
        "---\n",
        "\n",
        "Como pudimos verlo hasta este punto, los modelos _Word2Vec_ y _FastText_ se enfocan en codificar texto a nivel palabra. No obstante, en muchas oportunidades necesitamos codificar todo un documento como un vector.\n",
        "\n",
        "El modelo _Doc2Vec_ presenta una versión modificada de _Word2Vec_ que aplica para documentos completos. Esto se consigue al codificar un identificador del documento en la generación del modelo, como se muestra en la siguiente figura:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1yLRIZBzgJW9ls2Azc25YIdPhVtVtDOCx\" width=\"80%\">\n",
        "\n",
        "En `gensim` podemos usar la clase `Doc2Vec` para definir este modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9af9980",
      "metadata": {
        "id": "e9af9980"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import Doc2Vec"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22301f76",
      "metadata": {
        "id": "22301f76"
      },
      "source": [
        "También es necesario crear un `TaggedDocument` (documento con su `id`):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94c82686",
      "metadata": {
        "id": "94c82686"
      },
      "outputs": [],
      "source": [
        "from gensim.models.doc2vec import TaggedDocument"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caa28b7f",
      "metadata": {
        "id": "caa28b7f"
      },
      "source": [
        "Primero, creamos una lista con todos los documentos etiquetados al asignarles su `id` de documento como `tags`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "15c38b0a",
      "metadata": {
        "id": "15c38b0a"
      },
      "outputs": [],
      "source": [
        "tagged_corpus = [\n",
        "        TaggedDocument(doc, [i])\n",
        "        for i, doc in enumerate(tokens)\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b959524",
      "metadata": {
        "id": "6b959524"
      },
      "source": [
        "Veamos un ejemplo de documento con etiqueta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2842d286",
      "metadata": {
        "id": "2842d286"
      },
      "outputs": [],
      "source": [
        "display(tagged_corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58450840",
      "metadata": {
        "id": "58450840"
      },
      "source": [
        "El entrenamiento de _Doc2Vec_ es idéntico al de _Word2Vec_, la única diferencia es que deben entrar documentos etiquetados con el parámetro `documents` en lugar de `sentences`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74d15091",
      "metadata": {
        "id": "74d15091"
      },
      "outputs": [],
      "source": [
        "model = Doc2Vec(\n",
        "        documents = tagged_corpus,\n",
        "        vector_size = 100,\n",
        "        epochs = 20,\n",
        "        workers = -1 # específica que se debe usar el número máximo de procesos.\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab21a50d",
      "metadata": {
        "id": "ab21a50d"
      },
      "source": [
        "Veamos cómo podemos codificar un documento del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1eab7706",
      "metadata": {
        "id": "1eab7706"
      },
      "outputs": [],
      "source": [
        "display(tagged_corpus[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cdd286b",
      "metadata": {
        "id": "3cdd286b"
      },
      "source": [
        "Veamos el vector resultante, en este caso usamos el método `infer_vector` y extraemos las palabras con el atributo `words` a partir de un `TaggedDocument`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5976047d",
      "metadata": {
        "id": "5976047d"
      },
      "outputs": [],
      "source": [
        "vect = model.infer_vector(tagged_corpus[0].words)\n",
        "display(vect)\n",
        "display(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47c4b884",
      "metadata": {
        "id": "47c4b884"
      },
      "source": [
        "Como se puede ver, obtuvimos una codificación única para todo un documento y no por palabra, lo cual muestra la utilidad de _Doc2Vec_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d02fa86",
      "metadata": {
        "id": "9d02fa86"
      },
      "source": [
        "## **4. Modelos Pre-Entrenados**\n",
        "---\n",
        "\n",
        "Una de las principales desventajas de este tipo de modelos basados en _embeddings_ es que requieren mucho tiempo de entrenamiento y un corpus muy grande para llegar a resultados óptimos. Es por esto que muchas veces se suelen utilizar modelos pre-entrenados y posteriormente reajustarlos para nuestro corpus.\n",
        "\n",
        "Desde `gensim` disponemos de algunos modelos pre-entrenados sobre corpus masivos, podemos listar los modelos disponibles con información extraída del `api`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ad64457",
      "metadata": {
        "id": "0ad64457"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a4d43bd",
      "metadata": {
        "id": "4a4d43bd"
      },
      "source": [
        "Veamos un listado de los modelos disponibles:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c481877a",
      "metadata": {
        "id": "c481877a"
      },
      "outputs": [],
      "source": [
        "models = api.info()[\"models\"]\n",
        "display(models)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7adf566",
      "metadata": {
        "id": "c7adf566"
      },
      "source": [
        "Como se puede ver, tenemos distintos metadatos relacionados a los modelos pre-entrenados como:\n",
        "\n",
        "- Estrategia de preprocesamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8b06162",
      "metadata": {
        "id": "e8b06162"
      },
      "outputs": [],
      "source": [
        "model_meta = models[\"glove-twitter-25\"]\n",
        "display(model_meta[\"preprocessing\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3385121d",
      "metadata": {
        "id": "3385121d"
      },
      "source": [
        "- Tamaño del corpus de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627754f7",
      "metadata": {
        "id": "627754f7"
      },
      "outputs": [],
      "source": [
        "display(model_meta[\"num_records\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6cfb782d",
      "metadata": {
        "id": "6cfb782d"
      },
      "source": [
        "- Corpus de entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "347f7d74",
      "metadata": {
        "id": "347f7d74"
      },
      "outputs": [],
      "source": [
        "display(model_meta[\"base_dataset\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7fa9cb74",
      "metadata": {
        "id": "7fa9cb74"
      },
      "source": [
        "- Tamaño del _embedding_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f360f86",
      "metadata": {
        "id": "0f360f86"
      },
      "outputs": [],
      "source": [
        "display(model_meta[\"parameters\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a96ff6b",
      "metadata": {
        "id": "3a96ff6b"
      },
      "source": [
        "- Descripción del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dce9fbe1",
      "metadata": {
        "id": "dce9fbe1"
      },
      "outputs": [],
      "source": [
        "display(model_meta[\"description\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b1a57c5",
      "metadata": {
        "id": "9b1a57c5"
      },
      "source": [
        "Veamos cómo podemos cargar un modelo pre-entrenado. Vamos a seleccionar uno de la siguiente lista:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ea88a0c",
      "metadata": {
        "id": "1ea88a0c"
      },
      "outputs": [],
      "source": [
        "display(models.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "00a94128",
      "metadata": {
        "id": "00a94128"
      },
      "source": [
        "Descargamos y cargamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "102de735",
      "metadata": {
        "id": "102de735"
      },
      "outputs": [],
      "source": [
        "model = api.load(\"glove-twitter-25\")\n",
        "display(type(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa21e3bb",
      "metadata": {
        "id": "aa21e3bb"
      },
      "source": [
        "Esto nos permite cargar un vocabulario y los vectores asociados a cada palabra como `KeyedVectors`. Por ejemplo, podemos indexar una palabra en específico del vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38e571cc",
      "metadata": {
        "id": "38e571cc"
      },
      "outputs": [],
      "source": [
        "vect = model[\"father\"]\n",
        "display(vect)\n",
        "display(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fad5667d",
      "metadata": {
        "id": "fad5667d"
      },
      "source": [
        "Normalmente, los modelos pre-entrenados de `gensim` proveen únicamente los vectores anotados. No obstante, podemos encontrar otros modelos publicados por terceros. Por ejemplo, podemos usar un [modelo _FastText_ pre-entrenado de Facebook sobre el conjunto de datos de _WikiNews_ en Español](https://fasttext.cc/docs/en/crawl-vectors.html). Primero lo descargamos:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown\n",
        "!gdown https://drive.google.com/uc?id=1eCrRIJFR-QdX1wRL_bhg2xifMJPk0VKu"
      ],
      "metadata": {
        "id": "VqVvNOOWh1HB"
      },
      "id": "VqVvNOOWh1HB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b7926cfd",
      "metadata": {
        "id": "b7926cfd"
      },
      "source": [
        "Ahora lo cargamos con `gensim`:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.fasttext import load_facebook_model\n",
        "model = load_facebook_model(\"cc.es.300.bin\")"
      ],
      "metadata": {
        "id": "trRbC-rlds5f"
      },
      "id": "trRbC-rlds5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "b150fb6d",
      "metadata": {
        "id": "b150fb6d"
      },
      "source": [
        "Como podemos ver, cargamos un modelo completo que ya fue entrenado sobre un corpus grande:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "552366e6",
      "metadata": {
        "id": "552366e6"
      },
      "outputs": [],
      "source": [
        "display(type(model))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04b9ca1d",
      "metadata": {
        "id": "04b9ca1d"
      },
      "source": [
        "Podemos extraer vectores de este modelo pre-entrenado al igual que en los casos anteriores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f652f320",
      "metadata": {
        "id": "f652f320"
      },
      "outputs": [],
      "source": [
        "vect = model.wv[\"hola\"]\n",
        "display(vect)\n",
        "display(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bc53019",
      "metadata": {
        "id": "9bc53019"
      },
      "source": [
        "## **5. Similitudes**\n",
        "---\n",
        "\n",
        "Una de las aplicaciones más interesantes que tienen los _embeddings_ es que nos permiten comparar semánticamente dos textos por medio de una medida de similitud.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19lvjtDV29-RvY50U9GMRM42wyOqQf0Cb\" width=\"80%\">\n",
        "\n",
        "Por ejemplo, podemos codificar las siguientes cuatro palabras y ver sus similitudes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74e0d689",
      "metadata": {
        "id": "74e0d689"
      },
      "outputs": [],
      "source": [
        "words = [\"religión\", \"iglesia\", \"deporte\", \"futbol\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f156bc90",
      "metadata": {
        "id": "f156bc90"
      },
      "source": [
        "Extraemos su representación numérica:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc0467f9",
      "metadata": {
        "id": "dc0467f9"
      },
      "outputs": [],
      "source": [
        "vects = model.wv[words]\n",
        "display(vects.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc389c5a",
      "metadata": {
        "id": "bc389c5a"
      },
      "source": [
        "Podemos usar `sklearn` para calcular la similitud entre estas palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290df8c9",
      "metadata": {
        "id": "290df8c9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f7292421",
      "metadata": {
        "id": "f7292421"
      },
      "source": [
        "Calculamos la similitud:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "590bbcc1",
      "metadata": {
        "id": "590bbcc1"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(vects)\n",
        "display(sim.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34658f31",
      "metadata": {
        "id": "34658f31"
      },
      "source": [
        "El resultado es una matriz de `(4, 4)` con el valor de la similitud coseno entre cada combinación de palabra, veamos las similitudes de forma gráfica con un mapa de calor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7168c53d",
      "metadata": {
        "id": "7168c53d"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "fig, ax = plt.subplots()\n",
        "sns.heatmap(pd.DataFrame(sim, index=words, columns=words), annot=True)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fe1a1a1",
      "metadata": {
        "id": "4fe1a1a1"
      },
      "source": [
        "Como podemos ver hay una similitud alta entre `\"religion\"` e `\"iglesia\"` y también entre `\"deporte\"` y `\"futbol\"`, con esto podemos ver que el modelo está capturando relaciones semánticas entre las palabras.\n",
        "\n",
        "Adicional a esto, `gensim` nos permite recuperar las palabras del vocabulario más parecidas a una palabra dada con el método `most_similar` de los `KeyedVectors`, veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "065717d9",
      "metadata": {
        "id": "065717d9"
      },
      "outputs": [],
      "source": [
        "word = \"iglesia\"\n",
        "similar_words = model.wv.most_similar(word, topn=20)\n",
        "display(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7a3c5ae",
      "metadata": {
        "id": "d7a3c5ae"
      },
      "source": [
        "Como podemos ver, el resultado nos muestra palabras que parecidas a nivel textual y semántico. En este caso `gensim` nos da un listado con las 20 palabras más parecidas a `\"iglesia\"`.\n",
        "\n",
        "Con el método `most_similar` también podemos aplicar relaciones semánticas de las palabras, veamos el siguiente ejemplo:\n",
        "\n",
        "> `reina = rey - hombre + mujer`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "722ba953",
      "metadata": {
        "id": "722ba953"
      },
      "outputs": [],
      "source": [
        "similar_words = model.wv.most_similar(\n",
        "        positive=[\"rey\", \"mujer\"], negative=[\"hombre\"]\n",
        "        )\n",
        "display(similar_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e327f833",
      "metadata": {
        "id": "e327f833"
      },
      "source": [
        "## **6. Visualización**\n",
        "---\n",
        "\n",
        "Finalmente, podemos usar herramientas de visualización para mostrar relaciones semánticas entre distintas palabras, para ello, vamos a tomar 5 conceptos y a encontrar sus 10 palabras más parecidas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f89bbb7",
      "metadata": {
        "id": "6f89bbb7"
      },
      "outputs": [],
      "source": [
        "words = [\"religión\", \"deporte\", \"política\", \"economía\", \"farándula\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "91ea4de2",
      "metadata": {
        "id": "91ea4de2"
      },
      "source": [
        "Extraemos un listado con las 10 palabras más parecidas por cada categoría:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8dd2c80c",
      "metadata": {
        "id": "8dd2c80c"
      },
      "outputs": [],
      "source": [
        "all_words = []\n",
        "for word in words:\n",
        "    most_similar = model.wv.most_similar(word, topn=10)\n",
        "    all_words.extend(map(lambda case: case[0], most_similar))\n",
        "display(all_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "060c4a7c",
      "metadata": {
        "id": "060c4a7c"
      },
      "source": [
        "Extraemos los vectores de estas palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e46f3834",
      "metadata": {
        "id": "e46f3834"
      },
      "outputs": [],
      "source": [
        "vects = model.wv[all_words]\n",
        "display(vects.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a784e7af",
      "metadata": {
        "id": "a784e7af"
      },
      "source": [
        "En este caso tenemos 50 vectores de dimensión 300 (lo cual no es fácil de visualizar). Un enfoque típico para poder visualizar estos datos es utilizar una estrategia de reducción de dimensionalidad para mostrar los vectores proyectados en un plano cartesiano. En este caso, usaremos el modelo _Principal Components Analysis_ (PCA) de `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b60f50c1",
      "metadata": {
        "id": "b60f50c1"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af37c7a",
      "metadata": {
        "id": "3af37c7a"
      },
      "source": [
        "Obtenemos los vectores proyectados como 2 dimensiones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2961408",
      "metadata": {
        "id": "c2961408"
      },
      "outputs": [],
      "source": [
        "X = PCA(n_components=2).fit_transform(vects)\n",
        "display(X.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec0c08b4",
      "metadata": {
        "id": "ec0c08b4"
      },
      "source": [
        "Finalmente, con estos puntos podemos ver qué tan cerca se encuentran los conceptos que encontramos con una nube de puntos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0541dd5",
      "metadata": {
        "id": "c0541dd5"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "\n",
        "for word, x, y in zip(all_words, X[:, 0], X[:, 1]):\n",
        "    ax.annotate(\n",
        "            word, xy = (x + 0.1, y + 0.1),\n",
        "            xytext = (0, 0), textcoords = \"offset points\"\n",
        "            )\n",
        "ax.set_xlabel(\"$PC_1$\")\n",
        "ax.set_ylabel(\"$PC_2$\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39b3059d",
      "metadata": {
        "id": "39b3059d"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [Gensim: topic modeling for humans](https://radimrehurek.com/gensim/).\n",
        "- [FastText: library for efficient text classification and representation learning](https://fasttext.cc/)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e48be97",
      "metadata": {
        "id": "3e48be97"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "V28",
      "include_colab_link": true
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}