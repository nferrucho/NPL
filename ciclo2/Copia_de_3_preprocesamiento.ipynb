{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/ciclo2/Copia_de_3_preprocesamiento.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4848a6d6",
      "metadata": {
        "id": "4848a6d6"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1AQr9H9bXDeNPchTRufU78g8z0yxHvrmC\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b81c42d",
      "metadata": {
        "id": "4b81c42d"
      },
      "source": [
        "# Preprocesamiento\n",
        "---\n",
        "\n",
        "En este _notebook_ mostraremos un ejemplo práctico de preprocesamiento de textos usando un _corpus_ en español de _Kaggle_ llamado [Spanish tweets suggesting depression](https://www.kaggle.com/datasets/francescoronzano/spanish-tweets-suggesting-depression).\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Vh514lihPERVhRK4pxuwu157IGSvFM7k\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a5c087",
      "metadata": {
        "id": "d3a5c087"
      },
      "source": [
        "## **1. Carga de Datos**\n",
        "\n",
        "En este caso disponemos de un conjunto de datos tabular (formato `csv`) que contiene 4 columnas:\n",
        "\n",
        "- `id`: identificador único de cada tweet.\n",
        "- `user_id`: identificador único de un usuario.\n",
        "- `text`: texto del tweet.\n",
        "- `date`: fecha de creación del tweet.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mtV9eevIWZcJHq9aWRBHkn5kIr8pHBvf\" width=\"60%\">\n",
        "\n",
        "Vamos a cargar el conjunto de datos con `pandas`. Primero importamos la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dfa24d94",
      "metadata": {
        "id": "dfa24d94"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5970804",
      "metadata": {
        "id": "a5970804"
      },
      "source": [
        "Ahora, cargamos el conjunto de datos en la variable `df`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb3a64e3",
      "metadata": {
        "id": "bb3a64e3"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\n",
        "        \"https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u2/tweets_spa.csv\",\n",
        "        index_col=0\n",
        "    )\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e39e4297",
      "metadata": {
        "id": "e39e4297"
      },
      "source": [
        "Validemos los tipos de las columnas que cargamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50addf4c",
      "metadata": {
        "id": "50addf4c"
      },
      "outputs": [],
      "source": [
        "display(df.dtypes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83a0ee4e",
      "metadata": {
        "id": "83a0ee4e"
      },
      "source": [
        "## **2. Estructura Típica de Preprocesamiento**\n",
        "---\n",
        "\n",
        "En este caso veremos algunas de las operaciones más comunes en distintos preprocesamientos de información textual. Es necesario resaltar que no en todos los casos necesitará aplicar todas las técnicas e incluso en alguna aplicación específica va a necesitar estrategias más especializadas, no obstante, lo que presentamos en este caso aplica de forma general a la gran mayoría de aplicaciones de Procesamiento de Lenguaje Natural.\n",
        "\n",
        "Veamos un diagrama de los **componentes de preprocesamiento** que veremos en este _notebook_:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1LZ21z7HWloUlrZPxQPgG2656VPXgglUl\" width=\"100%\">\n",
        "\n",
        "Para estos ejemplos diseñaremos el preprocesamiento para un único documento y, posteriormente, aplicaremos todo el **preprocesamiento** al _corpus_ al finalizar el _notebook_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48a44e14",
      "metadata": {
        "id": "48a44e14"
      },
      "outputs": [],
      "source": [
        "text = df['text'].iloc[2]\n",
        "print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8728b2ca",
      "metadata": {
        "id": "8728b2ca"
      },
      "source": [
        "## **3. Tokenizado del Texto**\n",
        "---\n",
        "\n",
        "El proceso de tokenizado consiste en separar el texto en unidades lógicas (caracteres, palabras, oraciones).\n",
        "\n",
        "Para esto usaremos `spacy`, comenzamos importándolo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ccf76c1c",
      "metadata": {
        "id": "ccf76c1c"
      },
      "outputs": [],
      "source": [
        "import spacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a39a79d3",
      "metadata": {
        "id": "a39a79d3"
      },
      "source": [
        "Descargamos el _Pipeline_ para el español:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3acaaa57",
      "metadata": {
        "id": "3acaaa57"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43acb75e",
      "metadata": {
        "id": "43acb75e"
      },
      "source": [
        "Creamos el _Pipeline_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5e9d85e",
      "metadata": {
        "id": "f5e9d85e"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b4fa39",
      "metadata": {
        "id": "83b4fa39"
      },
      "source": [
        "Vamos a crear un documento de `spacy` a partir del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96d9560f",
      "metadata": {
        "id": "96d9560f"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)\n",
        "print(doc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7be06616",
      "metadata": {
        "id": "7be06616"
      },
      "source": [
        "Podemos extraer los tokens a nivel de palabras del documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c67335e",
      "metadata": {
        "id": "3c67335e"
      },
      "outputs": [],
      "source": [
        "tokens = [token for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33d90b59",
      "metadata": {
        "id": "33d90b59"
      },
      "source": [
        "Veamos el tipo de un elemento de `tokens` para validar que sea un `Token` de `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "583935de",
      "metadata": {
        "id": "583935de"
      },
      "outputs": [],
      "source": [
        "print(type(tokens[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a212f0c2",
      "metadata": {
        "id": "a212f0c2"
      },
      "source": [
        "## **4. Filtrado de Palabras**\n",
        "---\n",
        "\n",
        "Una práctica común en el preprocesamiento de textos es el filtrado de _tokens_ según distintas condiciones.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rgVQ933f6qXPslXbRLYE57MwvVl2b5hl\" width=\"100%\">\n",
        "\n",
        "Por ejemplo, podemos eliminar todas las palabras que sean _stopwords_, para ello definimos una condición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9d1e9ce",
      "metadata": {
        "id": "c9d1e9ce"
      },
      "outputs": [],
      "source": [
        "condition = lambda token: not token.is_stop"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a68c3da",
      "metadata": {
        "id": "4a68c3da"
      },
      "source": [
        "Filtramos los tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c1a3b5",
      "metadata": {
        "id": "d1c1a3b5"
      },
      "outputs": [],
      "source": [
        "filtered_tokens = list(filter(condition, tokens))\n",
        "print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "39cb3fdd",
      "metadata": {
        "id": "39cb3fdd"
      },
      "source": [
        "Como se puede observar, eliminamos _tokens_ como `\"a\"`, `\"las\"`, `\"y\"`, `\"los\"`, entre otras que son muy comunes (y poco informativas) en Español.\n",
        "\n",
        "También es posible filtrar palabras por longitud, por ejemplo, en español la palabra más larga tiene 23 letras (electroencefalografista), por ello, podemos definir una condición para filtrar palabras que tengan una longitud en un rango dado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8b0e3a1",
      "metadata": {
        "id": "a8b0e3a1"
      },
      "outputs": [],
      "source": [
        "condition = lambda token: len(token) > 0 and len(token) < 24"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f14256b8",
      "metadata": {
        "id": "f14256b8"
      },
      "source": [
        "Filtramos los tokens:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4374a65",
      "metadata": {
        "id": "e4374a65"
      },
      "outputs": [],
      "source": [
        "filtered_tokens2 = list(filter(condition, filtered_tokens))\n",
        "print(filtered_tokens2)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08611a5e",
      "metadata": {
        "id": "08611a5e"
      },
      "source": [
        "## **5. Lematización**\n",
        "---\n",
        "\n",
        "En algunas aplicaciones no es de importancia la conjugación de las palabras (tiempo, plurales, géneros, ...), por lo cual, se suele transformar el texto a sus versiones lematizadas.\n",
        "\n",
        "Vamos a aplicar este enfoque sobre cada uno de los _tokens_ filtrados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08618494",
      "metadata": {
        "id": "08618494"
      },
      "outputs": [],
      "source": [
        "lemmas = [token.lemma_ for token in filtered_tokens2]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e9e3d5a",
      "metadata": {
        "id": "3e9e3d5a"
      },
      "source": [
        "Como puede ver, palabras como `\"trae\"` se convierten en su infinitivo `\"traer\"`.\n",
        "\n",
        "Finalmente, unimos todos los _tokens_ en un único _string_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4850ac20",
      "metadata": {
        "id": "4850ac20"
      },
      "outputs": [],
      "source": [
        "lemma_text = \" \".join(lemmas)\n",
        "print(lemma_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2582a2f3",
      "metadata": {
        "id": "2582a2f3"
      },
      "source": [
        "## **6. Normalización de Caracteres**\n",
        "---\n",
        "\n",
        "En muchos idiomas tenemos caracteres modificadores de vocales o letras. Por ejemplo, en español tenemos las tildes y la letra \"ñ\"; o en el caso del portugués tenemos tres tipos de acentos.\n",
        "\n",
        "Este tipo de variaciones pueden ser eliminadas en un proceso de normalización de los textos con distintos tipos de codificaciones. Para el español es muy común usar la librería `unidecode`, vamos a instalarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e86eaa4",
      "metadata": {
        "id": "4e86eaa4"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "11625440",
      "metadata": {
        "id": "11625440"
      },
      "source": [
        "Ahora podemos importarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b90f0444",
      "metadata": {
        "id": "b90f0444"
      },
      "outputs": [],
      "source": [
        "from unidecode import unidecode"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97d10e2b",
      "metadata": {
        "id": "97d10e2b"
      },
      "source": [
        "Veamos cómo queda el texto luego de normalizarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38048fc8",
      "metadata": {
        "id": "38048fc8"
      },
      "outputs": [],
      "source": [
        "norm_text = unidecode(lemma_text)\n",
        "print(norm_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "92bd3b2c",
      "metadata": {
        "id": "92bd3b2c"
      },
      "source": [
        "Como puede ver, ya no hay tildes en el texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ffd14ef",
      "metadata": {
        "id": "7ffd14ef"
      },
      "source": [
        "## **7. Modificación de la Grafía**\n",
        "---\n",
        "\n",
        "Normalmente, los textos se suelen procesar en minúsculas. Con esto eliminamos modificadores relacionados con el inicio de un texto, palabras capitalizadas luego de signos de puntuación, entre otras cosas.\n",
        "\n",
        "En este caso, convertimos el texto a minúsculas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c78b687",
      "metadata": {
        "id": "0c78b687"
      },
      "outputs": [],
      "source": [
        "lower_text = norm_text.lower()\n",
        "print(lower_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0e6a79fb",
      "metadata": {
        "id": "0e6a79fb"
      },
      "source": [
        "## **8. Limpieza con Regex**\n",
        "---\n",
        "\n",
        "Comúnmente se suelen aplicar expresiones regulares para eliminar caracteres o secuencias de caracteres no deseadas. En el ejemplo que estamos desarrollando, podemos aplicar una expresión regular para eliminar todos los caracteres que no sean espacios ni letras minúsculas.\n",
        "\n",
        "Primero importamos la librería para expresiones regulares:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3fafcbc",
      "metadata": {
        "id": "f3fafcbc"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3d74f91b",
      "metadata": {
        "id": "3d74f91b"
      },
      "source": [
        "Ahora, definimos una expresión regular que cumpla con los criterios mencionados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "438236cc",
      "metadata": {
        "id": "438236cc"
      },
      "outputs": [],
      "source": [
        "pat = re.compile(r\"[^a-z ]\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0498c865",
      "metadata": {
        "id": "0498c865"
      },
      "source": [
        "Reemplazamos las coincidencias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e5cb5e1",
      "metadata": {
        "id": "4e5cb5e1"
      },
      "outputs": [],
      "source": [
        "clean_text = re.sub(pat, \"\", lower_text)\n",
        "print(clean_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de716a3f",
      "metadata": {
        "id": "de716a3f"
      },
      "source": [
        "Como se puede ver, ahora tenemos espacios repetidos (ya que algunos _tokens_ se eliminaron), podemos usar una segunda expresión regular para eliminar espacios repetidos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1bf3d11e",
      "metadata": {
        "id": "1bf3d11e"
      },
      "outputs": [],
      "source": [
        "spaces = re.compile(r\"\\s{2,}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dce20547",
      "metadata": {
        "id": "dce20547"
      },
      "source": [
        "Reemplazamos espacios repetidos por un único espacio:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acab5725",
      "metadata": {
        "id": "acab5725"
      },
      "outputs": [],
      "source": [
        "spaces_text = re.sub(spaces, \" \", clean_text)\n",
        "print(spaces_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4bddf155",
      "metadata": {
        "id": "4bddf155"
      },
      "source": [
        "## **9. Preprocesamiento Completo**\n",
        "---\n",
        "\n",
        "Como pudimos ver en el paso anterior, tenemos un texto más limpio que puede llegar a ser más fácil de analizar de manera automática.\n",
        "\n",
        "A continuación veremos la aplicación del preprocesamiento sobre el _corpus_ completo. Para ello, definimos la función `preprocess` la cual toma como entrada un documento cualquiera y retorna un documento preprocesado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c332435",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "3c332435"
      },
      "outputs": [],
      "source": [
        "pat = re.compile(r\"[^a-z ]\")\n",
        "spaces = re.compile(r\"\\s{2,}\")\n",
        "\n",
        "def preprocess(text, min_len=1, max_len=23):\n",
        "    # Creamos documento de spacy\n",
        "    doc = nlp(text)\n",
        "    # Eliminamos stopwords\n",
        "    filtered_tokens = filter(\n",
        "            lambda token: not token.is_stop,\n",
        "            doc\n",
        "            )\n",
        "    # Filtramos palabras por longitud\n",
        "    filtered_tokens2 = filter(\n",
        "            lambda token: len(token) >= min_len and len(token) <= max_len,\n",
        "            filtered_tokens\n",
        "        )\n",
        "    # Obtenemos los lemmas de cada token\n",
        "    lemmas = map(\n",
        "            lambda token: token.lemma_,\n",
        "            filtered_tokens2\n",
        "            )\n",
        "    lemma_text = \" \".join(lemmas)\n",
        "    # Normalizamos el texto\n",
        "    norm_text = unidecode(lemma_text)\n",
        "    # Quitamos grafía\n",
        "    lower_text = norm_text.lower()\n",
        "    # Eliminamos caracteres especiales\n",
        "    clean_text = re.sub(pat, \"\", lower_text)\n",
        "    # Eliminamos espacios duplicados\n",
        "    spaces_text = re.sub(spaces, \" \", clean_text)\n",
        "    return spaces_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13eee7a1",
      "metadata": {
        "id": "13eee7a1"
      },
      "source": [
        "Veamos un ejemplo de la función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b55f6597",
      "metadata": {
        "id": "b55f6597"
      },
      "outputs": [],
      "source": [
        "prep_text = preprocess(df['text'].iloc[0])\n",
        "print(prep_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4c73e2a",
      "metadata": {
        "id": "d4c73e2a"
      },
      "source": [
        "Podemos usar el método `apply` de los dataframes de `pandas` para aplicar el preprocesamiento sobre todo el _corpus_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9dc243ec",
      "metadata": {
        "id": "9dc243ec"
      },
      "outputs": [],
      "source": [
        "prep_corpus = df['text'].apply(preprocess).tolist()\n",
        "print(prep_corpus[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48226dd8",
      "metadata": {
        "id": "48226dd8"
      },
      "source": [
        "Como se puede ver, obtuvimos un _corpus_ más estandarizado y compacto con un flujo típico de preprocesamiento."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e201657b",
      "metadata": {
        "id": "e201657b"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [Text Preprocessing in Natural Language Processing](https://towardsdatascience.com/text-preprocessing-in-natural-language-processing-using-python-6113ff5decd8).\n",
        "- [Natural Language Processing | Text Preprocessing | Spacy vs NLTK](https://medium.com/nerd-for-tech/natural-language-processing-text-preprocessing-spacy-vs-nltk-b70b734f5560).\n",
        "- _Fuente de los íconos_\n",
        "    - Flaticon. Twitter free icon [PNG]. https://www.flaticon.com/free-icon/twitter_2504947\n",
        "    - Flaticon. User free icon [PNG]. https://www.flaticon.com/free-icon/user_3177440\n",
        "    - Flaticon. Document free icon [PNG]. https://www.flaticon.com/free-icon/document_888071\n",
        "    - Flaticon. Documents File free icon [PNG]. https://www.flaticon.com/free-icon/documents_3135874\n",
        "    - Flaticon. Filter free icon [PNG]. https://www.flaticon.com/free-icon/filter_7783331\n",
        "    - Flaticon. Transformation free icon [PNG]. https://www.flaticon.com/free-icon/transformation_1139032\n",
        "    - Flaticon. Languages free icon [PNG]. https://www.flaticon.com/free-icon/languages_3898150\n",
        "    - Flaticon. Letter A free icon [PNG]. https://www.flaticon.com/free-icon/letter-a_3665909\n",
        "    - Flaticon. Letter a free icon [PNG]. https://www.flaticon.com/free-icon/letter-a_3665887\n",
        "    - Flaticon. Down Right Arrow free icon [PNG]. https://www.flaticon.com/free-icon/down-right-arrow_3272625\n",
        "    - Flaticon. Broom free icon [PNG]. https://www.flaticon.com/free-icon/broom_2954880\n",
        "    - Flaticon. Hierarchy free icon [PNG]. https://www.flaticon.com/free-icon/hierarchy_4657126"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "291aa38d",
      "metadata": {
        "id": "291aa38d"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}