{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo2/M5U2_Redes_Neuronales_en_Keras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1Md0grlXXV4fB0bkYuMWzBiS64RvjaFGX\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "metadata": {
        "id": "l5b83QTyp5S9"
      },
      "id": "l5b83QTyp5S9"
    },
    {
      "cell_type": "markdown",
      "id": "1c31a62e",
      "metadata": {
        "id": "1c31a62e"
      },
      "source": [
        "#**Redes Neuronales en Keras**\n",
        "----\n",
        "\n",
        "\n",
        "Como vimos en la **Unidad 1**, _Keras_ es un paquete de _Tensorflow 2.0_, que actúa como un _framework_ de alto nivel simplificando todo el flujo de trabajo relacionado a modelos de _Machine Learning_ y _Deep Learning_. En esta unidad exploraremos más a fondo cómo se maneja este flujo de trabajo usando todas las herramientas que _Keras_ tiene a disposición. Veremos:\n",
        "\n",
        "- Fundamentos de Redes Neuronales Artificiales\n",
        "  - Perceptrón de Rosenblat\n",
        "  - Cómo aprender funciones lógicas\n",
        "- Redes Neuronales en _Keras_\n",
        "  - Capas o *Layers*\n",
        "    - Funciones de activación\n",
        "  - Definición de modelos\n",
        "    - Modelo secuencial\n",
        "    - Modelo funcional\n",
        "  - Flujo de entrenamiento\n",
        "    - Optimizadores\n",
        "    - Funciones de pérdida\n",
        "    - Compilación\n",
        "  - Evaluación\n",
        "- Ejemplos de aplicación\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importaremos algunos paquetes y definiremos algunas funciones que nos ayudarán a visualizar regiones de decisión :  "
      ],
      "metadata": {
        "id": "l-ZipDV_cBQ1"
      },
      "id": "l-ZipDV_cBQ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eefcd78a",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "eefcd78a"
      },
      "outputs": [],
      "source": [
        "# Importamos las librerías que utilizaremos\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import networkx as nx\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "plt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8bdb5d95",
      "metadata": {
        "id": "8bdb5d95"
      },
      "source": [
        "Definiremos algunas funciones para facilitar la visualización, usaremos estas funciones a lo largo del notebook para comprender los conceptos de redes neuronales :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "013355af",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "013355af"
      },
      "outputs": [],
      "source": [
        "# Función para mostrar regiones de decisión\n",
        "def plot_decision_region(X, y, model):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    min_x = np.min(X[:, 0])\n",
        "    max_x = np.max(X[:, 0])\n",
        "    min_y = np.min(X[:, 1])\n",
        "    max_y = np.max(X[:, 1])\n",
        "    min_x = min_x - (max_x - min_x)*0.05\n",
        "    max_x = max_x + (max_x - min_x)*0.05\n",
        "    min_y = min_y - (max_y - min_y)*0.05\n",
        "    max_y = max_y + (max_y - min_y)*0.05\n",
        "    x_1 = np.linspace(min_x, max_x, 100)\n",
        "    x_2 = np.linspace(min_y, max_y, 100)\n",
        "    x1,x2 = np.meshgrid(x_1, x_2)\n",
        "    X_grid = np.concatenate([x1.reshape(-1,1), x2.reshape(-1,1)], axis=1)\n",
        "    y_pred = model(tf.constant(X_grid, dtype=tf.float32))\n",
        "    Z = y_pred.numpy().reshape(x1.shape)\n",
        "    plt.contourf(x1, x2, Z, cmap=plt.cm.RdBu, alpha=0.2)\n",
        "    plt.scatter(X[:,0], X[:,1], c=y, alpha=1.0, cmap=plt.cm.RdBu, s=100)\n",
        "    plt.xlabel(\"$x_1$\"); plt.ylabel(\"$x_2$\")\n",
        "\n",
        "# Función para imprimir las predicciones\n",
        "def print_labels(y_true, y_pred):\n",
        "    Y1 = y_true.numpy().flatten()\n",
        "    Y2 = y_pred.numpy().flatten()\n",
        "    print(\"  Valor esperado   Valor predicho  \")\n",
        "    print(\"_\"*35)\n",
        "    for y1, y2 in zip(Y1, Y2):\n",
        "        print(f\"|      {y1:.2f}       |       {y2:.2f}      |\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6c81b9",
      "metadata": {
        "id": "fc6c81b9"
      },
      "source": [
        "Finalmente, importamos _TensorFlow_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "179d776b",
      "metadata": {
        "id": "179d776b"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "08d68943",
      "metadata": {
        "id": "08d68943"
      },
      "source": [
        "Seleccionamos las semillas para efectos de reproducibilidad :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0b04f260",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "0b04f260"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "tf.random.set_seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d17a33b6",
      "metadata": {
        "id": "d17a33b6"
      },
      "source": [
        "# **1. Perceptrón de Rosenblatt**\n",
        "---\n",
        "\n",
        "Una de las primeras versiones de red neuronal es el **modelo neuronal de McCulloch y Pitts (1943)**, también conocido como *Threshold Logic Unit* (TLU) o *Linear Threshold Unit*. Se trata de una unidad de cálculo que intenta modelar el comportamiento de una neurona, constituyendo la unidad esencial con la cual se construye una red neuronal artificial o *artificial neural network* (ANN).\n",
        "\n",
        "El resultado del cálculo en una neurona de este tipo consiste en obtener una combinación lineal (producto punto o suma ponderada) entre un vector de entrada $\\vec{x}=(x_1, x_2, \\dots, x_n)$ y unos pesos $\\vec{w}=(w_1, w_2, \\dots, w_n)$ más un sesgo $b$, seguido de la aplicación de una función no lineal o **función de activación** $\\phi$:\n",
        "\n",
        "$$\n",
        "\\phi( \\vec{x} \\cdot \\vec{w} + b).\n",
        "$$\n",
        "\n",
        "En 1957 Frank Rosenblatt, un profesor de psicología en la Universidad de Cornell, creó un modelo matemático conocido como el **perceptrón simple**. Se trata de una red neuronal de una capa y una única neurona que puede ser utilizada para el reconocimiento de patrones o tareas de clasificación. Rosenblatt desarrolló un modelo que podía ser entrenado con **aprendizaje supervisado**, es decir, extrayendo patrones de ejemplos etiquetados para aprender a clasificar de forma automática.\n",
        "\n",
        "El perceptrón simple es una red neuronal que usa una función de activación ***Heaviside*** o de salto unitario, cuya ecuación y gráfica se muestra a continuación:\n",
        "\n",
        "\n",
        "$$\n",
        "\\begin{align}\n",
        "    H(\\vec{x},\\vec{w},\\theta)=\\left\\{\\begin{array}{l}\n",
        "                 1 & \\text{si} ~ \\vec{x}\\cdot\\vec{w}+ \\theta >0 \\\\\n",
        "                 0 & \\text{en otro caso}\\\\\n",
        "                 \\end{array}\n",
        "       \\right.\n",
        "\\end{align}\n",
        "$$\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=19iKoRD7i-jTK7M89ViACc4eHAVhoeQAK\" alt =\"Gráfico de la función de activación Heavside\" width=\"50%\">\n",
        "</center>\n",
        "\n",
        "\n",
        "Por simplicidad en la notación, el término del sesgo $b$ lo llamaremos $w_0$. Entonces $\\vec{x}$ será $(x_0, x_1, x_2, \\dots, x_n)$ y  $\\vec{w}=(w_0, w_1, w_2, \\dots, w_n)$. De esta manera, el modelo del perceptrón se puede definir así:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ztvLUkbpS_iQ4xCspI-7xwgLMpHmfXRk\" alt=\"Gráfico de la definición del Perceptrón de Rosenblatt\" width=\"70%\">\n",
        "</center>\n",
        "\n",
        "$$\n",
        "\\widetilde{y}_i=H(\\vec{x},\\vec{w},\\theta)\\\\\n",
        "$$\n",
        "\n",
        "Veamos un ejemplo del comportamiento de un perceptrón simple en _TensorFlow_, comenzamos definiendo una función correspondiente al modelo: `X` será una fila de la matriz de observaciones, y `(w, w_0)` serán los pesos. `tf.matmul` calcula el producto punto de dos vectores, y `tf.sign` retorna $1$ o $-1$ según el signo del número que reciba como argumento. Sumando $1$ a la función `tf.sign` y dividiendo el resultado por $2$, obtenemos la función de Heaviside $H$:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a791fc9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "2a791fc9"
      },
      "outputs": [],
      "source": [
        "# Definimos el perceptrón de Rosenblatt\n",
        "def rosenblatt(X, w, w_0):\n",
        "    # Con ayuda de la función tf.sign podemos definir la función heaviside\n",
        "    return(tf.sign(tf.matmul(X, w) + w_0) + 1) / 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b910cdbe",
      "metadata": {
        "id": "b910cdbe"
      },
      "source": [
        "## 1.1 Aprendiendo compuertas lógicas"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c194e9c",
      "metadata": {
        "id": "4c194e9c"
      },
      "source": [
        "Ahora vamos a ajustar **manualmente** los pesos del modelo para solucionar funciones Booleanas básicas y comprender qué está aprendiendo el perceptrón."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e6233a6",
      "metadata": {
        "id": "3e6233a6"
      },
      "source": [
        "### 1.1.1 AND\n",
        "\n",
        "Veamos un ejemplo del perceptrón de Rosenblatt. En este caso, lo utilizaremos para aprender la función lógica **AND**, que recibe como entradas dos variables Booleanas $x_1$ y $x_2$ y retorna el valor Booleano $y$.\n",
        "\n",
        "|$x_1$|$x_2$|$y$|\n",
        "|---|---|---|\n",
        "|0|0|0|\n",
        "|0|1|0|\n",
        "|1|0|0|\n",
        "|1|1|1|\n",
        "\n",
        "\n",
        "A partir de esto podemos definir los datos ($X$, $y$) para entrenar el modelo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0241dbab",
      "metadata": {
        "id": "0241dbab"
      },
      "outputs": [],
      "source": [
        "# Definimos la matriz de observaciones o entradas\n",
        "X = tf.constant(\n",
        "    [\n",
        "        [0, 0],\n",
        "        [0, 1],\n",
        "        [1, 0],\n",
        "        [1, 1]\n",
        "        ],\n",
        "    dtype=tf.float32\n",
        "    )\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1f0ab2de",
      "metadata": {
        "id": "1f0ab2de"
      },
      "outputs": [],
      "source": [
        "# Definimos las etiquetas\n",
        "y_AND = tf.constant([0, 0, 0, 1], dtype=tf.float32)\n",
        "print(y_AND)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa172b45",
      "metadata": {
        "id": "fa172b45"
      },
      "source": [
        "Ahora, definimos los parámetros del modelo: $\\vec{w}$ y realizamos las predicciones :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc07276f",
      "metadata": {
        "id": "fc07276f"
      },
      "outputs": [],
      "source": [
        "# Inicializamos los pesos de forma aleatoria\n",
        "w_and = tf.Variable(\n",
        "        np.random.uniform(-1, 1, size=(2, 1)),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "w_0_and = tf.Variable(\n",
        "        np.random.uniform(-1, 1),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "# Realizamos las predicciones\n",
        "y_tilde = rosenblatt(X, w_and, w_0_and)\n",
        "print(y_tilde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "728c7517",
      "metadata": {
        "id": "728c7517"
      },
      "source": [
        "Como puede evidenciarse, no obtuvimos el resultado correcto, pero era de esperarse dado que los pesos del modelo fueron escogidos de forma aleatoria. Ahora, modificaremos los valores de `w` y `w_0` hasta conseguir predecir la función **AND**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cd6e615",
      "metadata": {
        "id": "6cd6e615"
      },
      "outputs": [],
      "source": [
        "# Para asignar un valor a w utilice: w_and.assign(val):\n",
        "\n",
        "w_and.assign([[1],[1]])\n",
        "\n",
        "# Para asignar un valor a w_0 utilice: w_0_and.assign(val):\n",
        "\n",
        "w_0_and.assign(-1.5)\n",
        "\n",
        "print_labels(y_AND, rosenblatt(X, w_and, w_0_and))\n",
        "plot_decision_region(\n",
        "        X, y_AND,\n",
        "        lambda x:rosenblatt(x, w_and, w_0_and)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25bd901",
      "metadata": {
        "id": "f25bd901"
      },
      "source": [
        "### 1.1.2 Función OR\n",
        "\n",
        "Ahora, vamos a solucionar el problema **OR**\n",
        "\n",
        "|$x_1$|$x_2$|$y$|\n",
        "|---|---|---|\n",
        "|0|0|0|\n",
        "|0|1|1|\n",
        "|1|0|1|\n",
        "|1|1|1|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2075ca7a",
      "metadata": {
        "id": "2075ca7a"
      },
      "outputs": [],
      "source": [
        "# Definimos la matriz de observaciones\n",
        "X = tf.constant(\n",
        "        [\n",
        "            [0, 0],\n",
        "            [0, 1],\n",
        "            [1, 0],\n",
        "            [1, 1]\n",
        "            ],\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "47e1c7d3",
      "metadata": {
        "id": "47e1c7d3"
      },
      "outputs": [],
      "source": [
        "# Definimos las etiquetas\n",
        "y_OR = tf.constant([0, 1, 1, 1], dtype=tf.float32)\n",
        "print(y_OR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23c94666",
      "metadata": {
        "id": "23c94666"
      },
      "outputs": [],
      "source": [
        "# Inicializamos los pesos de forma aleatoria\n",
        "w_or = tf.Variable(\n",
        "        np.random.uniform(-1, 1, size=(2, 1)),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "w_0_or = tf.Variable(\n",
        "        np.random.uniform(-1, 1),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "# Realizamos las predicciones\n",
        "y_tilde = rosenblatt(X, w_or, w_0_or)\n",
        "print(y_tilde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cd2d8307",
      "metadata": {
        "id": "cd2d8307"
      },
      "source": [
        "Ahora, modificaremos los valores de `w` y `w_0` hasta conseguir predecir la función **OR**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "188addc0",
      "metadata": {
        "id": "188addc0"
      },
      "outputs": [],
      "source": [
        "# Para asignar un valor a w utilice: w_or.assign(val):\n",
        "\n",
        "w_or.assign([[1.0],[1.0]])\n",
        "\n",
        "# Para asignar un valor a w_0 utilice: w_0_or.assign(val):\n",
        "\n",
        "w_0_or.assign(-0.5)\n",
        "\n",
        "print_labels(y_OR, rosenblatt(X, w_or, w_0_or))\n",
        "plot_decision_region(X, y_OR, lambda x:rosenblatt(x, w_or, w_0_or))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e202f0",
      "metadata": {
        "id": "35e202f0"
      },
      "source": [
        "### 1.1.3 Función XOR\n",
        "\n",
        "Ahora, intentemos solucionar el problema **XOR** con el perceptrón simple\n",
        "\n",
        "|$x_1$|$x_2$|$y$|\n",
        "|---|---|---|\n",
        "|0|0|0|\n",
        "|0|1|1|\n",
        "|1|0|1|\n",
        "|1|1|0|"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f65672bf",
      "metadata": {
        "id": "f65672bf"
      },
      "outputs": [],
      "source": [
        "# Definimos la matriz de observaciones\n",
        "X = tf.constant(\n",
        "        [\n",
        "            [0, 0],\n",
        "            [0, 1],\n",
        "            [1, 0],\n",
        "            [1, 1]\n",
        "            ],\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a79ea4b5",
      "metadata": {
        "id": "a79ea4b5"
      },
      "outputs": [],
      "source": [
        "# Definimos las etiquetas\n",
        "y_XOR = tf.constant([0, 1, 1, 0], dtype=tf.float32)\n",
        "print(y_XOR)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df830a1a",
      "metadata": {
        "id": "df830a1a"
      },
      "outputs": [],
      "source": [
        "# Inicializamos los pesos de forma aleatoria\n",
        "w = tf.Variable(\n",
        "        np.random.uniform(-1, 1, size=(2, 1)),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "w_0 = tf.Variable(\n",
        "        np.random.uniform(-1, 1),\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "# Realizamos las predicciones\n",
        "y_tilde = rosenblatt(X, w, w_0)\n",
        "print(y_tilde)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7137193e",
      "metadata": {
        "id": "7137193e"
      },
      "source": [
        "Intentaremos modificar los valores de `w` y `b` hasta conseguir predecir la función **XOR**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c524da8",
      "metadata": {
        "id": "2c524da8"
      },
      "outputs": [],
      "source": [
        "# Para asignar un valor a w utilice: w.assign(val):\n",
        "\n",
        "w.assign([[-1],[1]])\n",
        "\n",
        "# Para asignar un valor a w_0 utilice: w_0.assign(val):\n",
        "\n",
        "w_0.assign(0.5)\n",
        "\n",
        "print_labels(y_XOR, rosenblatt(X, w, w_0))\n",
        "plot_decision_region(X, y_XOR, lambda x:rosenblatt(x, w, w_0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c2dfc5c4",
      "metadata": {
        "id": "c2dfc5c4"
      },
      "source": [
        "En este caso la predicción falla, el perceptrón simple **NO** es capaz de resolver el problema **XOR**. La principal razón por la que el perceptrón simple falló (y por la que siempre fallará) es porque el problema **XOR** es un problema donde los puntos no son linealmente separables. Es decir, si lo vemos de forma gráfica, no es posible trazar una recta que separe por completo las dos clases, y es que el perceptrón simple precisamente, solo tiene capacidad para encontrar un hiperplano como frontera de separación entre las dos categorías."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d4ca237",
      "metadata": {
        "id": "2d4ca237"
      },
      "source": [
        "# **2. Redes Neuronales en Keras**\n",
        "---\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PTlZrJ2m0s2mfFPzR365VKAeGl82Bh5J\" width=\"50%\">\n",
        "</center>\n",
        "\n",
        "Actualmente, _Keras_ es uno de los módulos más importantes en _TensorFlow 2.0_. Permite diseñar de forma rápida arquitecturas de _*Deep Learning*_, cargar y guardar modelos, provee varias redes de propósito general y es totalmente compatible con todas las funcionalidades de _TensorFlow_. `tf.keras` provee varios módulos con los componentes fundamentales para diseñar una red neuronal. La idea es construir un modelo a partir de los siguientes elementos: _**capas, optimizador y funciones de pérdida**_.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Red neuronal multicapa**\n",
        "\n",
        "El **perceptrón multicapa o red neuronal multicapa** es una generalización del perceptrón que incluye varias neuronas organizadas en varias capas como se muestra en la siguiente figura :\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=16HuPzZdyHkjaLgZ-VAYdMR4Dxha3IlFn\" alt =\"Diagrama de una Red Neuronal Multicapa\" width=\"70%\"></center>\n",
        "\n",
        "Se trata de un tipo de modelo que tiene un mayor número de parámetros pero que permite solucionar problemas que no son linealmente separables (la cual es la principal limitación del perceptrón simple). **Las redes neuronales profundas (*deep*) son básicamente redes multicapa con alto número de capas.**\n",
        "\n",
        "El entrenamiento de las redes neuronales multicapa se basan fundamentalmente en un algoritmo conocido como _**Back-propagation**_, en el cual se aprovechan las propiedades de la derivada (fundamentalmente la regla de la cadena) y algoritmos de optimización basados en gradiente descendente para determinar los parámetros de la red.\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1BCM7QGQm5Lvph6mmLdsrbLmqGc-SPwVh\" alt =\"Gif explicativo del algoritmo de Back-Propagation\" width=\"60%\"></center>\n",
        "\n",
        "El objetivo al final siempre será optimizar una función de costo (o función de pérdida) **de forma iterativa**.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=16lFqMt2RCd5XokkplvCSuak_m07XqM08\" alt =\"Diagrama explicativo de un Gradiente Descendente\" width=\"50%\"></center>\n",
        "\n",
        "Las redes neuronales multicapa pueden ser fácilmente construidas utilizando _**Keras**_ (`tf.keras`), como se mostrará a continuación."
      ],
      "metadata": {
        "id": "YLBIPZxWI0d4"
      },
      "id": "YLBIPZxWI0d4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Proceso General\n",
        "\n",
        "Veamos un ejemplo de cómo entrenar una red multicapa para solucionar de nuevo el problema **XOR**. De paso veremos cómo es el proceso general para utilizar _Keras_:"
      ],
      "metadata": {
        "id": "G16j9gphRLoK"
      },
      "id": "G16j9gphRLoK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b14fa147",
      "metadata": {
        "id": "b14fa147"
      },
      "outputs": [],
      "source": [
        "# Definimos la matriz de observaciones o muestras de entrada\n",
        "X = tf.constant(\n",
        "        [\n",
        "            [0, 0],\n",
        "            [0, 1],\n",
        "            [1, 0],\n",
        "            [1, 1]\n",
        "            ],\n",
        "        dtype=tf.float32\n",
        "        )\n",
        "print(X)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9150699",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b9150699"
      },
      "outputs": [],
      "source": [
        "# Definimos las etiquetas o valores de salida\n",
        "y_XOR = tf.constant([0, 1, 1, 0], dtype=tf.float32)\n",
        "y_XOR"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Usaremos una capa `tf.keras.layers.Dense`. Recordemos cuáles son los argumentos necesarios para definirla :\n",
        "\n",
        "*   `units`: Un número entero que define el número de neuronas de la capa.\n",
        "*   `input_shape`: Una tupla con la dimensión de los datos de entrada.\n",
        "*   `activation`: Una función de activación que se aplica a la salida de la capa.\n",
        "\n",
        "Puede ser que no definamos una función de activación dentro de `tf.keras.layers.Dense`. En ese caso, **es equivalente** usar `tf.keras.layers.Activation` para definir la activación después de definir la capa densa, como sucede a continuación:\n"
      ],
      "metadata": {
        "id": "ylMtwRYqJUWi"
      },
      "id": "ylMtwRYqJUWi"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "655d0615",
      "metadata": {
        "id": "655d0615"
      },
      "outputs": [],
      "source": [
        "# Definimos el modelo\n",
        "model_seq = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.Input(shape=(2, )),                     # El input es de tamaño (2, )\n",
        "        tf.keras.layers.Dense(units = 16                 # Primera capa de 16 neuronas\n",
        "                              ),                         # No definimos función de activación\n",
        "        tf.keras.layers.Activation(\"sigmoid\"),           # Función de activación \"sigmoid\" como una capa aparte\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")   # Salida: Capa de dos neuronas con activación softmax\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "912dc7a0",
      "metadata": {
        "id": "912dc7a0"
      },
      "source": [
        "Veamos más en detalle:\n",
        "\n",
        "- La primera instrucción (`tf.keras.models.Sequential`) crea un modelo de tipo secuencial, es decir un modelo estructurado en diferentes capas de procesamiento (*layers*) organizadas de manera secuencial.\n",
        "\n",
        "Luego, las capas se definen en una lista que se pasa como argumento a `tf.keras.models.Sequential`:\n",
        "\n",
        "- Se agrega una capa de neuronas *densamente* conectadas con las 2 entradas (`tf.keras.layers.Dense(units = 16, ...`) y con una función de activación _sigmoide_ (`tf.keras.layers.Activation(\"sigmoid\")`).\n",
        "\n",
        "- Luego se agrega la capa de salida constituida por dos neuronas con una función de activación _softmax_ (`tf.keras.layers.Dense(2, activation=\"softmax\")`)."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> **¿Cómo es mejor definir la función de activación? ¿Dentro de `tf.keras.layers.Dense'? O ¿Usando `tf.keras.layers.Activation`?**\n",
        "> Desde el punto de vista del cómputo, es igual. Sin embargo, `tf.keras.layers.Activation` ofrece mayor flexibilidad modular, es decir, permite controlar directamente la función, por ejemplo en caso de querer hacer alguna edición sobre la arquitectura del modelo.\n",
        "\n",
        "Una vez definido el modelo, _Keras_ nos permite ver un resumen de la arquitectura, con detalles como el nombre de cada capa de neuronas, las activaciones, la dimensión de la salida en cada capa, y el número de parámetros asociados, usando el método `summary()`:\n"
      ],
      "metadata": {
        "id": "vo-5-AYSM-zC"
      },
      "id": "vo-5-AYSM-zC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd183104",
      "metadata": {
        "id": "dd183104"
      },
      "outputs": [],
      "source": [
        "model_seq.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este momento los 82 parámetros del modelo tienen valores de inicialización aleatorios. Es decir, el modelo lo podemos usar para hacer predicciones, pero muy seguramente van a ser erróneas :"
      ],
      "metadata": {
        "id": "JTPpU5IqNZ0z"
      },
      "id": "JTPpU5IqNZ0z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "205637e0",
      "metadata": {
        "id": "205637e0"
      },
      "outputs": [],
      "source": [
        "model_seq.predict(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestro modelo tiene una capa de salida de dos neuronas. Es decir, cada predicción arroja dos valores: la probabilidad de que la etiqueta sea $0$ y la probabilidad de que la etiqueta sea $1$. En este caso, el modelo está prediciendo todo hacia una sola etiqueta, es decir, no está haciendo nada interesante; **hay que entrenarlo**.\n",
        "\n",
        "Para entrenar el modelo hay que definir primero, por lo menos, una **función de pérdida** y un **optimizador**. Eso lo hacemos con la función `compile`. En este caso usaremos\n",
        "\n",
        "*   `tf.keras.losses.CategoricalCrossentropy` como función de pérdida, y\n",
        "*   `tf.keras.optimizers.Adam` como optimizador. Aquí además debemos definir un hiper-parámetro muy importante: el **_learning rate_** o tasa de aprendizaje. Usaremos un `learning_rate` de $0.1$.\n",
        "\n",
        "Más adelante en este notebook veremos en detalle las funciones de pérdida y los optimizadores.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "MIrP3D17NtXM"
      },
      "id": "MIrP3D17NtXM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14ecc87d",
      "metadata": {
        "id": "14ecc87d"
      },
      "outputs": [],
      "source": [
        "model_seq.compile(loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate = 1e-1)\n",
        "                  )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como la capa de salida del modelo que definimos tiene dos neuronas, una por cada clase, necesitamos una codificación *one-hot* para las etiquetas de los datos.\n",
        "\n",
        "Usamos la función `tf.one_hot` sobre las etiquetas:"
      ],
      "metadata": {
        "id": "mSXTFhm_OJxC"
      },
      "id": "mSXTFhm_OJxC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "915a558f",
      "metadata": {
        "id": "915a558f"
      },
      "outputs": [],
      "source": [
        "Y = tf.one_hot(tf.cast(y_XOR, \"int32\"), depth=2)\n",
        "print(Y)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con lo anterior, ahora sí podemos entrenar el modelo con la función `fit`, que como argumentos recibe los datos de entrenamiento, y el número de iteraciones o `epochs` del proceso de optimización. El método `fit` que entrena el modelo retorna además un objeto `History` que guarda toda la información del entrenamiento a manera de diccionario. Vamos a guardar todos estos datos de entrenamiento en `hist` para poder hacer rápidamente gráficas del comportamiento de la función de pérdida."
      ],
      "metadata": {
        "id": "pYMOv8WoOVnM"
      },
      "id": "pYMOv8WoOVnM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96c6659b",
      "metadata": {
        "id": "96c6659b"
      },
      "outputs": [],
      "source": [
        "hist = model_seq.fit(x=X,\n",
        "                     y=Y,\n",
        "                     epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y podemos hacer predicciones con la función `predict`:"
      ],
      "metadata": {
        "id": "ShAq7fQ6OvoR"
      },
      "id": "ShAq7fQ6OvoR"
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.predict(X)"
      ],
      "metadata": {
        "id": "bESJUXi10jgr"
      },
      "id": "bESJUXi10jgr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75341db9",
      "metadata": {
        "id": "75341db9"
      },
      "outputs": [],
      "source": [
        "# Calculamos en qué índice está la probabilidad más alta para obtener una predicción concreta de la etiqueta\n",
        "np.argmax(model_seq.predict(X), axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo predice `[0, 1, 1, 0]`, como se esperaba."
      ],
      "metadata": {
        "id": "tfvkQ6BuhXN9"
      },
      "id": "tfvkQ6BuhXN9"
    },
    {
      "cell_type": "markdown",
      "source": [
        "En `hist` queda almacenada la información del valor de la función de pérdida `loss`. Directamente con `matplotlib.pyplot` podemos graficar la curva:"
      ],
      "metadata": {
        "id": "ET9nhIk19nnI"
      },
      "id": "ET9nhIk19nnI"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54ff6300",
      "metadata": {
        "id": "54ff6300"
      },
      "outputs": [],
      "source": [
        "# Veamos la función de pérdida\n",
        "plt.plot(hist.history[\"loss\"])\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Pérdida\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y como es de esperar, la función de pérdida baja a medida que pasan las _epochs_."
      ],
      "metadata": {
        "id": "4NK5gIXct3cg"
      },
      "id": "4NK5gIXct3cg"
    },
    {
      "cell_type": "markdown",
      "id": "1faeb90d",
      "metadata": {
        "id": "1faeb90d"
      },
      "source": [
        "## 2.2 Capas - *Layers*\n",
        "\n",
        "Los objetos centrales de _Keras_ son las capas o *Layers* y (como pudimos ver en la sección anterior) estos serán los bloques de construcción básicos de los modelos de red neuronal. _Keras_ trae una serie de capas de uso general en el paquete `tf.keras.layers`. Cada capa define automáticamente los pesos y puede aplicar directamente una **función de activación**. Algunos ejemplos de *layers* son:\n",
        "\n",
        "* `tf.keras.layers.Input`: Es la capa de entrada del modelo. Siempre es necesaria. Aquí definimos la dimensión de los datos.\n",
        "* `tf.keras.layers.Dense`: Es una capa de neuronas totalmente conectada. Es decir, todas las neuronas de esta capa se conectan con todas las salidas de la capa anterior, y todas las entradas de la capa siguiente (a menos que sea la capa de salida).\n",
        "* `tf.keras.layers.Activation`: Permite aplicar una función de activación. Estas pueden ser `softmax`, `relu`, `tanh`, `sigmoid`, `linear`, entre otras.\n",
        "* `tf.keras.layers.Dropout`: Se trata de una capa de regularización, consiste en establecer al azar una fracción de unidades de la entrada a 0 en cada actualización durante la fase de entrenamiento (ayuda a evitar el **sobreajuste**).\n",
        "* `tf.keras.layers.Convolution2D`: Se trata de una capa convolucional, comúnmente usada en el análisis de imágenes.\n",
        "\n",
        "_***Nota: Otros tipos de capas se encuentran en el paquete***_ [`tf.keras.layers`](https://www.tensorflow.org/api_docs/python/tf/keras/layers).\n",
        "\n",
        "Salvo `Input`, todas estas capas implementan alguna operación sobre los datos de entrada. En este momento las capa que más nos interesa es `Dense`. Esta implementa la operación: `output = activation(dot(input, kernel) + bias)` donde `activation` es la función de activación elemento a elemento pasada como argumento de activación, `kernel` es la matriz de pesos (que luego se aprenden durante el entrenamiento) creada por la capa, y `bias` es el vector de sesgo creado por la capa (sólo aplicable si `use_bias` es `True` (que es de hecho, la opción por defecto)). Todos estos son atributos de `Dense`.\n",
        "\n",
        "Veamos por ejemplo un modelo que recibe vectores de tamaño 2, que luego pasan a través de una capa densa de tamaño 8 con activación `softmax`:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un modelo para el ejemplo:\n",
        "model_a = tf.keras.models.Sequential(\n",
        "    [\n",
        "      tf.keras.Input(shape=(2,)),                      # La entrada es de tamaño 2\n",
        "      tf.keras.layers.Dense(8, activation='softmax')   # Agregamos una capa densa de tamaño 8 con activación softmax\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_a.output_shape    # Verificamos el tamaño de la salida"
      ],
      "metadata": {
        "id": "GV0CXlROvP7F"
      },
      "id": "GV0CXlROvP7F",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Efectivamente el tamaño de la salida es de 8. Veamos cuántos parámetros tiene este modelo:"
      ],
      "metadata": {
        "id": "cHZcBp4VYJhP"
      },
      "id": "cHZcBp4VYJhP"
    },
    {
      "cell_type": "code",
      "source": [
        "model_a.summary()"
      ],
      "metadata": {
        "id": "TSbgAVDze16v"
      },
      "id": "TSbgAVDze16v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Son $24$ parámetros en total. Como la entrada es de tamaño $2$, y la capa densa tiene $8$ neuronas, eso suma $2\\times8=16$ conexiones. Pero la capa densa por defecto incluye un sesgo o `bias`. Esto es equivalente a que el tamaño de la capa de entrada sea $3$, y entonces: $(2+1)\\times8=24$.\n",
        "\n",
        "Veamos también las predicciones del modelo para la primera muestra del conjunto `X`:"
      ],
      "metadata": {
        "id": "6AZXzTSwe2ax"
      },
      "id": "6AZXzTSwe2ax"
    },
    {
      "cell_type": "code",
      "source": [
        "model_a.predict(X[:1])"
      ],
      "metadata": {
        "id": "0Aq-37IfVmgb"
      },
      "id": "0Aq-37IfVmgb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En efecto la salida de la capa es de tamaño 8. Esta capa puede entonces servir como entrada a otra capa densa (o a cualquier capa de _Keras_, guardando los requerimientos necesarios), y de esa manera se construyen modelos cada vez más complejos, por ejemplo:"
      ],
      "metadata": {
        "id": "FnDRodqGgmAn"
      },
      "id": "FnDRodqGgmAn"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un modelo para el ejemplo:\n",
        "model_2dense = tf.keras.models.Sequential(\n",
        "    [\n",
        "      tf.keras.Input(shape=(2,)),                      # La entrada es de tamaño 2\n",
        "      tf.keras.layers.Dense(8, activation='softmax'),  # Agregamos una capa densa de tamaño 8 con activación softmax\n",
        "      tf.keras.layers.Dense(8, activation='softmax')   # Agregamos una capa densa de tamaño 8 con activación softmax\n",
        "    ]\n",
        ")\n",
        "model_2dense.output_shape                                          # Verificamos el tamaño de la salida"
      ],
      "metadata": {
        "id": "EgSOBqEVhBlM"
      },
      "id": "EgSOBqEVhBlM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_2dense.summary()"
      ],
      "metadata": {
        "id": "zhzLqNCfhPXZ"
      },
      "id": "zhzLqNCfhPXZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "El modelo anterior ya tiene 96 parámetros. La segunda capa densa aporta 72 parámetros que se explican por 8 neuronas de entrada, más el `bias`, por 8 neuronas de salida: $(8+1)\\times8 = 72$."
      ],
      "metadata": {
        "id": "2df2u9lshN_h"
      },
      "id": "2df2u9lshN_h"
    },
    {
      "cell_type": "markdown",
      "id": "1f0a25b5",
      "metadata": {
        "id": "1f0a25b5"
      },
      "source": [
        "## 2.3 Función de activación\n",
        "\n",
        "El perceptrón simple utilizaba una función _heaviside_ para realizar una predicción. En contraste, en un **perceptrón multicapa** se pueden utilizar distintas activaciones (siempre y cuando sean diferenciables). _Tensorflow_ nos ofrece una capa de activación `tf.keras.layers.Activation` que recibe como argumento la función específica que uno defina. Algunas de las funciones de activación más usadas son :\n",
        "\n",
        "|Nombre|Expresión|Tensorflow|\n",
        "|---|---|---|\n",
        "|<img width=100/>|<img width=100/>|<img width=100/>|\n",
        "|Sigmoid|$\\frac{1}{1+e(-x)}$| `'sigmoid'`|\n",
        "|Tanh|$\\text{tanh}(x)$| `'tanh'`|\n",
        "|ReLU|$\\max\\{0,x\\}$| `'relu'`|\n",
        "|SoftMax|$\\frac{e^{x_i}}{\\sum_{j=1}^{l}e^{x_j}}$|`'softmax'`|\n",
        "\n",
        "La función de activación _SoftMax_ es una activación especial que es comúnmente utilizada en la salida de los modelos. Se trata de una función que convierte la salida del modelo en valores susceptibles de interpretarse como la probabilidad de que una observación pertenezca cada una de las categorías posibles.\n",
        "\n",
        "_***Nota: Otros tipos de activación se pueden consultar en el paquete***_ [`tf.keras.activations`](https://www.tensorflow.org/api_docs/python/tf/keras/activations).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos más a detalle la función _SoftMax_. Podemos programarla:"
      ],
      "metadata": {
        "id": "NGZ9KaqMBKsA"
      },
      "id": "NGZ9KaqMBKsA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "163c213e",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "163c213e"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def my_softmax(x):\n",
        "    return tf.exp(x)/tf.math.reduce_sum(tf.exp(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y calcularla sobre un vector, por ejemplo `[1,2,3]`:"
      ],
      "metadata": {
        "id": "7j_gZPNJBZWe"
      },
      "id": "7j_gZPNJBZWe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d49fd96",
      "metadata": {
        "id": "7d49fd96"
      },
      "outputs": [],
      "source": [
        "x = np.array([1., 2., 3.])\n",
        "my_softmax(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Facilmente podemos comprobar que _SoftMax_ convirtió el vector en uno nuevo, en donde la suma de los componentes es igual a $1$:"
      ],
      "metadata": {
        "id": "PNuMhSR2BpXQ"
      },
      "id": "PNuMhSR2BpXQ"
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.sum(my_softmax(x)))"
      ],
      "metadata": {
        "id": "aZG9GYY_5pXN"
      },
      "id": "aZG9GYY_5pXN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esta misma función la tenemos directamente disponible en _Tensorflow_ como `tf.nn.softmax`:"
      ],
      "metadata": {
        "id": "okJqRz1BB9UY"
      },
      "id": "okJqRz1BB9UY"
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([1., 2., 3.])\n",
        "print(tf.nn.softmax(x))"
      ],
      "metadata": {
        "id": "u70_S3sBCKec"
      },
      "id": "u70_S3sBCKec",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y también podemos definirla como una capa dentro de un modelo, por ejemplo:"
      ],
      "metadata": {
        "id": "WZsEbZ5mWQvg"
      },
      "id": "WZsEbZ5mWQvg"
    },
    {
      "cell_type": "code",
      "source": [
        "# Creamos un modelo para el ejemplo:\n",
        "model_b = tf.keras.models.Sequential(\n",
        "    [\n",
        "      tf.keras.Input(shape=(2,)),            # La entrada es de tamaño 2\n",
        "      tf.keras.layers.Dense(8),              # Agregamos una capa densa de tamaño 8\n",
        "      tf.keras.layers.Activation(my_softmax) # Agregamos una capa de función de activación softmax\n",
        "    ]\n",
        ")\n",
        "model_b.output_shape                               # Verificamos el tamaño de la salida"
      ],
      "metadata": {
        "id": "qfgKn-bSXXd7"
      },
      "id": "qfgKn-bSXXd7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que `tf.keras.layers.Activation` nos permite usar funciones definidas dentro de `tensorflow` o funciones propias definidas por nosotros. De nuevo veamos las predicciones de este modelo para la primera muestra del conjunto de datos `X`:"
      ],
      "metadata": {
        "id": "G-XNmqj3YWSF"
      },
      "id": "G-XNmqj3YWSF"
    },
    {
      "cell_type": "code",
      "source": [
        "model_b.predict(X[:1])"
      ],
      "metadata": {
        "id": "adIXjak0X7ff"
      },
      "id": "adIXjak0X7ff",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Nota:** los modelos `model_a` y `model_b` son equivalentes. Definir una función de activación dentro del _layer_ (`tf.keras.layers.Dense(8, activation='softmax')`) es igual a definir la capa sin activación (`tf.keras.layers.Dense(8)`) e inmediatamente después definir la capa de activación (`tf.keras.layers.Activation('softmax')`).  \n"
      ],
      "metadata": {
        "id": "sdY_I_38Yd3D"
      },
      "id": "sdY_I_38Yd3D"
    },
    {
      "cell_type": "markdown",
      "id": "a8a429c2",
      "metadata": {
        "id": "a8a429c2"
      },
      "source": [
        "## 2.4 Modelos\n",
        "\n",
        "Hay dos formas en las que se puede definir un modelo en `keras`, **la forma secuencial y la forma funcional**:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Rvzz7NcvFrTLGlJUoQdS8dYD30yzPOaR\" alt =\"Diagrama ilustrativo del concepto inicial de un Modelo secuencial y un Modelo funcional\" width=\"70%\"></center>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4.1 Secuencial\n",
        "\n",
        "`tf.keras.models.Sequential` es la forma más simple de definir un modelo y sólo sirve para modelos secuenciales, es decir, modelos en donde cada capa recibe únicamente como entrada la salida de la capa que se haya definido anteriormente. La sintaxis general es por tanto muy simple: hay que listar las capas una tras otra dentro de la función `tf.keras.models.Sequential`, es decir: `tf.keras.models.Sequential([Layer1, Layer2, ...])` Este tipo de modelo fue el que ya usamos en la Sección 2.1 (y de hecho en todos los ejemplos anteriores):"
      ],
      "metadata": {
        "id": "dvx-yPA5atMn"
      },
      "id": "dvx-yPA5atMn"
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq = tf.keras.models.Sequential(\n",
        "    [\n",
        "        tf.keras.layers.Dense(units = 16, input_shape=(2, )),\n",
        "        tf.keras.layers.Activation(\"sigmoid\"),\n",
        "        tf.keras.layers.Dense(2, activation=\"softmax\")\n",
        "    ]\n",
        ")"
      ],
      "metadata": {
        "id": "pXt6JO7p2gtC"
      },
      "id": "pXt6JO7p2gtC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Veamos en detalle otro ejemplo más de un modelo secuencial. Primero tenemos que definir el modelo, usando la función `tf.keras.models.Sequential()`. El modelo lo llamaremos `model_seq`:"
      ],
      "metadata": {
        "id": "Sco7UaC82ieB"
      },
      "id": "Sco7UaC82ieB"
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "j-ahcEdVHPjC"
      },
      "id": "j-ahcEdVHPjC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego de definirlo, podemos agregarle capas al modelo. Vamos a agregar dos capas densas, la primera compuesta por 64 neuronas con una función de activación _ReLU_. Recordemos que _ReLU_ es una abreviación de _Rectifier Linear Unit_, y está definida por:\n",
        "$$\n",
        "f(x)=\\max\\{0,x\\},\n",
        "$$\n",
        "es decir, es una función que filtra valores negativos.\n",
        "La capa la agregamos con la función `add()` aplicada sobre `model_seq`:"
      ],
      "metadata": {
        "id": "HWvBJE_IHSaF"
      },
      "id": "HWvBJE_IHSaF"
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=64,\n",
        "        input_shape=(100,),\n",
        "        activation=\"relu\"\n",
        "        )\n",
        "    )"
      ],
      "metadata": {
        "id": "1toUU27qHxC-"
      },
      "id": "1toUU27qHxC-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente agregamos la capa de salida, otra capa densa de dos neuronas con una activación _SoftMax_:"
      ],
      "metadata": {
        "id": "rj3H9eM1JCN2"
      },
      "id": "rj3H9eM1JCN2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dbf4354b",
      "metadata": {
        "id": "dbf4354b"
      },
      "outputs": [],
      "source": [
        "model_seq.add(\n",
        "    tf.keras.layers.Dense(\n",
        "        units=2,\n",
        "        activation=\"softmax\")\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y podemos vizualizar el modelo con la función `summary()`"
      ],
      "metadata": {
        "id": "JZtYmO1RJWbZ"
      },
      "id": "JZtYmO1RJWbZ"
    },
    {
      "cell_type": "code",
      "source": [
        "model_seq.summary()"
      ],
      "metadata": {
        "id": "Cv-BvX0FJX_L"
      },
      "id": "Cv-BvX0FJX_L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "También podemos usar la función `plot_model` para tener una vista diagramática del modelo. La función recibe como argumentos el modelo, y la variable Booleana `show_shapes` para mostrar el tamaño de la entrada y la salida de cada capa:"
      ],
      "metadata": {
        "id": "1YI_SH2-tFFl"
      },
      "id": "1YI_SH2-tFFl"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model_seq, show_shapes=True)"
      ],
      "metadata": {
        "id": "v-vlGH18sgre"
      },
      "id": "v-vlGH18sgre",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3b8099d4",
      "metadata": {
        "id": "3b8099d4"
      },
      "source": [
        "### 2.4.2 Funcional\n",
        "\n",
        "`tf.keras.models.Model` se trata del API funcional de `keras` y nos permite definir modelos de una forma más flexible, es decir, tenemos más control sobre la conexión entre capas, que ya no debe ser necesariamente secuencial. Esto permite por ejemplo que hagamos saltos entre capas, concatenación u operaciones matemáticas entre capas, o construir modelos con múltiples entradas y salidas. Por ejemplo, si queremos concatenar la salida de dos capas, podemos usar la función `tf.concat`, que recibe como argumento una lista con las capas que queremos concatenar.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vamos a construir la siguiente arquitectura de red neuronal como ejemplo mientras discutimos cada componente de _Keras_ :**"
      ],
      "metadata": {
        "id": "NyfAuaSolFnU"
      },
      "id": "NyfAuaSolFnU"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1tD6dZPx45BjkGwXMxpsjQTe4fvmfnEB2\" alt =\"Ejemplo de diagrama que ilustra la arquitectura de red a construir\" width=\"60%\"></center>"
      ],
      "metadata": {
        "id": "8v74hNSOUCqQ"
      },
      "id": "8v74hNSOUCqQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comencemos creando las capas de la arquitectura:"
      ],
      "metadata": {
        "id": "wIUg03O5lQsm"
      },
      "id": "wIUg03O5lQsm"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos las dos neuronas de entrada\n",
        "inp_layer = tf.keras.layers.Input(shape=(2,))\n",
        "# Definimos la neurona intermedia\n",
        "int_layer = tf.keras.layers.Dense(units=1,\n",
        "                                  activation=\"tanh\")\n",
        "# Definimos la neurona de salida\n",
        "out_layer = tf.keras.layers.Dense(units=1,\n",
        "                                  activation=\"sigmoid\")"
      ],
      "metadata": {
        "id": "j6nYZYWhlRMo"
      },
      "id": "j6nYZYWhlRMo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que en la capa `int_layer` usamos la activación _Tangente Hiperbólico_, que devuelve siempre un valor entre $-1$ y $1$. Aparte de introducir una *no-linealidad* en el modelo, esta activación permite controlar la magnitud de los valores que salen de las neuronas sobre las que actúa :\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1z-mSU2ueBCK2vAepyAkcKXmxR8iy5pe9\" alt = \"Gráfica de la Tangente hiperbólica\" width=\"60%\"></center>\n"
      ],
      "metadata": {
        "id": "lalR2089F3UI"
      },
      "id": "lalR2089F3UI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sigamos construyendo el modelo; un aspecto muy importante del modelo funcional es que nos permite hacer conexiones explicitas entre las capas :"
      ],
      "metadata": {
        "id": "j3UUGZ-yu82B"
      },
      "id": "j3UUGZ-yu82B"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_V67xqHvqsY4"
      },
      "outputs": [],
      "source": [
        "# Conectamos la neurona intermedia con la capa de entrada\n",
        "int_out = int_layer(inp_layer)\n",
        "# Conectamos la capa de salida con la neurona intermedia y la capa de entrada\n",
        "y_prime = out_layer(\n",
        "        tf.keras.layers.Concatenate(axis=1)([inp_layer, int_out],)\n",
        "        )"
      ],
      "id": "_V67xqHvqsY4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y luego de eso definimos el modelo con `tf.keras.models.Model()`, función que recibe dos argumentos:\n",
        "\n",
        "\n",
        "*   `inputs`: una lista con las capas de entrada, en este caso solamente `inp_layer`.\n",
        "*   `outputs`: una lista con las capas de salida, en este caso `y_prime`.\n",
        "\n"
      ],
      "metadata": {
        "id": "JibYfuhEvNGK"
      },
      "id": "JibYfuhEvNGK"
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos el modelo\n",
        "model_fun = tf.keras.models.Model(\n",
        "        inputs=[inp_layer],\n",
        "        outputs=[y_prime]\n",
        "        )\n",
        "# Y de nuevo podemos visualizarlo:\n",
        "model_fun.summary()"
      ],
      "metadata": {
        "id": "eRS6o-UFvIiS"
      },
      "id": "eRS6o-UFvIiS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hagamos el diagrama y comparémoslo con el modelo secuencial de la Sección 2.4.1:"
      ],
      "metadata": {
        "id": "U0AdOo3WtrHD"
      },
      "id": "U0AdOo3WtrHD"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model_fun, show_shapes=True)"
      ],
      "metadata": {
        "id": "6jPRnTGqsvLZ"
      },
      "id": "6jPRnTGqsvLZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ffffd0e9",
      "metadata": {
        "id": "ffffd0e9"
      },
      "source": [
        "En conclusión, la forma funcional permite crear modelos con arquitecturas más complejas, donde el flujo de la información no es necesariamente lineal.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=18fKRa1yjrj2_tRD_g9T-fVM74KwygBFD\" alt =\"Esquema ilustrativo del funcionamiento de un modelo secuencial y un modelo funcional\" width=\"60%\"></center>\n",
        "\n",
        "Los modelos de `keras` tienen métodos como `summary`, `get_config` o `to_json` que proveen información básica de la arquitectura (número de capas, número de parámetros, conexiones, entre otros). Además, contienen métodos como `load_weights` y `save_weights` para cargar y guardar un modelo en formato hdf5.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "999d4302",
      "metadata": {
        "id": "999d4302"
      },
      "source": [
        "## 2.5 Optimizadores\n",
        "\n",
        "`keras` requiere un optimizador para entrenar la red neuronal. Podemos utilizar los optimizadores ya implementados, como:\n",
        "\n",
        "* _Stochastic Gradient Descent_ (`tf.keras.optimizers.SGD`).\n",
        "* _Root Mean Square prop_ (`tf.keras.optimizers.RMSprop`)\n",
        "* _Adaptative Gradient Algorithm_ (`tf.keras.optimizers.Adagrad`)\n",
        "* _Adaptive Moment estimation_ (`tf.keras.optimizers.Adam`)\n",
        "* _Adaptive Learning Rate Method_ (`tf.keras.optimizers.Adadelta`)\n",
        "\n",
        "Para nuestro caso, utilizaremos un optimizador Adam. Todos los optimizadores deben recibir al menos un parámetro: `learning_rate`, es decir, la tasa de aprendizaje, o el tamaño de paso en la actualización de las variables que se hace en cada iteración."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b7f3988d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b7f3988d"
      },
      "outputs": [],
      "source": [
        "opt = tf.keras.optimizers.Adam(learning_rate=1e-1)\n",
        "opt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db6f712d",
      "metadata": {
        "id": "db6f712d"
      },
      "source": [
        "## 2.6 Funciones de pérdida\n",
        "\n",
        "Como vimos anteriormente, el entrenamiento de una red neuronal requiere definir una **función de pérdida diferenciable**. Esta función de pérdida debe recibir como argumento las etiquetas reales de los datos (`y_true`), y las etiquetas predichas por el modelo (`y_pred`). _Keras_ permite usar los mismos tipos de pérdida que hemos venido manejando :\n",
        "\n",
        "|Nombre|Expresión|Tensorflow|Descripción|\n",
        "|---|---|---|---|\n",
        "|<img width=100/>|<img width=250/>|<img width=100/>|<img width=50/>|\n",
        "|Mean Squared Error|$\\sum_{i=1}^{N}(y_i-\\widetilde{y}_i)^2$| `tf.losses.mse`|Comúnmente usada en problemas de <br/> regresión $y_i\\in\\mathbb{R}$|\n",
        "|Binary Crossentropy|$\\sum_{i=1}^{N}y_i\\log{\\widetilde{y}_i}+(1-y_i)\\log{1-\\widetilde{y}_i}$| `tf.losses.binary_crossentropy`|Comúnmente usada en problemas de <br/> clasificación $y_i\\in\\{0,1\\}$. Generalmente<br/> se utiliza junto con una salida con activación<br/>sigmoid|\n",
        "|Categorical Crossentropy|$\\sum_{i=1}^{N}\\sum_{j=1}^{l}Y[i,j]\\log{\\widetilde{Y}[i,j]}$| `tf.losses.categorical_crossentropy`|Comúnmente usada en problemas de <br/> clasificación $y_i\\in\\{0,1\\}$. Generalmente<br/> se utiliza junto con una salida con activación<br/>sofrmax|\n",
        "\n",
        "También, se puede definir una función de pérdida personalizada (siempre y cuando sea derivable) en `tensorflow`, por ejemplo:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c1bd14b9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c1bd14b9"
      },
      "outputs": [],
      "source": [
        "# Error cuadrático medio\n",
        "def custom_mse(y_true, y_pred):\n",
        "    return tf.reduce_mean((y_true - y_pred)**2)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos cómo funciona con un ejemplo aleatorio: tenemos `y_true` y `y_pred`, dos vectores unidimensionales de tamaño 100 cada uno. Es decir, como si tuviéramos las predicciones del modelo para 100 muestras junto con las etiquetas reales. Por tanto `(y_true - y_pred)**2` será también un vector unidimensional de tamaño 100, correspondiente al error cuadrático de las 100 muestras. Como lo que queremos es la media de ese error, tenemos que usar `tf.reduce_mean` para calcular el promedio de esos 100 errores:"
      ],
      "metadata": {
        "id": "WG-zXu3uP1Nh"
      },
      "id": "WG-zXu3uP1Nh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75e1479c",
      "metadata": {
        "id": "75e1479c"
      },
      "outputs": [],
      "source": [
        "# Definimos dos variables para evaluar la función de pérdida\n",
        "y_true = tf.random.uniform(shape=(100,), minval=0, maxval=10)\n",
        "y_pred = tf.random.uniform(shape=(100,), minval=0, maxval=10)\n",
        "\n",
        "# Evalúamos nuestra función de error cuadrático medio\n",
        "custom_mse(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cbed970",
      "metadata": {
        "id": "2cbed970"
      },
      "outputs": [],
      "source": [
        "# Evaluamos el MSE de keras\n",
        "tf.losses.mse(y_true, y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a35c212",
      "metadata": {
        "id": "8a35c212"
      },
      "source": [
        "## 2.7 Compilación\n",
        "\n",
        "Una vez el definido el modelo, este se debe compilar con la función `compile`. Para ello, debemos especificar **la función de pérdida** (`loss`), el **optimizador** (`optimizer`) y una lista de **métricas** opcionales (`metrics`) que pueden ser evaluadas durante el entrenamiento junto con la función de pérdida. En nuestro caso vamos a usar un *binary cross-entropy* como función de pérdida y vamos a monitorear el *accuracy* (`acc`) del modelo durante el entrenamiento :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7b94545a",
      "metadata": {
        "id": "7b94545a"
      },
      "outputs": [],
      "source": [
        "# Compilamos\n",
        "model_fun.compile(loss=tf.losses.binary_crossentropy,\n",
        "              optimizer=opt,\n",
        "              metrics=['acc'])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 *Callbacks*\n",
        "\n",
        "Cuando estamos entrenando un modelo **no sabemos cuántas iteraciones sobre todo el conjunto de datos vamos a necesitar para llegar al valor óptimo** de la función de pérdida. Puede suceder que desde el principio se defina que se va a iterar durante 100 `epochs`, pero el modelo necesite solo 10. Si esto sucede y no hacemos nada al respecto, el modelo continuará el proceso de entrenamiento por lo que pasaremos por alto el mejor modelo y muy seguramente después de completar las 100 `epochs`, acabaremos con un modelo sobre-entrenado y además: **habremos perdido mucho tiempo y muchos recursos**. Para prevenir esto es necesario detener el entrenamiento cuando ya veamos que no vamos a mejorar más, y además necesitamos guardar el modelo.\n",
        "\n",
        "**Keras** nos provee de una serie de objetos llamados [***Callbacks***](https://keras.io/api/callbacks/), que pueden ayudarnos a realizar este tipo de acciones y más durante el entrenamiento. Vamos a ver cómo usar dos de estos objetos: `ModelCheckpoint` y `EarlyStopping`.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1mQYjjc8NT_d-C2rMauDhY6CW6SbO8cJP\" alt = \"Diagrama ilustrativo que muestra cuando se tiene un subajuste y un sobreajuste\" width=\"60%\"></center>\n"
      ],
      "metadata": {
        "id": "Tp_T2gtHGWCv"
      },
      "id": "Tp_T2gtHGWCv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`ModelCheckpoint`**: Sirve para guardar el modelo y sus pesos en algún punto específico del entrenamiento, o guardar sucesivamente y con cierta frecuencia los modelos resultantes del entrenamiento. A continuación, vamos a definir este *callback* para que nos permita guardar el mejor modelo que suceda durante el entrenamiento. Pero _**¿cómo saber cuál es el mejor modelo?**_ Por lo general, el mejor modelo es el que presente el menor valor de la función de pérdida en el conjunto de validación; sin embargo, nosotros podemos definir a discreción qué es lo *mejor* según lo que estemos buscando; para esto usamos los parámetros `monitor` y `mode`.\n",
        "  - Además tenemos que definir en el `filepath`, una dirección dentro de nuestro sistema de archivos para que ahí se almacene lo que queremos guardar. Podemos guardar la arquitectura del modelo junto con los pesos, o solamente estos últimos; para esto usamos el parámetro `save_weights_only`. Asimismo, podemos guardar todos los modelos `epoch` tras `epoch`, o solamente el mejor modelo, según lo definamos en el parámetro `save_best_only`.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tXpQfk6nJsuf"
      },
      "id": "tXpQfk6nJsuf"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, definiremos que vamos a guardar propiamente. En esta ocasión guardaremos solo los pesos del modelo en el archivo `'best_weights.weights.h5'` cuando el *accuracy* del modelo en el conjunto de entrenamiento sea máximo (es decir, `monitor=\"acc\"` y `mode=\"max\"`).   "
      ],
      "metadata": {
        "id": "rvq2R4F9m2Os"
      },
      "id": "rvq2R4F9m2Os"
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                  filepath='best_weights.weights.h5', # Path del archivo donde se guardarán los pesos o el modelo.\n",
        "                  monitor=\"acc\",              # La métrica que se va a monitorear.\n",
        "                  mode=\"max\",                 # Se quiere guardar el modelo que reporte el accuracy máximo: max.\n",
        "                  save_best_only=True,        # Si se define True, entonces solo se guarda el mejor modelo.\n",
        "                  save_weights_only=True      # Si se define True, solo se guardan los pesos, no la arquitectura.\n",
        "              )"
      ],
      "metadata": {
        "id": "BGdiaT2uM8Dz"
      },
      "id": "BGdiaT2uM8Dz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **`EarlyStopping`**: Sirve para detener el proceso de entrenamiento una vez que hemos alcanzado el mejor modelo. Igual que con `ModelCheckpoint`, tenemos que definir qué es el mejor modelo, respecto a qué conjunto (entrenamiento o validación) y con qué métrica medirlo. Además, tenemos que definir el parámetro `patience`, que es la paciencia que va a tener antes de detener el proceso.\n",
        "  - Por ejemplo, si `patience=5`, entonces el entrenamiento se detendrá si la métrica que estamos monitoreando no ha mejorado en las últimas 5 `epochs`. También podemos fijar que, una vez terminado el entrenamiento, se restablezcan los pesos del mejor modelo encontrado definiendo el parámetro `restore_best_weights` como **`True`**.\n"
      ],
      "metadata": {
        "id": "qZoIRwK5M5OB"
      },
      "id": "qZoIRwK5M5OB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, vamos a establecer que si después de 50 `epochs` el *accuracy* no mejora, el entrenamiento se detenga y se carguen al modelo los mejores pesos encontrados:"
      ],
      "metadata": {
        "id": "aeTVVZw1nyYy"
      },
      "id": "aeTVVZw1nyYy"
    },
    {
      "cell_type": "code",
      "source": [
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"acc\",            # La métrica que se va a monitorear.\n",
        "                patience=50,              # Si después de 50 epochs la métrica no mejora, se detiene el entrenamiento.\n",
        "                mode=\"max\",               # Se quiere guardar el modelo que reporte el accuracy máximo: max.\n",
        "                restore_best_weights=True # Si True, automaticamente se cargan al modelo los mejores pesos.\n",
        "            )"
      ],
      "metadata": {
        "id": "gY9caRDNM36q"
      },
      "id": "gY9caRDNM36q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una vez definidos estos *callbacks* tenemos que pasarlos como una lista dentro argumentos de la función de entrenamiento del modelo: `callbacks=[checkpoint, stopping]`."
      ],
      "metadata": {
        "id": "BFQS5bxnTRq_"
      },
      "id": "BFQS5bxnTRq_"
    },
    {
      "cell_type": "markdown",
      "id": "233f2ac8",
      "metadata": {
        "id": "233f2ac8"
      },
      "source": [
        "## 2.9 Entrenamiento\n",
        "\n",
        "Para entrenar el modelo utilizamos la función `fit()`, la cual puede usarse de la siguiente forma:\n",
        "\n",
        "* `model.fit(x=X, y=Y, epochs, batch_size, callbacks)`,\n",
        "\n",
        "y se puede utilizar como argumento `X` y `y` arreglos de `numpy` o tensores siempre y cuando tengamos datasets pequeños que quepan en memoria.\n",
        "\n",
        "Un enfoque típico en el entrenamiento de redes neuronales es el entrenamiento por *batch* donde, en cada época (iteración en la que la red ve todo el dataset) la matriz $\\mathbf{X}$ se divide (a nivel de observaciones) en $k$ matrices, cada una con un número de observaciones conocido como *batch_size*:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1U1Y7H4PEGI-0Grj6h4g-hepwTRuSL8e8\" alt=\"Diagrama explicativo que ilustra lo que es una Epoch, un Batch y una Iteración\" width=\"80%\"></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, vamos a entrenar el modelo con un `bathc_size` de 1 elemento, durante máximo 200 `epochs`, usando los `callbacks` que definimos anteriormente. El método `fit` retorna un objeto `History` que guarda toda la información del entrenamiento a manera de diccionario. Vamos a guardar todos estos datos de entrenamiento en `hist` para poder hacer rápidamente gráficas del comportamiento de la función de pérdida."
      ],
      "metadata": {
        "id": "GuYbRvgoURng"
      },
      "id": "GuYbRvgoURng"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a6eefcb",
      "metadata": {
        "id": "7a6eefcb"
      },
      "outputs": [],
      "source": [
        "hist = model_fun.fit(x=X,\n",
        "                 y=y_XOR,\n",
        "                 epochs=200,\n",
        "                 batch_size=1,\n",
        "                 callbacks=[checkpoint, stopping])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a graficar el comportamiento de la función de pérdida y del *accuracy* durante entrenamiento. Esta información se encuentra en un objeto de tipo `keras.callbacks.History` que tiene un atributo `history` el cual corresponde a un diccionario con los valores que monitoreó durante el entrenamiento:"
      ],
      "metadata": {
        "id": "v8gb7KDF1kx5"
      },
      "id": "v8gb7KDF1kx5"
    },
    {
      "cell_type": "code",
      "source": [
        "hist.history.keys()"
      ],
      "metadata": {
        "id": "oYAI4NOLDDql"
      },
      "id": "oYAI4NOLDDql",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso vemos que tiene la información del loss así como la información del accuracy sobre el conjunto de entrenamiento."
      ],
      "metadata": {
        "id": "521trrZRRsQq"
      },
      "id": "521trrZRRsQq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77c6afa6",
      "metadata": {
        "id": "77c6afa6"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history[\"loss\"])\n",
        "plt.title(\"Training Loss\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Pérdida\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history[\"acc\"])\n",
        "plt.title(\"Training Accuracy\")\n",
        "plt.xlabel(\"Época\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "metadata": {
        "id": "YZ2K8CDy1S-J"
      },
      "id": "YZ2K8CDy1S-J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "f3be9ab3",
      "metadata": {
        "id": "f3be9ab3"
      },
      "source": [
        "Como se puede ver en las gráficas, el mejor modelo según el *accuracy* se obtiene alrededor de la `epoch` 30 o 40, y como teníamos una paciencia de 50 `epochs`, se hizo el entrenamiento durante alrededor de 80 o 90 `epochs` en total. En ese punto el `EarlyStopping` actúa y detiene la iteración, evitando que lleguemos a 200 `epochs`.\n",
        "\n",
        "> **Nota: el número exacto de epochs en la descripción anterior puede cambiar cada vez que se ejecute de nuevo el ejemplo.**\n",
        "\n",
        "Veamos a continuación los pesos que aprendió el modelo:\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_fun.layers"
      ],
      "metadata": {
        "id": "n2kfoEoZAtT0"
      },
      "id": "n2kfoEoZAtT0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "07c552f4",
      "metadata": {
        "id": "07c552f4"
      },
      "outputs": [],
      "source": [
        "# Neurona intermedia\n",
        "model_fun.layers[1].weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3bd3c66c",
      "metadata": {
        "id": "3bd3c66c"
      },
      "outputs": [],
      "source": [
        "# Neurona de salida\n",
        "model_fun.layers[3].weights"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "628ca790",
      "metadata": {
        "id": "628ca790"
      },
      "source": [
        "## 2.10 Evaluación\n",
        "\n",
        "Podemos ver la región de decisión :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf958d26",
      "metadata": {
        "id": "bf958d26"
      },
      "outputs": [],
      "source": [
        "print_labels(y_XOR, model_fun(X))\n",
        "plot_decision_region(X, y_XOR, model_fun)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y también podemos usar la función `evaluate` sobre el conjunto que queramos probar. `evaluate` recibe dos argumentos:\n",
        "\n",
        "\n",
        "*   `x`: las observaciones de prueba.\n",
        "*   `y`: las etiquetas reales de los casos de prueba.\n",
        "\n",
        "\n",
        "Y nos va a devolver el valor de las métricas que definimos en la compilación, junto con el valor de la función de pérdida, todo evaluado sobre el conjunto que pasemos en `x`:"
      ],
      "metadata": {
        "id": "S8hm7PV8yjqH"
      },
      "id": "S8hm7PV8yjqH"
    },
    {
      "cell_type": "code",
      "source": [
        "model_fun.evaluate(x=X,\n",
        "               y=y_XOR\n",
        "               )"
      ],
      "metadata": {
        "id": "7_lRqNiWzIW1"
      },
      "id": "7_lRqNiWzIW1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos entonces un accuracy de `1.0`.\n",
        "\n",
        "También podemos usar la función `predict`, que calcula la predicción del modelo sobre un conjunto de observaciones específico. Por ejemplo:"
      ],
      "metadata": {
        "id": "EErQtqBpzXO5"
      },
      "id": "EErQtqBpzXO5"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model_fun.predict(x=X)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "Ppk5pp9nz95c"
      },
      "id": "Ppk5pp9nz95c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que las predicciones de este modelo para cada caso no son completamente binarias, sino un número entre 0 y 1. Recuerde que la salida del modelo es una sola neurona. La asignación final de la etiqueta se hace de acuerdo con si la predicción está más cerca de 0 o de 1. Entonces nos podemos valer de funciones de numpy para hacer esta aproximación, y calcular las etiquetas concretas predichas por el modelo:"
      ],
      "metadata": {
        "id": "seto31hj0dZj"
      },
      "id": "seto31hj0dZj"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = np.rint(y_pred)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "id": "l60k-6_U0c3m"
      },
      "id": "l60k-6_U0c3m",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Con esto podemos usar funciones de _Tensorflow_ o de _Scikit-Learn_ para calcular la métrica que queramos. Por ejemplo, podemos calcular un `classification_report` de `sklearn`:"
      ],
      "metadata": {
        "id": "l8hCvhy51alQ"
      },
      "id": "l8hCvhy51alQ"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "print(classification_report(y_XOR, y_pred))"
      ],
      "metadata": {
        "id": "Dwvkzqj41stZ"
      },
      "id": "Dwvkzqj41stZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "O podemos también calcular el _accuracy_ utilizando TensorFlow, usando la clase `tf.keras.metrics.Accuracy`. Para esto hay que definir primero un objeto de la clase, y luego aplicar el método `update_state`, que calcula en efecto la métrica a partir de las etiquetas reales y las predichas por el modelo:"
      ],
      "metadata": {
        "id": "TGcH8LJBDmtY"
      },
      "id": "TGcH8LJBDmtY"
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear el objeto\n",
        "accuracy = tf.keras.metrics.Accuracy()\n",
        "# Actualizar la métrica con las etiquetas reales y las predichas por el modelo\n",
        "accuracy.update_state(y_XOR, y_pred)\n",
        "# Calcular el acuracy\n",
        "acc = accuracy.result().numpy()\n",
        "# Imprimir\n",
        "print(\"Accuracy:\", acc)"
      ],
      "metadata": {
        "id": "Vg_LahHt_Z4s"
      },
      "id": "Vg_LahHt_Z4s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "c29d735d",
      "metadata": {
        "id": "c29d735d"
      },
      "source": [
        "# **3. Ejemplo clasificación binaria**\n",
        "---\n",
        "\n",
        "Ahora, veamos un ejemplo práctico de la aplicación de una red neuronal. Para ello, utilizaremos un _Dataset_ que contiene características extraídas de imágenes de billetes falsos y auténticos.\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1-ZUhQlJ-c9HYq5WS6IkWEE0kKouc-_9U\" width=\"50%\">\n",
        "</center>\n",
        "\n",
        "Se trata de un dataset público conocido como [*banknote authentication Data Set*](https://archive.ics.uci.edu/ml/datasets/banknote+authentication) y contiene cuatro estadísticas de cada imagen y una etiqueta binaria. Comencemos en primer lugar cargando el _Dataset_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91feb03",
      "metadata": {
        "id": "c91feb03"
      },
      "outputs": [],
      "source": [
        "# Utilizamos pandas para leer el csv con el dataset\n",
        "!wget = https://drive.google.com/uc?id=10p8a-2FWG7uJkYo8RIht78KVRLuj4OUu\n",
        "df_bank_note = pd.read_csv('/content/uc?id=10p8a-2FWG7uJkYo8RIht78KVRLuj4OUu', header=None)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_bank_note.head()"
      ],
      "metadata": {
        "id": "v49V3EYTB8AP"
      },
      "id": "v49V3EYTB8AP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "597a7f69",
      "metadata": {
        "id": "597a7f69"
      },
      "outputs": [],
      "source": [
        "# Separamos las características de las etiquetas\n",
        "X = np.array(df_bank_note.values[1:,:-1],np.float64)\n",
        "y = np.array(df_bank_note.values[1:,-1],np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37765a81",
      "metadata": {
        "id": "37765a81"
      },
      "outputs": [],
      "source": [
        "# Veamos X y su tamaño\n",
        "X, X.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0533c344",
      "metadata": {
        "id": "0533c344"
      },
      "outputs": [],
      "source": [
        "# Veamos y su tamaño\n",
        "y, y.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos entonces 1371 muestras de dimensión 4."
      ],
      "metadata": {
        "id": "QW38CMOnuPN9"
      },
      "id": "QW38CMOnuPN9"
    },
    {
      "cell_type": "markdown",
      "id": "25e44e84",
      "metadata": {
        "id": "25e44e84"
      },
      "source": [
        "## 3.1 Entrenamiento, validación y prueba\n",
        "\n",
        "La validación o también conocida como _**evaluación fuera de muestra**_ es una técnica de validación estadística de modelos que busca evaluar cómo los resultados de un análisis estadístico se generalizan a un conjunto independiente de datos. Primordialmente, consiste en dividir el conjunto de datos en un conjunto conocido (entrenamiento) que es usado para ajustar los parámetros del modelo y un conjunto de datos desconocido (prueba) que es utilizado para evaluar el desempeño del modelo en datos diferentes a los del entrenamiento.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1hmlQPzhL4zcwTVGQJZ1psrv94GuqsGfr\" alt =\"Diagrama ilustrativo de la composición de un Dataset\" width=\"60%\"></center>\n",
        "\n",
        "La división de los datos en estos dos conjuntos se debe hacer de forma estratificada, es decir, que el conjunto de entrenamiento y el de prueba tengan las mismas propiedades estadísticas para hacer válida una comparación.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Vamos a realizar estas particiones del dataset:"
      ],
      "metadata": {
        "id": "lh4kPq81pNFU"
      },
      "id": "lh4kPq81pNFU"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "439d44de",
      "metadata": {
        "id": "439d44de"
      },
      "outputs": [],
      "source": [
        "# Dividimos el dataset en: 60% para entrenamiento y 40% para prueba\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.4, stratify=y\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Verifiquemos el tamaño de cada partición:"
      ],
      "metadata": {
        "id": "somAPxwAuAEZ"
      },
      "id": "somAPxwAuAEZ"
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "id": "CjiFk40AuXSg"
      },
      "id": "CjiFk40AuXSg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test.shape"
      ],
      "metadata": {
        "id": "pC4bJlkquZrH"
      },
      "id": "pC4bJlkquZrH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A diferencia del conjunto de prueba, en el conjunto de validación no necesitamos definirlo explícitamente (aunque también es posible). _Keras_ nos ofrece un parámetro dentro de la función `fit`, llamado `validation_split`, que se encarga de reservar una porción de forma aleatoria (a una razón definida) del conjunto de entrenamiento como conjunto de validación, para evaluar en este pedazo el desempeño del modelo al final de cada `epoch`. El desempeño del modelo se monitorea entonces sobre ese conjunto de validación, es decir, el mejor modelo se escoge según el mejor desempeño que se obtenga en el conjunto de validación."
      ],
      "metadata": {
        "id": "BfpNjXxDEaxD"
      },
      "id": "BfpNjXxDEaxD"
    },
    {
      "cell_type": "markdown",
      "id": "656eed2a",
      "metadata": {
        "id": "656eed2a"
      },
      "source": [
        "## 3.2 Modelo\n",
        "\n",
        "Ahora, definiremos la arquitectura de nuestra red neuronal, como se trata de un problema de clasificación binaria utilizaremos una salida de tipo _sigmoide_ y como función de pérdida la entropía cruzada binaria (similar a la regresión logística)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43e35353",
      "metadata": {
        "id": "43e35353"
      },
      "outputs": [],
      "source": [
        "# Definimos el modelo\n",
        "model = tf.keras.models.Sequential()\n",
        "model.add(\n",
        "        tf.keras.Input(shape=(4, )) # Definir forma como primera capa\n",
        "        )\n",
        "model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            units=32, activation=tf.nn.relu, name=\"int_layer\"\n",
        "            )\n",
        "        )\n",
        "model.add(\n",
        "        tf.keras.layers.Dense(\n",
        "            units=1, activation=tf.nn.sigmoid,\n",
        "            name=\"out\"\n",
        "            )\n",
        "        )\n",
        "# Compilamos el modelo\n",
        "model.compile(\n",
        "        loss=tf.losses.binary_crossentropy,\n",
        "        optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "        metrics=[\"acc\"]\n",
        "        )\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el diagrama:"
      ],
      "metadata": {
        "id": "y_wIL_Xkt1DS"
      },
      "id": "y_wIL_Xkt1DS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd66e39b",
      "metadata": {
        "id": "fd66e39b"
      },
      "outputs": [],
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True, )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9fc8b54",
      "metadata": {
        "id": "a9fc8b54"
      },
      "source": [
        "## 3.3 Entrenamiento\n",
        "\n",
        "Ahora, definimos los *callbacks* y entrenamos el modelo. Note que definimos `validation_split=0.2`. Es decir, en cada `epoch` se reserva el 20% del conjunto de datos de entrenamiento para evaluar al final en ese conjunto la función de pérdida y las métricas que se definieron en la compilación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6cbfcf34",
      "metadata": {
        "id": "6cbfcf34"
      },
      "outputs": [],
      "source": [
        "# Definimos Callbacks:\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "                  filepath='best_weights_bank.weights.h5',\n",
        "                  monitor=\"val_acc\",\n",
        "                  verbose=1,\n",
        "                  save_best_only=True,\n",
        "                  save_weights_only=True,\n",
        "                  mode=\"max\",\n",
        "            )\n",
        "stopping = tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_acc\",\n",
        "                patience=10,\n",
        "                verbose=1,\n",
        "                mode=\"max\",\n",
        "                restore_best_weights=True,\n",
        "            )\n",
        "# Entrenamos el modelo\n",
        "hist = model.fit(\n",
        "        X_train, y_train,\n",
        "        epochs=200,\n",
        "        batch_size=32,\n",
        "        validation_split=0.2,\n",
        "        callbacks=[checkpoint, stopping]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que al final de cada `epoch` tenemos cuatro mediciones: `loss`, `acc`, `val_loss` y `val_acc`, que corresponden a los valores de la función de pérdida y del accuracy medidos en el conjunto de entrenamiento y en el conjunto de validación respectivamente."
      ],
      "metadata": {
        "id": "QaVWkr1SRPHw"
      },
      "id": "QaVWkr1SRPHw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El comportamiento de todas esas mediciones lo podemos entonces graficar :"
      ],
      "metadata": {
        "id": "QAfYOWsupyWx"
      },
      "id": "QAfYOWsupyWx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "675a69f2",
      "metadata": {
        "id": "675a69f2"
      },
      "outputs": [],
      "source": [
        "plt.plot(hist.history[\"loss\"])\n",
        "plt.title(\"Train Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history[\"acc\"])\n",
        "plt.title(\"Train Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "metadata": {
        "id": "meDkfako5k5h"
      },
      "id": "meDkfako5k5h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history[\"val_loss\"])\n",
        "plt.title(\"Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")"
      ],
      "metadata": {
        "id": "0DZFyr5ZEo7x"
      },
      "id": "0DZFyr5ZEo7x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(hist.history[\"val_acc\"])\n",
        "plt.title(\"Validation Accuracy\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Accuracy\")"
      ],
      "metadata": {
        "id": "uUFWhxrp55Ag"
      },
      "id": "uUFWhxrp55Ag",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solo necesitamos alrededor de 30 `epochs` para obtener un buen modelo, según el conjunto de validación. Ahora hay que evaluarlo en el conjunto de prueba."
      ],
      "metadata": {
        "id": "615WfjAp6DlY"
      },
      "id": "615WfjAp6DlY"
    },
    {
      "cell_type": "markdown",
      "id": "99bfda2c",
      "metadata": {
        "id": "99bfda2c"
      },
      "source": [
        "## 3.4 Evaluación\n",
        "\n",
        "Ahora miramos el desempeño del modelo en términos de exactitud :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27fa0626",
      "metadata": {
        "id": "27fa0626"
      },
      "outputs": [],
      "source": [
        "loss, acc = model.evaluate(X_test, y_test)\n",
        "print(\"Pérdida en test:\",loss)\n",
        "print(\"Accuracy en test:\",acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finalmente, Obtuvimos un accuracy en el conjunto de prueba cercano al 100% **¡Buen trabajo!**"
      ],
      "metadata": {
        "id": "nWwsB8v9S0oa"
      },
      "id": "nWwsB8v9S0oa"
    },
    {
      "cell_type": "markdown",
      "id": "7ccb90b5",
      "metadata": {
        "id": "7ccb90b5"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "---\n",
        "Los siguientes enlaces corresponden a sitios en donde encontrará información muy útil para profundizar en el conocimiento de las funcionalidades de *Keras* y la teoría detrás de las redes neuronales:\n",
        "\n",
        "* [*Neural Network Models (Supervised)*](https://scikit-learn.org/stable/modules/neural_networks_supervised.html)\n",
        "* [*Introduction to Keras for Engineers*](https://keras.io/getting_started/intro_to_keras_for_engineers/)\n",
        "* [*Introducción a la Inteligencia Artificial*](https://relopezbriega.github.io/tag/redes-neuronales.html)\n",
        "* [*Introduction to Deep Learning with Keras*](https://towardsdatascience.com/introduction-to-deep-learning-with-keras-17c09e4f0eb2)\n",
        "\n",
        "* _Origen de los íconos_\n",
        "    - Towards Data Science. A Visual Explanation of Gradient Descent Methods [GIF]. https://miro.medium.com/max/1400/1*47skUygd3tWf3yB9A10QHg.gif\n",
        "    - Keras. Keras Logo [PNG]. https://keras.io/img/logo.png\n",
        "    - Flaticon. Blocks free icon [PNG]. https://www.flaticon.com/free-icon/lego_302607\n",
        "    - Flaticon. Blocks free icon [PNG]. https://www.flaticon.com/free-icon/blocks_2466955\n",
        "    - Onfido Product and Tech. Derivative (gradient) for different values of p [GIF]. https://miro.medium.com/max/640/1*fAHaaT3I003D6tjZQzZ32Q.gif\n",
        "    - VSH Solutions. Bank note [JPEG]. https://www.vshsolutions.com/wp-content/uploads/2019/02/bankNote1.jpeg"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "  * [Juan Sebastián Lara](https://http://juselara.com/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](https://www.linkedin.com/in/mario-andres-rodriguez-triana-394806145/).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "metadata": {
        "id": "dbeR1BKnpDB2"
      },
      "id": "dbeR1BKnpDB2"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}