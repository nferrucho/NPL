{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso1/ciclo3/1_bolsa_palabras.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "086624b8",
      "metadata": {
        "id": "086624b8"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1e7ctPi8O3bTQoLZaO9ZZjwGr2r8Z93RS\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "940e67cd",
      "metadata": {
        "id": "940e67cd"
      },
      "source": [
        "# Bolsas de Palabras\n",
        "---\n",
        "\n",
        "En este notebook veremos cómo extraer características a partir de textos por medio de la estrategia de representación más común conocida como bolsa de palabras (Bag-of-Words en inglés). Comenzaremos importando las librerías de ciencia de datos necesarias para manejo de datos, visualización, y manipulación de strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "588f9f6e",
      "metadata": {
        "id": "588f9f6e"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42b6e8c5",
      "metadata": {
        "id": "42b6e8c5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "from IPython.display import display\n",
        "plt.style.use(\"ggplot\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55536b1",
      "metadata": {
        "id": "f55536b1"
      },
      "source": [
        "## **1. Motivación y Definición**\n",
        "---\n",
        "\n",
        "Las representaciones basadas en bolsas de palabras (BoW - *Bag-of-Words*) hacen parte de los métodos de representación más intuitivos y comúnmente usados en procesamiento de lenguaje natural. Consiste en encontrar una distribución de todos los términos $T=\\{t_1,t_2,\\dots,t_m\\}$ que aparecen en un conjunto de documentos $D=\\{d_1, d_2, \\dots, d_n\\}$, es decir, es un problema de estimación de una distribución de probabilidad: $P(T=t_j | D=d_i)$, como se muestra a continuación:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1rGG382UMl3M2jW9EzooibJz-G9GQ3kFu\" width=\"100%\">\n",
        "\n",
        "Normalmente, una bolsa de palabras se calcula por medio de técnicas de conteo, buscando que cada documento esté representado por la frecuencia absoluta $f(t_j, d_i)$ (la frecuencia relativa corresponde a las probabilidades) de cada término $t_j$ en el documento $d_i$, por ejemplo, el siguiente texto:\n",
        "\n",
        "```python\n",
        "\"el perro y el gato no quieren al otro perro\"\n",
        "```\n",
        "\n",
        "Se puede representar numéricamente de la siguiente forma:\n",
        "\n",
        "```python\n",
        "{'el': 2, 'perro': 2, 'y': 1, 'gato': 1, 'no': 1, 'quieren': 1, 'al': 1, 'otro': 1}\n",
        "```\n",
        "\n",
        "Como puede ver, las palabras `\"el\"` y `\"perro\"` que aparecen dos veces terminan representadas por el número `2`, mientras que el resto de palabras que aparecen una única vez se representan con el número `1`.\n",
        "\n",
        "Las bolsas de palabras se suelen usar como **representaciones basadas en histogramas** y nos permiten entrenar modelos de *machine learning*, implementar sistemas de recuperación de información, entre otras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c02807f9",
      "metadata": {
        "id": "c02807f9"
      },
      "source": [
        "## **2. Implementación Paso a Paso**\n",
        "---\n",
        "\n",
        "Primero, veremos cómo podemos calcular una representación de bolsa de palabras directamente en _Python_ con ayuda de `numpy`. Para este ejemplo usaremos el siguiente corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e035ac26",
      "metadata": {
        "id": "e035ac26"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"el cielo es azul y hermoso\",\n",
        "    \"me encanta este cielo azul y hermoso\",\n",
        "    \"el zorro marron rapido salta sobre el perro perezoso\",\n",
        "    \"el desayuno de un rey tiene salchichas jamon huevos tostadas y frijoles\",\n",
        "    \"me encantan los huevos verdes jamon salchichas y bacon\",\n",
        "    \"el zorro marron es rapido y el perro azul es perezoso\",\n",
        "    \"el cielo es muy azul y el cielo es muy hermoso hoy\",\n",
        "    \"el perro es perezoso pero el zorro marron es rapido\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8141918",
      "metadata": {
        "id": "c8141918"
      },
      "source": [
        "Vamos a dividir cada documento del corpus en palabras y a convertirlas en arreglos de `numpy`, de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9660912",
      "metadata": {
        "id": "c9660912"
      },
      "outputs": [],
      "source": [
        "tokens = list(map(lambda doc: np.array(doc.split()), corpus))\n",
        "display(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f219a8c",
      "metadata": {
        "id": "5f219a8c"
      },
      "source": [
        "Para el cálculo de una bolsa de palabras es necesario determinar el vocabulario (palabras únicas dentro del corpus), esto lo podemos hacer con los conjuntos de _Python_ como mostramos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd367e9f",
      "metadata": {
        "id": "bd367e9f"
      },
      "outputs": [],
      "source": [
        "doc_vocab = list(map(set, tokens))\n",
        "display(doc_vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef26fc23",
      "metadata": {
        "id": "ef26fc23"
      },
      "source": [
        "Calculamos las palabras únicas con la unión `|` de los vocabularios de cada documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c812fc6",
      "metadata": {
        "id": "7c812fc6"
      },
      "outputs": [],
      "source": [
        "vocab = set()\n",
        "for words in doc_vocab:\n",
        "    vocab = vocab | words\n",
        "vocab = list(vocab)\n",
        "display(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0de6472c",
      "metadata": {
        "id": "0de6472c"
      },
      "source": [
        "Con este vocabulario, creamos una tabla de referencia (LUT - *Look-Up Table*) para asociar cada palabra del corpus a un índice:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "627c30bb",
      "metadata": {
        "id": "627c30bb"
      },
      "outputs": [],
      "source": [
        "word2int = pd.Series(index=list(vocab), data=np.arange(len(vocab)))\n",
        "display(word2int)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4cb1c100",
      "metadata": {
        "id": "4cb1c100"
      },
      "source": [
        "Con esta tabla de referencia, podemos obtener el índice al que corresponde una palabra en específico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a158798e",
      "metadata": {
        "id": "a158798e"
      },
      "outputs": [],
      "source": [
        "display(word2int[\"huevos\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9bd8638",
      "metadata": {
        "id": "a9bd8638"
      },
      "source": [
        "Para calcular conteos a partir del corpus podemos usar la función de `np.unique` con el parámetro `return_counts`. Por ejemplo, veamos los conteos de palabras para un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98bbb592",
      "metadata": {
        "id": "98bbb592"
      },
      "outputs": [],
      "source": [
        "words, counts = np.unique(tokens[2], return_counts=True)\n",
        "display(words)\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "643c19c7",
      "metadata": {
        "id": "643c19c7"
      },
      "source": [
        "Con esta función podemos estimar la bolsa de palabras para todo el corpus. Primero inicializamos una matriz de tamaño $n$ (número de documentos) por $V$ (tamaño del vocabulario):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48e1c0a5",
      "metadata": {
        "id": "48e1c0a5"
      },
      "outputs": [],
      "source": [
        "bow = np.zeros((len(corpus), len(vocab)))\n",
        "display(bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9dd0d1bc",
      "metadata": {
        "id": "9dd0d1bc"
      },
      "source": [
        "Ahora, podemos asignar los valores de la matriz con las funciones que vimos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90112811",
      "metadata": {
        "id": "90112811"
      },
      "outputs": [],
      "source": [
        "for i in range(len(tokens)):\n",
        "    words, counts = np.unique(tokens[i], return_counts=True)\n",
        "    idx = word2int[words]\n",
        "    bow[i, idx] = counts\n",
        "display(bow)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9bf79a15",
      "metadata": {
        "id": "9bf79a15"
      },
      "source": [
        "Como puede observar, obtenemos una matriz numérica a partir de los textos. Esto es lo que se conoce como **embedding**. Podemos generar una visualización para uno de los documentos del conjunto de datos. Primero veamos el texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77f8bec1",
      "metadata": {
        "id": "77f8bec1"
      },
      "outputs": [],
      "source": [
        "display(corpus[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "275bbfe8",
      "metadata": {
        "id": "275bbfe8"
      },
      "source": [
        "Esto corresponde al siguiente vector de características:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0a61896",
      "metadata": {
        "id": "e0a61896"
      },
      "outputs": [],
      "source": [
        "display(bow[2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdbafc0b",
      "metadata": {
        "id": "cdbafc0b"
      },
      "source": [
        "Veamos un diagrama de barras con los conteos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2241a8",
      "metadata": {
        "id": "5f2241a8"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.bar(vocab, bow[2])\n",
        "ax.set_xlabel(\"Palabras\")\n",
        "ax.set_ylabel(\"Conteo\")\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_rotation(90)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c06badd2",
      "metadata": {
        "id": "c06badd2"
      },
      "source": [
        "## **3. Implementación de sklearn**\n",
        "---\n",
        "\n",
        "Como pudimos ver, las bolsas de palabras se pueden implementar fácilmente en _Python_, no obstante, hay implementaciones más eficientes que permiten manipular corpus más grandes y guardar los elementos de forma eficiente en la memoria.\n",
        "\n",
        "Una de las implementaciones más típicas es el `CountVectorizer` de `sklearn`. Veamos cómo podemos importarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c1858fd",
      "metadata": {
        "id": "1c1858fd"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6101b68b",
      "metadata": {
        "id": "6101b68b"
      },
      "source": [
        "Una de las ventajas de trabajar con el `CountVectorizer` es que funciona como cualquier transformador de `sklearn`, es decir, hace uso de métodos como `fit`, `transform` y `fit_transform`. Además de esto, puede usarse junto a utilidades como `Pipeline` o `ColumnTransformer` del mismo.\n",
        "\n",
        "Veamos cómo podemos calcular una representación de bolsa de palabras con el dataset [Language Detection de Kaggle](https://www.kaggle.com/datasets/basilb2s/language-detection):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8cc3cae",
      "metadata": {
        "id": "f8cc3cae"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv(\"https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u3/language.csv\")\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3fa686f1",
      "metadata": {
        "id": "3fa686f1"
      },
      "source": [
        "Se trata de un conjunto de datos que contiene distintos textos en múltiples idiomas con su correspondiente etiqueta.\n",
        "\n",
        "Veamos cuántos documentos tenemos por idioma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d74c020",
      "metadata": {
        "id": "0d74c020"
      },
      "outputs": [],
      "source": [
        "counts = data.language.value_counts()\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cc63ff9",
      "metadata": {
        "id": "3cc63ff9"
      },
      "source": [
        "Vamos a preprocesar los documentos con una función de preprocesamiento similar a la que usamos en la unidad anterior:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "813a21ea",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "813a21ea"
      },
      "outputs": [],
      "source": [
        "pat = re.compile(r\"[^a-z ]\")\n",
        "spaces = re.compile(r\"\\s{2,}\")\n",
        "def preprocess(text, min_len=1, max_len=23):\n",
        "    # Normalizamos el texto\n",
        "    norm_text = unidecode(text).lower()\n",
        "\n",
        "    # Extraemos tokens\n",
        "    tokens = norm_text.split()\n",
        "\n",
        "    # Filtramos palabras por longitud\n",
        "    filtered_tokens = filter(\n",
        "            lambda token: len(token) >= min_len and len(token) <= max_len,\n",
        "            tokens\n",
        "        )\n",
        "    filtered_text = \" \".join(filtered_tokens)\n",
        "    # Eliminamos caracteres especiales\n",
        "    clean_text = re.sub(pat, \"\", filtered_text)\n",
        "    # Eliminamos espacios duplicados\n",
        "    spaces_text = re.sub(spaces, \" \", clean_text)\n",
        "    return spaces_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "767a2b45",
      "metadata": {
        "id": "767a2b45"
      },
      "source": [
        "Preprocesamos el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d482e89f",
      "metadata": {
        "id": "d482e89f"
      },
      "outputs": [],
      "source": [
        "corpus_prep = data.text.apply(preprocess).to_list()\n",
        "display(corpus_prep[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fd784eeb",
      "metadata": {
        "id": "fd784eeb"
      },
      "source": [
        "Con este corpus, podemos entrenar un vectorizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80136350",
      "metadata": {
        "id": "80136350"
      },
      "outputs": [],
      "source": [
        "vect = CountVectorizer().fit(corpus_prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c022fea4",
      "metadata": {
        "id": "c022fea4"
      },
      "source": [
        "Entre los parámetros más importantes encontramos:\n",
        "\n",
        "- `input`: tipo de entrada de texto, puede ser `\"filename\"` (nombre de un archivo a cargar), `file` (fichero), `content` (variables en _Python_).\n",
        "- `encoding`: tipo de [codificación](https://docs.python.org/3/library/codecs.html#standard-encodings).\n",
        "- `strip_accents`: establece cómo manejar los acentos, puede ser `\"ascii\"` o `\"unicode\"` (como lo hicimos en la función de preprocesamiento.\n",
        "- `lowercase`: determina si el texto se debe convertir a minúsculas.\n",
        "- `preprocessor`: función que permite preprocesar el texto antes de extraer los tokens.\n",
        "- `tokenizer`: función que extrae los tokens a partir de cada documento.\n",
        "- `stopwords`: listado de stopwords para filtrar del texto.\n",
        "- `token_pattern`: expresión regular que describe cada token.\n",
        "- `ngram_range`: una tupla representando secuencias de palabras (hablaremos más en detalle de esto en el notebook de N-grams).\n",
        "- `analyzer`: determina si los tokens se manejarán a nivel de `\"word\"` (palabra) o de `\"char\"` (carácter).\n",
        "- `max_df`: establece la proporción máxima de documentos que contengan un término, es decir, una cuota superior del número de documentos donde puede aparecer un término.\n",
        "- `min_df`: establece la proporción mínima de documentos que contengan un término.\n",
        "- `max_features`: número máximo de términos a considerar en la construcción del vocabulario, se consideran los más frecuentes.\n",
        "- `vocabulary`: vocabulario pre-establecido.\n",
        "- `binary`: establece si se usan conteos `False` o ocurrencias `True`. En el caso de ocurrencias, el valor será 1 para todos los conteos que sean mayores que 0.\n",
        "\n",
        "Como lo puede ver, muchos de los parámetros del `CountVectorizer` simplifican algunas partes del preprocesamiento, no obstante, comúnmente la etapa de preprocesamiento se deja aparte de la extracción de características. Podemos usar el vectorizador ajustado en la celda anterior para extraer la representación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6433d45",
      "metadata": {
        "id": "f6433d45"
      },
      "outputs": [],
      "source": [
        "X = vect.transform(corpus_prep)\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059d84c8",
      "metadata": {
        "id": "059d84c8"
      },
      "source": [
        "Como se puede ver, el resultado no es directamente un arreglo de `numpy`, se obtiene una matriz dispersa de `scipy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c91b03f1",
      "metadata": {
        "id": "c91b03f1"
      },
      "outputs": [],
      "source": [
        "display(type(X))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8e10de9a",
      "metadata": {
        "id": "8e10de9a"
      },
      "source": [
        "`sklearn` lo maneja de esta forma ya que estas representaciones generalmente están llenas de ceros (no todos los términos aparecen en todos los documentos). Las matrices dispersas de `scipy` manejan de forma eficiente este tipo de datos para no consumir tanta memoria RAM.\n",
        "\n",
        "Si quisiéramos acotar el número de características para poder hacer la conversión a un arreglo de `numpy`, podríamos utilizar algunos de los parámetros del método `fit`, así:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0686d75f",
      "metadata": {
        "id": "0686d75f"
      },
      "outputs": [],
      "source": [
        "vect = (\n",
        "    CountVectorizer(max_features=1000, max_df=0.7)\n",
        "    .fit(corpus_prep)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87e68183",
      "metadata": {
        "id": "87e68183"
      },
      "source": [
        "Extraemos la bolsa de palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18fb68ec",
      "metadata": {
        "id": "18fb68ec"
      },
      "outputs": [],
      "source": [
        "X = vect.transform(corpus_prep)\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d279a962",
      "metadata": {
        "id": "d279a962"
      },
      "source": [
        "Veamos cómo obtener un arreglo de `numpy` con la bolsa de características usando el método `toarray`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90d32856",
      "metadata": {
        "id": "90d32856"
      },
      "outputs": [],
      "source": [
        "X_np = X.toarray()\n",
        "display(X_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "773f90ce",
      "metadata": {
        "id": "773f90ce"
      },
      "source": [
        "También podemos extraer el vocabulario calculado (nombres de las columnas en la matriz obtenida) usando el método `get_feature_names_out`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d82edd",
      "metadata": {
        "id": "01d82edd"
      },
      "outputs": [],
      "source": [
        "vocab = vect.get_feature_names_out()\n",
        "display(vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e36735e8",
      "metadata": {
        "id": "e36735e8"
      },
      "source": [
        "Podemos visualizar de mejor forma la bolsa de palabras con `pandas`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e0f51670",
      "metadata": {
        "id": "e0f51670"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=vocab, data=X_np)\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8584ecef",
      "metadata": {
        "id": "8584ecef"
      },
      "source": [
        "## **4. Term-Frequency Inverse-Document-Frequency**\n",
        "---\n",
        "\n",
        "Una de las principales desventajas de las bolsas de palabras es que asumen que todos los términos tienen igual importancia. No obstante, existen dos casos en los que algunos términos deberían tener mayor o menor importancia:\n",
        "\n",
        "1. Palabras comunes que aparecen en todos los documentos y no aportan mucha información para distinguir un documento de otro.\n",
        "2. Términos únicos y poco frecuentes que son sumamente relevantes para distinguir algunos documentos en específico.\n",
        "\n",
        "En este ámbito, surge la necesidad de estrategias que permitan capturar este tipo de relaciones. Una de las soluciones más comunes es _term frequency - inverse document frequency (TF-IDF)_. Se trata de un método que fue propuesto como métrica para la evaluación de resultados en motores de búsqueda y se convirtió en un estándar dentro de los sistemas de recuperación de información.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1tOQfDdO-z2htfu0tYqmwFe1HcTdNJtY_\" width=\"60%\">\n",
        "\n",
        "_TF-IDF_ amplía la idea de bolsas de palabras al ponderar cada término $t_j$ por un peso $w_{j}$ como se muestra a continuación:\n",
        "\n",
        "$$\n",
        "\\text{TFIDF}(t_j, d_i) = \\text{TF}(t_j, d_i) \\cdot w_j\n",
        "$$\n",
        "\n",
        "Con esto, se puede asignar un peso menor a aquellos términos comunes entre documentos y un peso mayor a aquellos términos poco frecuentes. Una de las formas más comunes para determinar estos pesos es con la **frecuencia inversa de documento** $w_j$ que se calcula de la siguiente forma:\n",
        "\n",
        "$$\n",
        "w_j = 1 + \\log{\\frac{n}{1 + \\text{df}(t_j)}}\n",
        "$$\n",
        "\n",
        "Donde $n$ es el número de documentos en el corpus y $\\text{df}(t_j)$ es el número de documentos en los que se encuentra el término $t_j$.\n",
        "\n",
        "Adicionalmente, una extensión de _TF-IDF_ consiste en un cambio de escala de la matriz de términos, con esto, se busca atenuar el impacto que tienen los términos que aparecen muchas veces en un documento. Esto se consigue utilizando _sub-linear scaling_ $wf(\\text{TF}(t_j, d_i))$ y consiste en transformar las ocurrencias a una escala logarítmica donde los valores grandes se ven atenuados:\n",
        "\n",
        "$$\n",
        "wf(\\text{TF}(t_j, d_i)) = \\left\\{\n",
        "  \\begin{array}{cl}\n",
        "      1+\\log{\\text{TF}(t_j, d_i)} & \\mathrm{si\\ } \\text{TF}(t_j, d_i) > 0 \\\\\n",
        "      0 & \\mathrm{si\\ } \\text{TF}(t_j, d_i) \\le 0 \\\\\n",
        "  \\end{array}\n",
        "\\right.\n",
        "$$\n",
        "\n",
        "Finalmente, una versión de _TF-IDF_ con _sub-linear scaling_ se muestra a continuación:\n",
        "\n",
        "$$\n",
        "\\text{TFIDF}(t_j, d_i) = wf(\\text{TF}(t_j, d_i)) w_j\n",
        "$$\n",
        "\n",
        "Podemos extraer características con _TF-IDF_ usando el `TfidfVectorizer` de `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7358861",
      "metadata": {
        "id": "e7358861"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "82545a08",
      "metadata": {
        "id": "82545a08"
      },
      "source": [
        "El `TfidfVectorizer` funciona de una forma muy similar al `CountVectorizer`, sin embargo, incorpora algunos parámetros adicionales:\n",
        "\n",
        "- `norm`: normalización de cada vector de características, puede usar las [normas](https://en.wikipedia.org/wiki/Minkowski_distance) `\"l1\"` o `\"l2\"`\n",
        "- `use_idf`: específica si se usa ponderación _IDF_ ($w_j$).\n",
        "- `smooth_idf`: suaviza el cálculo de los pesos $w_j$ al agregar uno a las frecuencias de documentos.\n",
        "- `sublinear_tf`: especifica si se aplica el reescalamiento logarítmico.\n",
        "\n",
        "Veamos cómo calcular una representación _TF-IDF_, comenzamos definiendo el vectorizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a3b4150",
      "metadata": {
        "id": "8a3b4150"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(\n",
        "    max_features=1000, max_df=0.7, norm=\"l2\",\n",
        "    sublinear_tf=True\n",
        "    ).fit(corpus_prep)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0c2d38d",
      "metadata": {
        "id": "b0c2d38d"
      },
      "source": [
        "Podemos extraer la representación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a55b178",
      "metadata": {
        "id": "0a55b178"
      },
      "outputs": [],
      "source": [
        "X = vect.transform(corpus_prep)\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69409cc2",
      "metadata": {
        "id": "69409cc2"
      },
      "source": [
        "Como se puede ver, el resultado también es una matriz dispersa, podemos convertirla a `numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ffd147e9",
      "metadata": {
        "id": "ffd147e9"
      },
      "outputs": [],
      "source": [
        "X_np = X.toarray()\n",
        "display(X_np)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "971f2f1b",
      "metadata": {
        "id": "971f2f1b"
      },
      "source": [
        "En este caso, los valores no corresponden directamente a conteos (números enteros), sino que obtenemos valores continuos ponderados."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4dbb60e8",
      "metadata": {
        "id": "4dbb60e8"
      },
      "source": [
        "## **5. Recuperación de Información**\n",
        "---\n",
        "\n",
        "Una aplicación muy típica de _TF-IDF_ consiste en la implementación de sistemas de recuperación de información. De forma general un sistema de este tipo busca satisfacer una necesidad de información (consulta o **query**) que pueda tener un usuario, para ello encuentra registros dentro de una base de datos que sean lo más parecidos a la consulta, tal y como se muestra en la siguiente figura:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_RpmT0rqjksu_duWzLnaC52whQY3npwH\" width=\"100%\">\n",
        "\n",
        "Una de las formas más simples y clásicas de implementar este tipo de sistemas es con una representación _TF-IDF_ para obtener una representación numérica y vectorial de los datos y, posteriormente, usar una **medida de similitud** para determinar qué vectores son similares entre sí. Veamos algunos ejemplos de medidas de similitud:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6d91c5f",
      "metadata": {
        "id": "b6d91c5f"
      },
      "source": [
        "### **5.1. Distancia Euclidiana**\n",
        "---\n",
        "\n",
        "Una de las formas más simples para comparar la representación de dos documentos es por medio de la distancia entre los vectores o la distancia Euclidiana.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1NdjcZMdTDwgFTgiDwmKnPXePWgsAtvN_\" width=\"60%\">\n",
        "\n",
        "No obstante, esta métrica no es muy apropiada con representaciones basadas en conteos _TF-IDF_, esto se debe principalmente a que en esta representación importan más los términos comunes que los términos contiguos (que en una representación de bolsa de palabras, no tienen ningún orden) y las magnitudes. Veamos un ejemplo de lo anterior, para ello, definimos tres oraciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da904049",
      "metadata": {
        "id": "da904049"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"nuevo en america\",\n",
        "    \"soy nuevo aqui pero america es un gran lugar para vivir\",\n",
        "    \"la espectrometria es algo nuevo e indispensable\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3532d661",
      "metadata": {
        "id": "3532d661"
      },
      "source": [
        "Representamos los documentos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "578a4ba6",
      "metadata": {
        "id": "578a4ba6"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(\n",
        "        norm=None\n",
        "        )\n",
        "X = vect.fit_transform(corpus).toarray()\n",
        "df = pd.DataFrame(\n",
        "        np.round(X, 2),\n",
        "        columns=vect.get_feature_names_out()\n",
        "        )\n",
        "display(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc08684",
      "metadata": {
        "id": "7bc08684"
      },
      "source": [
        "Ahora, podemos calcular las distancias entre el primer documento y los otros dos, para ver cuál es más similar (entre menor distancia, más parecido):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53e13744",
      "metadata": {
        "id": "53e13744"
      },
      "outputs": [],
      "source": [
        "d1 = np.linalg.norm(X[0] - X[1])\n",
        "d2 = np.linalg.norm(X[0] - X[2])\n",
        "display(\"La distancia euclideana entre el documento 0 y el documento 1 es: {}\".format(d1))\n",
        "display(\"La distancia euclideana entre el documento 0 y el documento 2 es: {}\".format(d2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9171bdb",
      "metadata": {
        "id": "e9171bdb"
      },
      "source": [
        "Como se puede ver, pareciera que el documento 2 es más similar al documento 0 que el documento 1 (lo cual sabemos que es erróneo)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8627ec1a",
      "metadata": {
        "id": "8627ec1a"
      },
      "source": [
        "### **5.2. Similitud Coseno**\n",
        "---\n",
        "\n",
        "Una alternativa es la **[similitud coseno](https://es.wikipedia.org/wiki/Similitud_coseno)**, la cual es más apropiada para representaciones basadas en histogramas como TF-IDF, ya que, es una medida del alineamiento de dos vectores.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1cipIAJzEMrs0YH2U3gz2sfqQLS3XTMjj\" width=\"60%\">\n",
        "\n",
        "$$\n",
        "\\text{cos}(\\mathbf{a}, \\mathbf{b}) = \\frac{\\mathbf{a} \\cdot \\mathbf{b}}{|| \\mathbf{a} ||~||\\mathbf{b}||}\n",
        "$$\n",
        "\n",
        "Veamos el mismo ejemplo de las tres oraciones pero con la similitud coseno, primero la importamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b6cf766",
      "metadata": {
        "id": "6b6cf766"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c1db8a4f",
      "metadata": {
        "id": "c1db8a4f"
      },
      "source": [
        "Veamos la similitud entre el primer documento y los otros dos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d23e27fd",
      "metadata": {
        "id": "d23e27fd"
      },
      "outputs": [],
      "source": [
        "sim = cosine_similarity(X)\n",
        "display(sim)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cbae58e0",
      "metadata": {
        "id": "cbae58e0"
      },
      "source": [
        "Veamos las similitudes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42612466",
      "metadata": {
        "id": "42612466"
      },
      "outputs": [],
      "source": [
        "display(\"La distancia coseno entre el documento 0 y el documento 1 es: {}\".format(sim[0, 1]))\n",
        "display(\"La distancia coseno entre el documento 0 y el documento 2 es: {}\".format(sim[0, 2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3102a25b",
      "metadata": {
        "id": "3102a25b"
      },
      "source": [
        "En este caso, los documentos son más similares entre más alto sea el resultado de la similitud coseno. Por lo tanto, podemos observar que esta medida tiene más sentido que la distancia Euclidiana para comparar representaciones _TF-IDF_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "712d1a33",
      "metadata": {
        "id": "712d1a33"
      },
      "source": [
        "### **5.3. BM25**\n",
        "---\n",
        "\n",
        "_Okapi BM25_ es una técnica de scoring probabilístico para texto que extiende representaciones de tipo _TF-IDF_ para recuperación de información, en especial, la fórmula de ponderación se modifica de acuerdo a tres componentes:\n",
        "\n",
        "- Frecuencia inversa de documento.\n",
        "- Curva de saturación para limitar la frecuencia de términos.\n",
        "- Ponderación por longitud de cada documento.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1EHW4HDFjI1rZxVTTQjw8spH8Cw9Up3FU\" width=\"100%\">\n",
        "\n",
        "Desde _Python_, podemos usar la librería `rank-bm25` para el enfoque de recuperación de información. Veamos cómo instalarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a083cdf4",
      "metadata": {
        "id": "a083cdf4"
      },
      "outputs": [],
      "source": [
        "!pip install rank-bm25"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b531984",
      "metadata": {
        "id": "0b531984"
      },
      "source": [
        "Importamos la clase que nos permitirá usar _BM25_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8577050c",
      "metadata": {
        "id": "8577050c"
      },
      "outputs": [],
      "source": [
        "from rank_bm25 import BM25Okapi"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddc8abf0",
      "metadata": {
        "id": "ddc8abf0"
      },
      "source": [
        "Para este problema, usaremos el texto en español del conjunto de datos que teníamos cargado:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b600da",
      "metadata": {
        "id": "33b600da"
      },
      "outputs": [],
      "source": [
        "corpus_spa = (\n",
        "        data\n",
        "        .query(\"language == 'Spanish'\")\n",
        "        .text.apply(preprocess)\n",
        "        .to_list()\n",
        "        )\n",
        "display(corpus_spa)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eb5ae36",
      "metadata": {
        "id": "1eb5ae36"
      },
      "source": [
        "Para usar _BM25_ debemos obtener una lista con los tokens por cada documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "53b75926",
      "metadata": {
        "id": "53b75926"
      },
      "outputs": [],
      "source": [
        "tokens = list(map(lambda doc: doc.split(), corpus_spa))\n",
        "display(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db8de0eb",
      "metadata": {
        "id": "db8de0eb"
      },
      "source": [
        "Creamos el rankeador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd67945f",
      "metadata": {
        "id": "fd67945f"
      },
      "outputs": [],
      "source": [
        "rank = BM25Okapi(tokens)\n",
        "display(rank)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a4b778",
      "metadata": {
        "id": "d3a4b778"
      },
      "source": [
        "Podemos encontrar el score entre un texto dado y cada documento del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "35201f49",
      "metadata": {
        "id": "35201f49"
      },
      "outputs": [],
      "source": [
        "query = \"la comida estaba muy buena\".split()\n",
        "display(query)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b08fd703",
      "metadata": {
        "id": "b08fd703"
      },
      "source": [
        "Veamos los scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74996517",
      "metadata": {
        "id": "74996517"
      },
      "outputs": [],
      "source": [
        "scores = rank.get_scores(query)\n",
        "display(scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ff9c1b",
      "metadata": {
        "id": "99ff9c1b"
      },
      "source": [
        "Como podemos ver, nos da un score contra cada uno de los documentos del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c964a4a8",
      "metadata": {
        "id": "c964a4a8"
      },
      "outputs": [],
      "source": [
        "display(len(corpus_spa))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3075be9",
      "metadata": {
        "id": "d3075be9"
      },
      "source": [
        "Esto corresponde con la longitud de los scores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "903e1b7b",
      "metadata": {
        "id": "903e1b7b"
      },
      "outputs": [],
      "source": [
        "display(scores.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b7d3fd0",
      "metadata": {
        "id": "1b7d3fd0"
      },
      "source": [
        "Veamos el documento más similar a la consulta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bebf8a01",
      "metadata": {
        "id": "bebf8a01"
      },
      "outputs": [],
      "source": [
        "idx = np.argmax(scores)\n",
        "display(idx)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1190445",
      "metadata": {
        "id": "a1190445"
      },
      "source": [
        "Veamos el documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e9c9e9c8",
      "metadata": {
        "id": "e9c9e9c8"
      },
      "outputs": [],
      "source": [
        "display(corpus_spa[idx])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d29a7aa5",
      "metadata": {
        "id": "d29a7aa5"
      },
      "source": [
        "Como podemos ver, el resultado es **semánticamente** cercano a la consulta.\n",
        "\n",
        "También podemos extraer los $k$ documentos más similares a la consulta con el método `get_top_n`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0dc581f",
      "metadata": {
        "id": "a0dc581f"
      },
      "outputs": [],
      "source": [
        "top_k = rank.get_top_n(query, corpus_spa, n=10)\n",
        "display(top_k)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "611b8df1",
      "metadata": {
        "id": "611b8df1"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [Working with Text Data - sklearn](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html)\n",
        "- [Okapi BM25](https://en.wikipedia.org/wiki/Okapi_BM25)\n",
        "- _Fuente de los íconos_\n",
        "    - Flaticon. Cloud free icon [PNG]. https://www.flaticon.com/free-icon/cloud_8702580\n",
        "    - Flaticon. User free icon [PNG]. https://www.flaticon.com/free-icon/user_1144709\n",
        "    - Flaticon. Query free icon [PNG]. https://www.flaticon.com/free-icon/query_7722230\n",
        "    - Flaticon. Documents File free icon [PNG]. https://www.flaticon.com/free-icon/documents_2954086\n",
        "    - Flaticon. Document free icon [PNG]. https://www.flaticon.com/free-icon/document_8965322"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4079aea",
      "metadata": {
        "id": "a4079aea"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}