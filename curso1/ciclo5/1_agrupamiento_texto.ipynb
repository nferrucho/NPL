{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso1/ciclo5/1_agrupamiento_texto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40b687f5",
      "metadata": {
        "id": "40b687f5"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1WNLKH10YpQNNk9eeRIyYLwGkxNbNp-Mm\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cf5cade",
      "metadata": {
        "id": "1cf5cade"
      },
      "source": [
        "# Agrupamiento de Textos\n",
        "---\n",
        "\n",
        "En este notebook veremos una introducción al aprendizaje no supervisado en procesamiento de lenguaje natural con un énfasis en agrupamiento de textos o *text clustering*. Comenzamos importando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9ad82007",
      "metadata": {
        "id": "9ad82007"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d12c37f7",
      "metadata": {
        "id": "d12c37f7"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a87e7a08",
      "metadata": {
        "id": "a87e7a08"
      },
      "source": [
        "## **1. Análisis no Supervisado en Textos**\n",
        "---\n",
        "\n",
        "Hasta el momento, hemos visto técnicas de aprendizaje supervisado para casos con textos etiquetados. Sin embargo, una gran cantidad de la información textual existente no contiene etiquetas, por lo que no es posible entrenar un modelo de clasificación. En estos casos, se deben aplicar modelos descriptivos usando técnicas de aprendizaje no supervisado. Esto con el fin de explicar el comportamiento de los datos, ya que podría contener información semántica de gran utilidad.\n",
        "\n",
        "Este tipo de modelos descriptivos tiene gran aplicación, principalmente en el entendimiento de grandes corpus a través de la identificación de los temas comunes entre varios documentos. Con este tipo de modelos podemos encontrar:\n",
        "\n",
        "- **Opiniones**: identificar opiniones o tendencias en documentos sobre una temática en particular, por ejemplo, en política, deportes, creencias, entre otras.\n",
        "- **Temas**: permite encontrar diversos temas dentro de un corpus, por ejemplo, géneros literarios en una biblioteca, tipos de noticias, género de películas o series a partir de su sinopsis, entre otros.\n",
        "- **Estilos**: permite encontrar tipos de escritura de distintos autores, por ejemplo, escritos formales, lenguaje coloquial, escritos técnicos, entre otros.\n",
        "\n",
        "Por lo general, el análisis no supervisado de texto es una tarea que actualmente consiste en la aplicación de un modelo no supervisado de *machine learning*; en la siguiente figura podemos ver el proceso general de esta tarea:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=18xoF0c2qREKfH4yYQA0xR8koKFnAkeXH\" width=\"80%\">\n",
        "\n",
        "Lo cual comprende las siguientes etapas:\n",
        "\n",
        "1. **Preprocesamiento**: consiste en la preparación de los textos para simplificar el entrenamiento de un modelo. Generalmente esta etapa involucra tareas como normalización, tokenization, eliminación de stopwords, y otras tareas tal y como se presentó en la unidad 2.\n",
        "2. **Extracción de características**: en este caso se extrae una representación numérica o características con las que se pueda modelar, tal y como se presentó en la unidad 3.\n",
        "3. **Construcción del modelo**: en esta etapa se entrena un modelo no supervisado de _machine learning_ (por ejemplo, clustering, reducción de dimensionalidad, modelos generativos, entre otros) o modelos no supervisados específicos para procesamiento de lenguaje natural (modelos de tópicos).\n",
        "4. **Interpretación del modelo**: en esta última etapa se suele dar una interpretación a los resultados obtenidos, en especial, validar si los patrones extraídos por los modelos tienen algún significado y aplicabilidad."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75782410",
      "metadata": {
        "id": "75782410"
      },
      "source": [
        "## **2. Conjunto de Datos**\n",
        "---\n",
        "\n",
        "En este caso usaremos el conjunto de datos [TMDB 5000 Movie Dataset](https://www.kaggle.com/tmdb/tmdb-movie-metadata/download), el cual contiene información sobre películas como su título, presupuesto, resumen, popularidad, géneros, entre otras.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1D-y9Vu-TxrenTRtd-hmBU-Fi3M15Th7h\" width=\"80%\">\n",
        "\n",
        "Procedemos a cargar el conjunto de datos (simplificado):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e588074",
      "metadata": {
        "id": "1e588074"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u5/tmdb5000.parquet\").dropna()\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dc0553a0",
      "metadata": {
        "id": "dc0553a0"
      },
      "source": [
        "Se trata de un conjunto de datos que contiene 4800 películas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "840bca81",
      "metadata": {
        "id": "840bca81"
      },
      "outputs": [],
      "source": [
        "display(df.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1bde9755",
      "metadata": {
        "id": "1bde9755"
      },
      "source": [
        "En este caso usaremos únicamente algunos campos relevantes para el análisis no supervisado de textos, estos son:\n",
        "\n",
        "- `title`: título de la película.\n",
        "- `overview`: resumen de la película.\n",
        "- `genres`: listado de géneros asociados a la película.\n",
        "\n",
        "Nuestro corpus estará conformado principalmente por el texto en el campo `overview`; vamos a definir una función de preprocesamiento para dejarlo más limpio. Primero definimos un pipeline en blanco de `spacy` (para tokenizar y eliminar stopwords)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d58577ad",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "d58577ad"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "display(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a781c6",
      "metadata": {
        "id": "01a781c6"
      },
      "source": [
        "Ahora, definimos la función de preprocesamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86d2d5b2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "86d2d5b2"
      },
      "outputs": [],
      "source": [
        "def preprocess(text):\n",
        "    doc = nlp(text) # creamos un documento de spacy\n",
        "    no_stops = \" \".join(\n",
        "        token.text\n",
        "        for token in filter(\n",
        "            lambda token: not token.is_stop and len(token) > 3 and len(token) < 24,\n",
        "            doc,\n",
        "            )\n",
        "        ) # eliminamos stopwords y palabras por longitud\n",
        "    norm_text = unidecode(no_stops.lower()) # normalizamos el texto\n",
        "    no_chars = re.sub(r\"[^a-z ]\", \" \", norm_text) # eliminamos caracteres especiales\n",
        "    no_spaces = re.sub(r\"\\s+\", \" \", no_chars) # eliminamos espacios duplicados\n",
        "    return no_spaces.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e91fe88",
      "metadata": {
        "id": "3e91fe88"
      },
      "source": [
        "Aplicamos la función sobre todo el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ac37dd9",
      "metadata": {
        "id": "8ac37dd9"
      },
      "outputs": [],
      "source": [
        "df = df.assign(\n",
        "        norm_text = df.overview.apply(preprocess)\n",
        "        )\n",
        "display(df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57f85dc6",
      "metadata": {
        "id": "57f85dc6"
      },
      "source": [
        "## **3. Extracción de Características**\n",
        "---\n",
        "\n",
        "En este caso utilizaremos una representación de tipo _TF-IDF_. Comenzamos importando el vectorizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f0d1a3",
      "metadata": {
        "id": "03f0d1a3"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25e3f083",
      "metadata": {
        "id": "25e3f083"
      },
      "source": [
        "Ahora, entrenamos el vectorizador sobre el corpus preprocesado, extraemos únicamente los términos que aparecen como mínimo en 5 documentos (`min_df`)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22b0706c",
      "metadata": {
        "id": "22b0706c"
      },
      "outputs": [],
      "source": [
        "vect = TfidfVectorizer(sublinear_tf=True, min_df=0.05).fit(df.norm_text)\n",
        "display(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a9c5f2a",
      "metadata": {
        "id": "7a9c5f2a"
      },
      "source": [
        "Ahora extraemos la representación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6d7ba7d4",
      "metadata": {
        "id": "6d7ba7d4"
      },
      "outputs": [],
      "source": [
        "features = vect.transform(df.norm_text).toarray()\n",
        "display(features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d27ef351",
      "metadata": {
        "id": "d27ef351"
      },
      "source": [
        "Como podemos ver, el conjunto de datos tiene casi el mismo número de características que de muestras. Esto puede llevar a efectos indeseados por la \"[maldición de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)\". Por este motivo, vamos a reducir el número de características con PCA:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97b6cc0a",
      "metadata": {
        "id": "97b6cc0a"
      },
      "outputs": [],
      "source": [
        "from sklearn.decomposition import PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c02c17f",
      "metadata": {
        "id": "2c02c17f"
      },
      "source": [
        "Este modelo será entrenado para obtener el 95% de la varianza explicada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1687c39a",
      "metadata": {
        "id": "1687c39a"
      },
      "outputs": [],
      "source": [
        "reductor = PCA(n_components=0.95).fit(features)\n",
        "reduced_features = reductor.transform(features)\n",
        "display(reduced_features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cedd07fe",
      "metadata": {
        "id": "cedd07fe"
      },
      "source": [
        "Como podemos ver, obtenemos casi la mitad de características con una representación más manejable."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f878138",
      "metadata": {
        "id": "4f878138"
      },
      "source": [
        "## **4. K-Means**\n",
        "---\n",
        "\n",
        "El agrupamiento de textos es una técnica no supervisada que se utiliza para organizar y categorizar documentos de texto en grupos similares. El objetivo es agrupar los documentos de manera que los documentos dentro de un grupo sean similares entre sí y diferentes de los documentos en otros grupos.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=15vVIbDqKp9qNQjB6G-r1g4VXZzswnuT8\" width=\"80%\">\n",
        "\n",
        "El agrupamiento de textos es útil para tareas como la recuperación de información, la indexación de documentos y la organización automática de noticias o correos electrónicos. También puede ser utilizado para descubrir patrones y relaciones entre documentos, lo que puede ser útil en aplicaciones como la minería de datos y el análisis de sentimientos.\n",
        "\n",
        "K-means es el algoritmo de agrupamiento más utilizado, en especial, cuando tenemos grandes conjuntos de datos. Este modelo busca dividir un conjunto de $N$ observaciones en $K$ grupos (donde $K$ es un número especificado previamente) de manera tal que las observaciones dentro de cada grupo son lo más similares posible entre sí.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1UNUKcthC49ZX7vPJfYy4b0yWe0cDCOJr\" width=\"80%\">\n",
        "\n",
        "El algoritmo funciona iterativamente, asignando cada observación a su grupo más cercano (según la distancia Euclidiana) y luego recalculando el centroide (o punto medio) de cada grupo. Este proceso se repite hasta que las asignaciones de grupo y los centroides no cambian significativamente.\n",
        "\n",
        "El algoritmo K-Means funciona de la siguiente manera:\n",
        "\n",
        "1. Elegir el número de clusters (k) que se desea encontrar en los datos.\n",
        "2. Seleccionar aleatoriamente k puntos de los datos como los centroides iniciales.\n",
        "3. Asignar cada punto del conjunto de datos al cluster cuyo centroide esté más cerca (basado en la distancia Euclidiana).\n",
        "4. Calcular los nuevos centroides de los clusters asignados.\n",
        "5. Repetir los pasos 3 y 4 hasta que los centroides de los clusters no cambien o se alcance el número máximo de iteraciones.\n",
        "\n",
        "En otras palabras, los puntos asignados a cada cluster se consideran como un grupo o cluster. K-Means es un algoritmo iterativo donde, en cada iteración, se asignan los puntos a los clusters más cercanos y se calculan los nuevos centroides. Los puntos se asignan a los nuevos clusters basados en los nuevos centroides. Este proceso se repite hasta que los centroides de los clusters no cambien o se alcance el número máximo de iteraciones.\n",
        "\n",
        "Si se sigue este proceso, veremos cómo el algoritmo mueve los centroides hasta la convergencia, tal y como se muestra en la siguiente figura:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Tu1p098KJYLJrvPJCruvQs3o5ol7vJTb\" width=\"50%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "295ecbfa",
      "metadata": {
        "id": "295ecbfa"
      },
      "source": [
        "### **4.1. Implementación**\n",
        "---\n",
        "\n",
        "K-means se puede utilizar directamente desde `sklearn`. Vamos a importarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "acc03f33",
      "metadata": {
        "id": "acc03f33"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3f47bbf6",
      "metadata": {
        "id": "3f47bbf6"
      },
      "source": [
        "Adicionalmente, debemos encontrar el número de clusters $K$ del modelo. Para ello, usaremos el coeficiente de silueta.\n",
        "\n",
        "El **coeficiente de silueta (Silhouette Coefficient)** es una medida de cuán bien un punto dado está asignado a su grupo respectivo en un agrupamiento. Se utiliza para evaluar la calidad de un agrupamiento y para comparar diferentes algoritmos de clustering y diferentes configuraciones de parámetros. El coeficiente de silueta se calcula para cada punto individualmente y tiene un valor entre -1 y 1.\n",
        "\n",
        "Para calcular el coeficiente de silueta de un punto, se utiliza la distancia promedio entre ese punto y los demás puntos en su mismo cluster (llamada \"cohesión\" o $a$) y la distancia promedio entre ese punto y los puntos en el cluster más cercano (llamada \"separación\" o $b$). El coeficiente de silueta se calcula como:\n",
        "\n",
        "$$\n",
        "( b - a ) / \\text{max}(a, b).\n",
        "$$\n",
        "\n",
        "Un coeficiente de silueta cercano a 1 indica que el punto está muy bien asignado al cluster, mientras que un coeficiente cercano a -1 indica que el punto probablemente pertenezca a otro cluster. Valores cercanos a 0 indican que el punto está en el límite entre dos clusters y podría pertenecer a cualquiera de ellos.\n",
        "\n",
        "El promedio de los coeficientes de silueta en todos los puntos es una medida global de la calidad del agrupamiento, donde un valor alto indica un agrupamiento de alta calidad y un valor bajo indica un agrupamiento de baja calidad.\n",
        "\n",
        "Importamos esta métrica desde `sklearn`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fbc059b",
      "metadata": {
        "id": "3fbc059b"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import silhouette_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f308c0f7",
      "metadata": {
        "id": "f308c0f7"
      },
      "source": [
        "Definimos un rango del hiperparámetro $K$ que vamos a explorar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8070afd4",
      "metadata": {
        "id": "8070afd4"
      },
      "outputs": [],
      "source": [
        "k_range = np.arange(2, 11, 1)\n",
        "display(k_range)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c626197c",
      "metadata": {
        "id": "c626197c"
      },
      "source": [
        "Ahora, entrenamos varios modelos K-means para cada valor de $K$. Vamos a almacenar el coeficiente de silueta por cada uno, y también el modelo con mejor coeficiente de silueta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e04bef2",
      "metadata": {
        "id": "9e04bef2"
      },
      "outputs": [],
      "source": [
        "best_score = -1 # variable donde almacenamos el mejor modelo\n",
        "metrics = [] # lista donde almacenamos las métricas\n",
        "for k in k_range:\n",
        "    model = KMeans(n_clusters=k, random_state=0, n_init=10).fit(reduced_features) # entrenamiento\n",
        "    score = silhouette_score(\n",
        "        reduced_features,\n",
        "        model.predict(reduced_features)\n",
        "        ) # evaluación\n",
        "    metrics.append(score)\n",
        "    if score > best_score: # validamos si la métrica mejora\n",
        "        best_score = score # guardamos la mejor métrica\n",
        "        best_model = model # guardamos el mejor modelo"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "525ecbf1",
      "metadata": {
        "id": "525ecbf1"
      },
      "source": [
        "Veamos una gráfica de las variaciones del coeficiente de silueta frente al número de clusters del modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "503d50e3",
      "metadata": {
        "id": "503d50e3"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.plot(k_range, metrics)\n",
        "ax.set_xlabel(\"$K$\")\n",
        "ax.set_ylabel(\"Coeficiente de Silueta\")\n",
        "ax.set_xticks(k_range)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c625174",
      "metadata": {
        "id": "8c625174"
      },
      "source": [
        "### **4.2. Interpretación**\n",
        "---\n",
        "\n",
        "Existen varias formas de interpretar los resultados de K-means para agrupamiento de textos. Primero, veamos la distribución de los clusters encontrados en el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be33e786",
      "metadata": {
        "id": "be33e786"
      },
      "outputs": [],
      "source": [
        "clusters = best_model.predict(reduced_features)\n",
        "cats, counts = np.unique(clusters, return_counts=True)\n",
        "fig, ax = plt.subplots()\n",
        "ax.bar(cats, counts)\n",
        "ax.set_xlabel(\"Cluster\")\n",
        "ax.set_ylabel(\"Conteo\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b742ed8d",
      "metadata": {
        "id": "b742ed8d"
      },
      "source": [
        "Como podemos ver, los clusters no se encuentran uniformemente distribuidos. Ahora, debemos tratar de interpretar qué es cada cluster. Para ello, vamos a agrupar los textos preprocesados del conjunto de datos original según el cluster asignado por el modelo de K-means. Para ello vamos a crear el siguiente `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2603b2ff",
      "metadata": {
        "id": "2603b2ff"
      },
      "outputs": [],
      "source": [
        "predictions = pd.DataFrame({\"text\": df.norm_text, \"cluster\": clusters})\n",
        "display(predictions.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48891835",
      "metadata": {
        "id": "48891835"
      },
      "source": [
        "Ahora, vamos a extraer todos los textos concatenados por cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2114708",
      "metadata": {
        "id": "e2114708"
      },
      "outputs": [],
      "source": [
        "grouped_texts = (\n",
        "        predictions\n",
        "        .groupby(\"cluster\")\n",
        "        .agg({\"text\": lambda series: \" \".join(series)})\n",
        "        .reset_index()\n",
        "        )\n",
        "display(grouped_texts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "758e9000",
      "metadata": {
        "id": "758e9000"
      },
      "source": [
        "Con esto podemos pasar a generar una visualización de tipo nube de palabras para los clusters encontrados, importamos `WordCloud`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d574f36b",
      "metadata": {
        "id": "d574f36b"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83b99fb8",
      "metadata": {
        "id": "83b99fb8"
      },
      "source": [
        "Ahora, generamos una gráfica para visualizar nubes de palabras de todos los textos dentro de un cluster específico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fa3b9ed",
      "metadata": {
        "id": "8fa3b9ed"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(\n",
        "    1,\n",
        "    best_model.n_clusters,\n",
        "    figsize=(10 * best_model.n_clusters, 10),\n",
        "    )\n",
        "for cluster in range(best_model.n_clusters):\n",
        "    ax = axes[cluster]\n",
        "    ax.set_title(f\"Cluster: {cluster}\")\n",
        "    text = grouped_texts.loc[grouped_texts.cluster == cluster, \"text\"].iloc[0]\n",
        "    wc = WordCloud(\n",
        "        background_color=\"#FFFFFF\",\n",
        "        width=500,\n",
        "        height=500\n",
        "        ).generate(text)\n",
        "    ax.imshow(wc)\n",
        "    ax.axis(\"off\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b839731",
      "metadata": {
        "id": "9b839731"
      },
      "source": [
        "Los resultados pueden variar entre cada ejecución, no obstante, veamos un ejemplo de lo que se puede obtener:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1WzCVyYd6YYyxdTzCtAdDcK3BH4so7JzL\" width=\"100%\">\n",
        "\n",
        "Podemos ver los siguientes patrones:\n",
        "\n",
        "- Cluster 0, 1 y 5: probablemente hacen referencia a películas relacionadas a relaciones familiares o amistades.\n",
        "- Cluster 2: parecen películas de deportes.\n",
        "- Cluster 3: películas de adolescentes ambientadas en el colegio.\n",
        "- Cluster 4: la temática puede estar relacionada a películas de drama.\n",
        "- Cluster 6: probablemente son películas policiales o de crímenes.\n",
        "- Cluster 7: películas sobre aliens o misiones espaciales.\n",
        "- Cluster 8: películas de romance\n",
        "- Cluster 9: películas con relaciones de hombre-mujer (esposos, padre e hija, entre otras).\n",
        "\n",
        "Por último, podemos cruzar información categórica adicional a los clusters, como por ejemplo los géneros de las películas. Vamos a calcular los géneros totales por cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dde97ea3",
      "metadata": {
        "id": "dde97ea3"
      },
      "outputs": [],
      "source": [
        "genres = pd.DataFrame({\"genres\": df.genres, \"cluster\": clusters})\n",
        "grouped_genres = (\n",
        "        genres\n",
        "        .groupby(\"cluster\")\n",
        "        .agg({\"genres\": lambda genres: sum(map(list, genres.tolist()), [])})\n",
        "        .reset_index()\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c03be85c",
      "metadata": {
        "id": "c03be85c"
      },
      "source": [
        "Ahora, podemos generar la distribución de géneros por cada cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "700f0e2a",
      "metadata": {
        "id": "700f0e2a"
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(\n",
        "    2, 5,\n",
        "    figsize=(20, 20)\n",
        "    )\n",
        "for cluster in range(best_model.n_clusters):\n",
        "    ax = axes[cluster // 5, cluster % 5]\n",
        "    ax.set_title(f\"Cluster: {cluster}\")\n",
        "    genres = grouped_genres.loc[grouped_genres.cluster == cluster, \"genres\"].iloc[0]\n",
        "    unique, counts = np.unique(genres, return_counts=True)\n",
        "    counts_df = (\n",
        "            pd.DataFrame(data={\"unique\": unique, \"counts\": counts})\n",
        "            .sort_values(by=\"counts\", ascending=False)\n",
        "            .head(5)\n",
        "            )\n",
        "\n",
        "    ax.barh(counts_df.unique, counts_df.counts)\n",
        "    ax.set_xlabel(\"Conteos\")\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "22d38aa8",
      "metadata": {
        "id": "22d38aa8"
      },
      "source": [
        "Nuevamente, los resultados pueden variar entre una ejecución y otra. Veamos una gráfica de la distribución de géneros para el ejemplo que veníamos mostrando:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17sosOpdIDsQwYD_llTxzGA_B2MXIk_Zt\" width=\"100%\">\n",
        "\n",
        "Se pueden observar correspondencias con los patrones que identificamos en las nubes de palabras. Por ejemplo, el cluster 6 tiene una alta distribución de términos relacionados con el género de horror, suspenso y crimen, lo que se relaciona con temas de asesinos seriales, asesinatos, víctimas y similares. El cluster 7 tiene una alta distribución de términos relacionados con acción, aventura y ciencia ficción, lo que se relaciona con temáticas de aliens y espacio. El cluster 8 tiene una alta distribución de términos relacionados con romance, drama y comedia, lo que corresponde con palabras como amor."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "98d80d6a",
      "metadata": {
        "id": "98d80d6a"
      },
      "source": [
        "## **5. Clustering Jerárquico**\n",
        "---\n",
        "\n",
        "Clustering jerárquico es un enfoque de agrupamiento no supervisado que se utiliza para organizar los datos en una estructura jerárquica de grupos o clusters. Existen dos tipos principales de clustering jerárquico:\n",
        "\n",
        "- **Aglomerativo**: este enfoque comienza con cada punto como un cluster individual y luego agrupa gradualmente los puntos más similares en grupos más grandes hasta que se alcanza el nivel deseado de agrupamiento.\n",
        "- **Divisivo**: este enfoque comienza con todos los puntos en un único grupo y luego divide gradualmente el grupo en subgrupos más pequeños hasta que se alcanza el nivel deseado de agrupamiento.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1ELvKAB4FvlMnveKPoqBPmq5e1jVhoNGU\" width=\"80%\">\n",
        "\n",
        "El clustering jerárquico utiliza un enfoque de \"todos contra todos\" para comparar todos los puntos entre sí y determinar cuáles son los más similares. Luego, se utilizan diferentes criterios para unir o dividir los grupos. Uno de los criterios comúnmente utilizados es la distancia euclidiana entre los puntos.\n",
        "\n",
        "Una de las ventajas del clustering jerárquico es que permite visualizar la estructura jerárquica de los datos mediante un dendrograma. Además, también permite seleccionar el nivel deseado de agrupamiento, lo que puede ser útil en algunas aplicaciones. Sin embargo, una desventaja es que puede ser computacionalmente costoso para conjuntos de datos muy grandes.\n",
        "\n",
        "El clustering jerárquico se utiliza mucho en el procesamiento del lenguaje natural (NLP) debido a varias razones:\n",
        "\n",
        "- Permite la exploración de los datos: El clustering jerárquico permite visualizar la estructura jerárquica de los datos mediante un dendrograma, lo que puede ser útil para entender la relación entre los documentos y descubrir patrones y temas emergentes.\n",
        "- No requiere un número previamente especificado de clusters: A diferencia de otros algoritmos de clustering, como k-means, el clustering jerárquico no requiere que se especifique el número de clusters previamente. Esto es útil en el NLP, ya que a menudo es difícil saber cuántos grupos o temas hay en un conjunto de documentos.\n",
        "- Permite la selección de nivel de agrupamiento: El clustering jerárquico permite seleccionar el nivel deseado de agrupamiento. Por ejemplo, se pueden crear clusters más generales para tener una vista panorámica de los datos o clusters más específicos para obtener una comprensión detallada de los temas.\n",
        "- Es adecuado para datos no numéricos: El clustering jerárquico se utiliza a menudo con datos no numéricos, como texto, lo que lo hace adecuado para el NLP. El clustering jerárquico se puede aplicar a los documentos después de convertirlos en una representación numérica, utilizando técnicas como el modelo de lenguaje o la frecuencia de términos.\n",
        "\n",
        "El enfoque aglomerativo es el tipo más común de clustering jerárquico. Comienza suponiendo que cada uno de los puntos es un cluster, los cuales se irán fusionando a lo largo de las iteraciones hasta obtener un número deseado de clusters. El método consiste en los siguientes pasos:\n",
        "\n",
        "1. Calcular una matriz de similitud $\\mathbf{S}$.\n",
        "2. Fusionar los dos clusters más cercanos.\n",
        "3. Repetir desde el paso 1 hasta obtener un único cluster o un número $K$ de clusters.\n",
        "\n",
        "Como un cluster puede estar compuesto por varios puntos, existen diversas formas de obtener similitudes:\n",
        "\n",
        "- `single`: la similitud entre dos clusters es la mínima similitud o distancia entre los puntos de cada cluster.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1VJbbWag36EJ_dEQ9Brwe6UPvNngY46-d\" width=\"60%\">\n",
        "\n",
        "- `complete`: la similitud entre dos clusters es la máxima distancia entre los puntos de cada cluster.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1zQzghA8Sy75R-C0tniGHqKqTjL00OV1S\" width=\"60%\">\n",
        "\n",
        "- `average`: la similitud entre dos clusters es el promedio entre la similitud de todas las combinaciones de puntos de cada cluster.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12Vqk_h6aDcrie2hiY-P-rctmsh56zqok\" width=\"60%\">\n",
        "\n",
        "- `centroid`: la similitud entre dos clusters es la similitud entre los puntos promedio o centroides de cada cluster.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1IMYYFSGsEzL0V4VaUOQX4hibM01eOvUF\" width=\"60%\">\n",
        "\n",
        "- `ward`: la similitud entre dos clusters es la suma de cuadrados media de todas las combinaciones de puntos de cada cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5b11d72",
      "metadata": {
        "id": "f5b11d72"
      },
      "source": [
        "### **5.1. Implementación**\n",
        "---\n",
        "\n",
        "Veamos la aplicación de agglomerative clustering, comenzamos importando las funciones necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4726a31e",
      "metadata": {
        "id": "4726a31e"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import linkage"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9269c069",
      "metadata": {
        "id": "9269c069"
      },
      "source": [
        "Para aplicar el clustering jerárquico debemos obtener una matriz conocida como _linkage matrix_, la cual tiene tamaño $(N-1) \\times 4$. Cada fila $i$ contiene el resultado de cada iteración, y en las columnas se encuentra la siguiente información: $L_{i,0}$ y $L_{i,1}$ contienen los índices de los clusters que se unirán, $L_{i,2}$ contiene la similitud entre los dos clusters y $L_{i,3}$ contiene el número de puntos totales que hay en el cluster formado.\n",
        "\n",
        "Para aplicar clustering jerárquico se debe construir una matriz de similitud entre cada documento. En este caso se utiliza la distancia Euclidiana.\n",
        "\n",
        "La función `linkage` tiene los siguientes parámetros:\n",
        "\n",
        "- `y`: matriz de características de los datos.\n",
        "- `method`: método de unión aglomerativo.\n",
        "- `metric`: medida de similitud usada para comparar muestras."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "05be6783",
      "metadata": {
        "id": "05be6783"
      },
      "outputs": [],
      "source": [
        "linkage_matrix = linkage(reduced_features, method=\"ward\", metric=\"euclidean\")\n",
        "display(linkage_matrix)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9816c221",
      "metadata": {
        "id": "9816c221"
      },
      "source": [
        "### **5.2. Interpretación**\n",
        "---\n",
        "\n",
        "Un dendrograma es un tipo de diagrama utilizado para representar la estructura jerárquica de los datos en un clustering jerárquico. Se ve como una especie de árbol con ramas y hojas, donde las ramas representan los clusters y las hojas representan los puntos individuales.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1gIn3v1dhQ843TMhvP93zR_HHiuPVKDyj\" width=\"80%\">\n",
        "\n",
        "Cada nivel del dendrograma representa un nivel de agrupamiento diferente, con los clusters más generales en la parte superior y los clusters más específicos en la parte inferior. Los puntos se agrupan en clusters más pequeños a medida que se desciende por el dendrograma.\n",
        "\n",
        "Las ramas del dendrograma se unen mediante un punto de unión, que representa el punto en el que se combinaron dos clusters para crear un cluster más grande. La distancia entre los puntos de unión también puede ser utilizada para medir la similitud entre los clusters.\n",
        "\n",
        "El dendrograma permite visualizar la estructura jerárquica de los datos y seleccionar el nivel deseado de agrupamiento. También es útil para detectar patrones y relaciones entre los puntos, y para comparar diferentes algoritmos de clustering y diferentes configuraciones de parámetros.\n",
        "\n",
        "Esta visualización se genera usando la función `dendrogram`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7023df41",
      "metadata": {
        "id": "7023df41"
      },
      "outputs": [],
      "source": [
        "from scipy.cluster.hierarchy import dendrogram"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3bbb8e8e",
      "metadata": {
        "id": "3bbb8e8e"
      },
      "source": [
        "Vamos a generar la visualización para el resultado de clustering jerárquico y mostrando el título de cada película:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a074e14",
      "metadata": {
        "id": "1a074e14"
      },
      "outputs": [],
      "source": [
        "titles = df.title.tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8e0a3cf",
      "metadata": {
        "id": "d8e0a3cf"
      },
      "source": [
        "Construimos el dendrograma (árbol jerárquico):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea8d7ac5",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "ea8d7ac5"
      },
      "outputs": [],
      "source": [
        "R = dendrogram(\n",
        "    linkage_matrix, orientation=\"left\",\n",
        "    labels=titles, truncate_mode='lastp',\n",
        "    p=100, no_plot=True,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6567a0b",
      "metadata": {
        "id": "f6567a0b"
      },
      "source": [
        "Definimos una función para etiquetar cada una de las ramas del árbol."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59866cbe",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "59866cbe"
      },
      "outputs": [],
      "source": [
        "def llf(x):\n",
        "    # Asignamos cada uno de los títulos de las películas a cada una de las ramas\n",
        "    temp = {R[\"leaves\"][i]: titles[i] for i in range(len(R[\"leaves\"]))}\n",
        "    return \"{}\".format(temp[x])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dc775ee",
      "metadata": {
        "id": "0dc775ee"
      },
      "source": [
        "Visualizamos el dendrograma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "644cb16b",
      "metadata": {
        "id": "644cb16b"
      },
      "outputs": [],
      "source": [
        "fig = plt.figure(figsize=(10, 40))\n",
        "ax = dendrogram(linkage_matrix, truncate_mode='lastp', orientation=\"left\", p=100,\n",
        "              leaf_label_func=llf, leaf_font_size=10.)\n",
        "fig.tight_layout()\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "625e7d3f",
      "metadata": {
        "id": "625e7d3f"
      },
      "source": [
        "Esta visualización nos permite interpretar qué películas se parecen más entre sí, de acuerdo a su sinopsis, con lo que podríamos llegar a identificar grupos de películas similares.\n",
        "\n",
        "Por último, también es posible usar clustering jerárquico como cualquier otro modelo de clustering de `sklearn` de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a01b76c5",
      "metadata": {
        "id": "a01b76c5"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import AgglomerativeClustering"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07206f19",
      "metadata": {
        "id": "07206f19"
      },
      "source": [
        "Esta clase tiene los siguientes parámetros:\n",
        "\n",
        "- `n_clusters`: permite especificar cuántos clusters deseamos obtener.\n",
        "- `metric`: medida de similitud a usar para comparar muestras.\n",
        "- `linkage`: permite especificar el tipo de unión que se usará.\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fe46bab",
      "metadata": {
        "id": "1fe46bab"
      },
      "outputs": [],
      "source": [
        "hc = (\n",
        "        AgglomerativeClustering(n_clusters=3, linkage=\"ward\")\n",
        "        .fit(reduced_features)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f5d4757",
      "metadata": {
        "id": "6f5d4757"
      },
      "source": [
        "Veamos las etiquetas asignadas:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a468685",
      "metadata": {
        "id": "0a468685"
      },
      "outputs": [],
      "source": [
        "labels = hc.labels_\n",
        "display(labels[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a03b70b5",
      "metadata": {
        "id": "a03b70b5"
      },
      "source": [
        "Por último, veamos cuántas películas quedaron agrupadas en cada uno de los 3 clusters:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cdb2f2a",
      "metadata": {
        "id": "2cdb2f2a"
      },
      "outputs": [],
      "source": [
        "pd.value_counts(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c6b6e19",
      "metadata": {
        "id": "4c6b6e19"
      },
      "source": [
        "## **Recursos Adicionales**\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [Hierarchical clustering - scipy](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html).\n",
        "- [Clustering - sklearn](https://scikit-learn.org/stable/modules/clustering.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23820ba3",
      "metadata": {
        "id": "23820ba3"
      },
      "source": [
        "## **Créditos**\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}