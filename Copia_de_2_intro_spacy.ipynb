{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/Copia_de_2_intro_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d236b4",
      "metadata": {
        "id": "21d236b4"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1AQr9H9bXDeNPchTRufU78g8z0yxHvrmC\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94699a4a",
      "metadata": {
        "id": "94699a4a"
      },
      "source": [
        "# Introducción a Spacy\n",
        "---\n",
        "\n",
        "En este taller guiado presentaremos una introducción práctica a la librería `spacy` para procesamiento de lenguaje natural. Comenzamos importándola:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724943d4",
      "metadata": {
        "id": "724943d4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b817a40",
      "metadata": {
        "id": "5b817a40"
      },
      "source": [
        "## **1. ¿Qué es Spacy?**\n",
        "---\n",
        "\n",
        "Se trata de una librería para procesamiento de lenguaje natural (NLP) que provee una forma de uso simple y flexible para implementar soluciones de NLP de forma inmediata en aplicaciones industriales.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_hGDcMnNp0MAS7ERGPcmnGTX-xAs1iiG\" width=\"40%\">\n",
        "\n",
        "`spacy` tiene las siguientes características:\n",
        "\n",
        "* Soporta más de 66 lenguajes.\n",
        "* Trae más de 80 _pipelines_ pre-entrenados para más de 24 lenguajes.\n",
        "* Permite usar modelos del estado del arte en NLP.\n",
        "* Embeddings pre-entrenados.\n",
        "* Velocidad competitiva con el estado del arte.\n",
        "* Sistema de entrenamiento de modelos.\n",
        "* Herramientas de tokenizado lingüístico.\n",
        "* Componentes de distintas tareas generales de NLP como: reconocimiento de entidades nombradas, part-of-speech, segmentación de textos, entre otros.\n",
        "* Componentes personalizados.\n",
        "* Soporta modelos personalizados de diversas librerías de machine learning.\n",
        "* Herramientas de visualización para texto.\n",
        "* Fácil empaquetado de modelos para el despliegue y la gestión.\n",
        "* Modelos con un buen desempeño base y con una rigurosa evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d760d1",
      "metadata": {
        "id": "71d760d1"
      },
      "source": [
        "## **2. Modelos de Spacy**\n",
        "---\n",
        "\n",
        "`spacy` tiene distintos modelos pre-entrenados y listos para usar en distintos lenguajes. Puedes revisar los distintos lenguajes y modelos, como se muestra a continuación, a partir de [este enlace](https://spacy.io/models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462281f",
      "metadata": {
        "id": "0462281f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/1ncW1HPrkU7aHw2eUhBl-zeoaukypmIZ0/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16154a1f",
      "metadata": {
        "id": "16154a1f"
      },
      "source": [
        "Los modelos pre-entrenados generalmente tienen un código de nombramiento como `{lang}_{from}_{corpus}_{size}`:\n",
        "\n",
        "* `lang`: código del idioma de los modelos a cargar.\n",
        "* `from`: repositorio de dónde vienen los modelos.\n",
        "* `corpus`: conjunto de datos donde fueron entrenados los modelos.\n",
        "* `size`: tamaño del modelo, generalmente se maneja `sm` (small - pequeño), `md` (medium - mediano), `lg` (large - grande).\n",
        "\n",
        "Los modelos los podemos descargar automáticamente usando la función `download` dentro del `cli` de `spacy` como mostramos a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183ca35b",
      "metadata": {
        "id": "183ca35b"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa92ffee",
      "metadata": {
        "id": "fa92ffee"
      },
      "source": [
        "## **_3. Pipeline_**\n",
        "---\n",
        "\n",
        "Los _pipelines_ de `spacy` nos proveen una interfaz por medio de la cual podemos aplicar distintas técnicas y modelos de NLP de forma ordenada y secuencial. Un _pipeline_ representa una serie de pasos (componentes) que se aplican uno tras otro para obtener información específica de un texto. Por lo general siguen la siguiente estructura:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12MczaMlyMo12kIbk5FZC0PzA1h5E6fQq\" width=\"60%\">\n",
        "<font size=\"1\" color=\"black\"><i>CNN/CPU pipeline design [Imagen]. Extraída de https://spacy.io/models </i></font>\n",
        "</center>\n",
        "\n",
        "\n",
        "Cada uno de los componentes del _pipeline_ será explicado en detalle posteriormente en este mismo taller guiado. Por ahora, podemos centrarnos en el proceso, el cual consiste en los siguientes pasos:\n",
        "\n",
        "* Los componentes `tagger`, `morphologizer` y `parser` esperan la salida del componente `tok2vec`. Si el `lemmatizer` es entrenable, este también espera la salida de `tok2vec`.\n",
        "* El componente `attribute_ruler` utiliza los resultados del `tagger` y valida que los espacios y caracteres especiales estén etiquetados correctamente.\n",
        "* El `lemmatizer` utiliza las reglas obtenidas del `attribute_ruler` para transformar los textos.\n",
        "* El componente `ner` es independiente del resto de etapas y puede tener su propio componente de `tok2vec`.\n",
        "\n",
        "Veamos cómo crear un _pipeline_ de `spacy` con la función `load` y el _pipeline_ pre-entrenado que descargamos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6773e5b",
      "metadata": {
        "id": "a6773e5b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4660a9",
      "metadata": {
        "id": "7f4660a9"
      },
      "source": [
        "Como puede se puede ver, cargamos un objeto de tipo `Spanish`, lo que indica que `spacy` por detrás define clases personalizadas para los distintos lenguajes que queramos manejar.\n",
        "\n",
        "Podemos obtener el listado de los componentes que tiene el _pipeline_ que cargó usando el atributo `component_names`, como se muestra a continuación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c893106",
      "metadata": {
        "id": "0c893106"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8028faf3",
      "metadata": {
        "id": "8028faf3"
      },
      "source": [
        "Generalmente, un _pipeline_ de `spacy` aplica todos los componentes que tiene definidos sobre un documento específico, no obstante, en algunas aplicaciones únicamente llegamos a necesitar 1 o 2 componentes. Podemos deshabilitarlos usando el parámetro `exclude` de la función `load`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8df55b",
      "metadata": {
        "id": "be8df55b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\", exclude=[\"ner\"])\n",
        "print(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cb1e79",
      "metadata": {
        "id": "d3cb1e79"
      },
      "source": [
        "En este caso, cargamos un _pipeline_ con el componente `ner` deshabilitado, veamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aaada7d",
      "metadata": {
        "id": "6aaada7d"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c889a7",
      "metadata": {
        "id": "66c889a7"
      },
      "source": [
        "## **4. Objetos de Spacy**\n",
        "---\n",
        "\n",
        "En `spacy` generalmente estamos trabajando sobre tres tipos de objetos: los documentos `Doc`, las palabras `Token` y las secuencias `Span`.\n",
        "\n",
        "Primero, vamos a definir un corpus con 3 documentos de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e986c2c",
      "metadata": {
        "id": "7e986c2c"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "        \"Alan Mathison Turing fue un matemático, lógico, informático teórico, criptógrafo, filósofo y biólogo teórico británico\",\n",
        "        \"Marvin Lee Minsky fue un científico estadounidense. Es considerado uno de los padres de la inteligencia artificial. Fue cofundador del laboratorio de inteligencia artificial del Instituto de Tecnología de Massachusetts (MIT).\",\n",
        "        \"Geoffrey Hinton es un informático británico. Hinton fue galardonado con el Premio Turing en 2018 junto con Yoshua Bengio y Yann LeCun por su trabajo en deep learning.\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9ffb6d",
      "metadata": {
        "id": "2f9ffb6d"
      },
      "source": [
        "Ahora, podemos convertir cada documento a un `Doc` de `spacy` por medio del método `pipe` del *pipeline* como se muestra a continuación. El parámetro `n_process` nos permite ejecutar los componentes de forma paralelizada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884cbca6",
      "metadata": {
        "id": "884cbca6"
      },
      "outputs": [],
      "source": [
        "corpus = list(nlp.pipe(texts, n_process=4))\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1f5e0f",
      "metadata": {
        "id": "4b1f5e0f"
      },
      "source": [
        "A simple vista, pareciera que los documentos no han cambiado mucho, no obstante, podemos validar que el tipo de un elemento dentro de `corpus` efectivamente es de tipo `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae8790f",
      "metadata": {
        "id": "3ae8790f"
      },
      "outputs": [],
      "source": [
        "doc = corpus[0]\n",
        "print(type(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45151f6e",
      "metadata": {
        "id": "45151f6e"
      },
      "source": [
        "Podemos extraer el texto asociado a un `Doc` con el atributo `text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d189fa",
      "metadata": {
        "id": "e1d189fa"
      },
      "outputs": [],
      "source": [
        "print(doc.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf8da3e",
      "metadata": {
        "id": "eaf8da3e"
      },
      "source": [
        "El tipo `Doc` tiene distintos atributos que posteriormente usaremos para acceder a los resultados de los distintos componentes del modelo. Entre ellos, podemos obtener tokens a nivel de palabra con una simple indexación sobre el documento. Es decir, si queremos extraer la primera palabra del documento podemos hacer lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bde886",
      "metadata": {
        "id": "04bde886"
      },
      "outputs": [],
      "source": [
        "token = doc[0]\n",
        "print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4481d",
      "metadata": {
        "id": "19d4481d"
      },
      "source": [
        "De nuevo pareciera que fuera un string, pero en realidad es un objeto de tipo `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60cb72cb",
      "metadata": {
        "id": "60cb72cb"
      },
      "outputs": [],
      "source": [
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7274b851",
      "metadata": {
        "id": "7274b851"
      },
      "source": [
        "El objeto `Token` tiene distintos atributos asociados a cada componente como mostraremos posteriormente.\n",
        "\n",
        "Por último, podemos extraer las oraciones del documento por medio del atributo `sents` para ver el tipo de datos que nos entrega:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a3f830",
      "metadata": {
        "id": "36a3f830"
      },
      "outputs": [],
      "source": [
        "sents = list(doc.sents)\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ad88f3",
      "metadata": {
        "id": "99ad88f3"
      },
      "source": [
        "Al igual que en los casos anteriores los resultados no son de tipo string sino que son de tipo `Span` (secuencia):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8897aa80",
      "metadata": {
        "id": "8897aa80"
      },
      "outputs": [],
      "source": [
        "sent = sents[0]\n",
        "print(type(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ece158f",
      "metadata": {
        "id": "9ece158f"
      },
      "source": [
        "El objeto `Span` también tiene distintos atributos asociados a cada componente como lo veremos más adelante. Cada elemento de un `Span` es un `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1bc8e1",
      "metadata": {
        "id": "ea1bc8e1"
      },
      "outputs": [],
      "source": [
        "token = sent[0]\n",
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e07a52",
      "metadata": {
        "id": "70e07a52"
      },
      "source": [
        "## **5. Componentes**\n",
        "---\n",
        "\n",
        "Para ver el detalle de qué hace cada componente vamos a cargar el _pipeline_ `en_core_web_sm` en inglés, ya que, es la forma más estandarizada y general para entender los componentes y sus resultados.\n",
        "\n",
        "En este caso no usamos el _pipeline_ en español ya que este no posee implementados todos los componentes que el _pipeline_ en inglés sí. Por ejemplo, no contiene etiquetas _POS_ de grano fino.\n",
        "\n",
        "El ejemplo en español lo retomaremos más adelante, una vez veamos todos los componentes que puede llegar a tener un _pipeline_ de `spacy`.\n",
        "\n",
        "Veamos cómo descargar el _pipeline_ `en_core_web_sm` para análisis del inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb55581c",
      "metadata": {
        "id": "eb55581c"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b696c4ab",
      "metadata": {
        "id": "b696c4ab"
      },
      "source": [
        "También lo cargamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acdb172",
      "metadata": {
        "id": "1acdb172"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3043cd43",
      "metadata": {
        "id": "3043cd43"
      },
      "source": [
        "Veamos los componentes que trae este _pipeline_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d5663a",
      "metadata": {
        "id": "02d5663a"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d70a91f",
      "metadata": {
        "id": "4d70a91f"
      },
      "source": [
        "Estos son los componentes típicos que trae un _pipeline_ de `spacy`. En algunos lenguajes puede cambiar `tagger` por `morphologizer` si no hay muchos modelos disponibles para etiquetado de palabras en un idioma en específico (`tagger` es más completo que `morphologizer`).\n",
        "\n",
        "Vamos a ver el detalle de los componentes más importantes de `spacy` con el siguiente texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ea083c",
      "metadata": {
        "id": "48ea083c"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. He is widely considered to be the father of theoretical computer science and artificial intelligence.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3980c3cc",
      "metadata": {
        "id": "3980c3cc"
      },
      "source": [
        "Ejecutamos el _pipeline_ para obtener un objeto de tipo `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a439512",
      "metadata": {
        "id": "1a439512"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89ca946",
      "metadata": {
        "id": "c89ca946"
      },
      "source": [
        "### **5.1. Tokenizer**\n",
        "---\n",
        "`spacy` trae un componente llamado `Tokenizer` que está definido por defecto en un paquete de lenguaje. Se trata de un componente que aplica reglas del lenguaje para separar palabras, detectar signos de puntuación y encontrar atributos generales de los objetos de tipo `Token`.\n",
        "\n",
        "El `Tokenizer` nos permite obtener el listado de tokens (incluye signos de puntuación como tokens separados) por medio de algunas reglas específicas de cada lenguaje.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=114bsjIq5vjLLLQYLwcUh14jObuUzbydy\" width=\"60%\">\n",
        "\n",
        "\n",
        "Veamos cómo podemos obtener una lista de todas las palabras del documento con `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96749016",
      "metadata": {
        "id": "96749016"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755f95fe",
      "metadata": {
        "id": "755f95fe"
      },
      "source": [
        "Adicionalmente, desde el `Tokenizer` se calculan atributos en los tokens como:\n",
        "\n",
        "* `is_stop`: si la palabra es un stopword, es decir, una palabra muy frecuente y poco informativa del lenguaje en cuestión (e.g., is, the, to, entre otras).\n",
        "* `is_punct`: valida si el token es un signo de puntuación.\n",
        "* `is_title`: valida si el token está en formato de título.\n",
        "* `is_upper`: valida si el token está en mayúsculas.\n",
        "* `is_lower`: valida si el token está en minúsculas.\n",
        "* `lower_`: versión en minúsculas del token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176671a0",
      "metadata": {
        "id": "176671a0"
      },
      "source": [
        "Por ejemplo, podemos validar qué tokens son _stopwords_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dfd2d8",
      "metadata": {
        "id": "40dfd2d8"
      },
      "outputs": [],
      "source": [
        "stops = [token.is_stop for token in doc]\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba790f6b",
      "metadata": {
        "id": "ba790f6b"
      },
      "source": [
        "También podemos filtrar el listado de palabras que son _stopwords_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1389b654",
      "metadata": {
        "id": "1389b654"
      },
      "outputs": [],
      "source": [
        "stops = list(filter(lambda token: token.is_stop, doc))\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7aa0dce",
      "metadata": {
        "id": "c7aa0dce"
      },
      "source": [
        "### **5.2. Senter**\n",
        "---\n",
        "\n",
        "El componente `senter` hace uso de la clase `Sentencizer` la cual permite segmentar el texto en distintas oraciones usando un modelo basado en reglas (expresiones regulares), el resultado queda almacenado en dos niveles:\n",
        "\n",
        "* Como el atributo `is_sent_start` de cada `Token`.\n",
        "* Como el atributo `sents` dentro del documento `Doc`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OtK3KZuKAeAwbAQpcmzGCz-CQxVfb192\" width=\"80%\">\n",
        "\n",
        "Veamos qué tokens son inicio de oración dentro del documento que estamos manejando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6113c4b6",
      "metadata": {
        "id": "6113c4b6"
      },
      "outputs": [],
      "source": [
        "sent_starts = list(filter(lambda token: token.is_sent_start, doc))\n",
        "print(sent_starts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c5a206",
      "metadata": {
        "id": "a6c5a206"
      },
      "source": [
        "También podemos extraer un listado de las oraciones del documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4f5a48",
      "metadata": {
        "id": "aa4f5a48"
      },
      "outputs": [],
      "source": [
        "sents = list(doc.sents)\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54123d02",
      "metadata": {
        "id": "54123d02"
      },
      "source": [
        "En total tenemos 3 oraciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59226c6a",
      "metadata": {
        "id": "59226c6a"
      },
      "outputs": [],
      "source": [
        "print(len(sents))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d843b0d",
      "metadata": {
        "id": "0d843b0d"
      },
      "source": [
        "### **5.3. Tok2vec**\n",
        "---\n",
        "\n",
        "El componente `tok2vec` hace uso de la clase `Tok2Vec` para implementar distintos modelos de **embedding**, de los cuales hablaremos en detalle en la **Unidad 3**.\n",
        "\n",
        "De forma resumida, este tipo de modelos permite extraer características numéricas a partir de palabras, oraciones o documentos y representarlas por medio de un vector. El vector se almacena como un atributo `vector` dentro de `Doc`, `Span` y `Token`. Esto es útil para capturar relaciones semánticas (relacionadas con el significado) en las palabras, oraciones o documentos. Como por ejemplo en el caso de la relación existente entre las palabras `Mujer` y `Hombre`, con respecto a las palabras `Reina` y `Rey`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1w4z22p_X4KQ-sim0kZBnjkAZOaiojUCv\" width=\"80%\">\n",
        "\n",
        "Veamos un ejemplo de cómo extraer características de todo un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4001798d",
      "metadata": {
        "id": "4001798d"
      },
      "outputs": [],
      "source": [
        "vect = doc.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0666992d",
      "metadata": {
        "id": "0666992d"
      },
      "source": [
        "`spacy` extrae un vector de características como un arreglo de `numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda7d04a",
      "metadata": {
        "id": "fda7d04a"
      },
      "outputs": [],
      "source": [
        "print(type(vect))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6c2076",
      "metadata": {
        "id": "1c6c2076"
      },
      "source": [
        "También podemos extraer características de una oración:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec9cf7b",
      "metadata": {
        "id": "eec9cf7b"
      },
      "outputs": [],
      "source": [
        "sent = list(doc.sents)[0]\n",
        "print(type(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556b8079",
      "metadata": {
        "id": "556b8079"
      },
      "source": [
        "Veamos el vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762539ca",
      "metadata": {
        "id": "762539ca"
      },
      "outputs": [],
      "source": [
        "vect = sent.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c473e3c",
      "metadata": {
        "id": "9c473e3c"
      },
      "source": [
        "El proceso es similar para un `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0932cadd",
      "metadata": {
        "id": "0932cadd"
      },
      "outputs": [],
      "source": [
        "token = sent[0]\n",
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49526b",
      "metadata": {
        "id": "ef49526b"
      },
      "source": [
        "Veamos el vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd03a6d1",
      "metadata": {
        "id": "fd03a6d1"
      },
      "outputs": [],
      "source": [
        "vect = token.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2e3d66",
      "metadata": {
        "id": "de2e3d66"
      },
      "source": [
        "En estos casos el vector tiene 96 características numéricas para representar el `Token`, `Sent` o el `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459355ff",
      "metadata": {
        "id": "459355ff"
      },
      "outputs": [],
      "source": [
        "print(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d7da91e",
      "metadata": {
        "id": "1d7da91e"
      },
      "source": [
        "### **5.4. Tagger**\n",
        "---\n",
        "\n",
        "El componente `tagger` permite extraer etiquetas de tipo part-of-speech (POS) a cada `Token`. El proceso consiste en asignarle a cada palabra de una oración una etiqueta de acuerdo con su función gramatical dentro de la oración. Con POS podemos determinar si un token es un sustantivo, una preposición, un adjetivo, un adverbio, un verbo, complemento directo o indirecto, entre otros.\n",
        "\n",
        "`spacy` maneja dos etiquetas POS dentro del texto:\n",
        "\n",
        "* Para ver la etiqueta POS gruesa, use `token.pos_`.\n",
        "* Para ver la etiqueta de grano fino, use `token.tag_`.\n",
        "\n",
        "Para ver la descripción de cualquier tipo de etiqueta, use `spacy.explain(tag)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee63c30",
      "metadata": {
        "id": "bee63c30"
      },
      "source": [
        "#### **5.4.1. Etiquetas POS de Grano Grueso**\n",
        "---\n",
        "\n",
        "A cada token se le asigna una etiqueta POS de la siguiente lista si usamos el _pipeline_ estandarizado en inglés:\n",
        "\n",
        "<table><tr><th>POS</th><th>Descripción</th><th>Ejemplos</th></tr>\n",
        "    \n",
        "<tr><td>ADJ</td><td>adjective</td><td>big, old, green, incomprehensible, first</td></tr>\n",
        "<tr><td>ADP</td><td>adposition</td><td>in, to, during</td></tr>\n",
        "<tr><td>ADV</td><td>adverb</td><td>very, tomorrow, down, where, there</td></tr>\n",
        "<tr><td>AUX</td><td>auxiliary</td><td>is, has (done), will (do), should (do)</td></tr>\n",
        "<tr><td>CONJ</td><td>conjunction</td><td>and, or, but</td></tr>\n",
        "<tr><td>CCONJ</td><td>coordinating conjunction</td><td>and, or, but</td></tr>\n",
        "<tr><td>DET</td><td>determiner</td><td>a, an, the</td></tr>\n",
        "<tr><td>INTJ</td><td>interjection</td><td>psst, ouch, bravo, hello</td></tr>\n",
        "<tr><td>NOUN</td><td>noun</td><td>girl, cat, tree, air, beauty</td></tr>\n",
        "<tr><td>NUM</td><td>numeral</td><td>1, 2017, one, seventy-seven, IV, MMXIV</td></tr>\n",
        "<tr><td>PART</td><td>particle</td><td>'s, not,</td></tr>\n",
        "<tr><td>PRON</td><td>pronoun</td><td>I, you, he, she, myself, themselves, somebody</td></tr>\n",
        "<tr><td>PROPN</td><td>proper noun</td><td>Mary, John, London, NATO, HBO</td></tr>\n",
        "<tr><td>PUNCT</td><td>punctuation</td><td>., (, ), ?</td></tr>\n",
        "<tr><td>SCONJ</td><td>subordinating conjunction</td><td>if, while, that</td></tr>\n",
        "<tr><td>SYM</td><td>symbol</td><td>$, %, §, ©, +, −, ×, ÷, =, :), 😝</td></tr>\n",
        "<tr><td>VERB</td><td>verb</td><td>run, runs, running, eat, ate, eating</td></tr>\n",
        "<tr><td>X</td><td>other</td><td>sfpksdpsxmsa</td></tr>\n",
        "<tr><td>SPACE</td><td>space</td></tr>\n",
        "</table>\n",
        "\n",
        "Veamos cómo extraer las etiquetas de grano grueso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcc9231",
      "metadata": {
        "id": "afcc9231"
      },
      "outputs": [],
      "source": [
        "pos_tags = [token.pos_ for token in doc]\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9aec377",
      "metadata": {
        "id": "f9aec377"
      },
      "source": [
        "Veamos una descripción más detallada de cada etiqueta usando el método `explain` y mostrando los datos como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e336f8b",
      "metadata": {
        "id": "3e336f8b"
      },
      "outputs": [],
      "source": [
        "explained_tags = [spacy.explain(tag) for tag in pos_tags]\n",
        "tokens = [token.text for token in doc]\n",
        "pd.DataFrame({\"token\": tokens, \"pos\": pos_tags, \"descripción\": explained_tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d34127",
      "metadata": {
        "id": "a9d34127"
      },
      "source": [
        "#### **5.4.1. Etiquetas POS de Grano Fino**\n",
        "---\n",
        "\n",
        "Las etiquetas de grano fino dan más detalles del tipo de palabra que estamos manejando, se pueden ver como subcategorías sobre las etiquetas de grano grueso (mayor detalle). Con el _pipeline_ general en inglés de `spacy` tenemos las siguientes etiquetas:\n",
        "\n",
        "<table>\n",
        "<tr><th>Grano Grueso</th><th>Grano Fino</th><th>Descripción</th><th>Ejemplo</th></tr>\n",
        "<tr><td>ADJ</td><td>AFX</td><td>affix</td><td>The Flintstones were a **pre**-historic family.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJ</td><td>adjective</td><td>This is a **good** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJR</td><td>adjective, comparative</td><td>This is a **better** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJS</td><td>adjective, superlative</td><td>This is the **best** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>PDT</td><td>predeterminer</td><td>Waking up is **half** the battle.</td></tr>\n",
        "<tr><td>ADJ</td><td>PRP\\$</td><td>pronoun, possessive</td><td>**His** arm hurts.</td></tr>\n",
        "<tr><td>ADJ</td><td>WDT</td><td>wh-determiner</td><td>It's blue, **which** is odd.</td></tr>\n",
        "<tr><td>ADJ</td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>We don't know **whose** it is.</td></tr>\n",
        "<tr><td>ADP</td><td>IN</td><td>conjunction, subordinating or preposition</td><td>It arrived **in** a box.</td></tr>\n",
        "<tr><td>ADV</td><td>EX</td><td>existential there</td><td>**There** is cake.</td></tr>\n",
        "<tr><td>ADV</td><td>RB</td><td>adverb</td><td>He ran **quickly**.</td></tr>\n",
        "<tr><td>ADV</td><td>RBR</td><td>adverb, comparative</td><td>He ran **quicker**.</td></tr>\n",
        "<tr><td>ADV</td><td>RBS</td><td>adverb, superlative</td><td>He ran **fastest**.</td></tr>\n",
        "<tr><td>ADV</td><td>WRB</td><td>wh-adverb</td><td>**When** was that?</td></tr>\n",
        "<tr><td>CONJ</td><td>CC</td><td>conjunction, coordinating</td><td>The balloon popped **and** everyone jumped.</td></tr>\n",
        "<tr><td>DET</td><td>DT</td><td>determiner</td><td>**This** is **a** sentence.</td></tr>\n",
        "<tr><td>INTJ</td><td>UH</td><td>interjection</td><td>**Um**, I don't know.</td></tr>\n",
        "<tr><td>NOUN</td><td>NN</td><td>noun, singular or mass</td><td>This is a **sentence**.</td></tr>\n",
        "<tr><td>NOUN</td><td>NNS</td><td>noun, plural</td><td>These are **words**.</td></tr>\n",
        "<tr><td>NOUN</td><td>WP</td><td>wh-pronoun, personal</td><td>**Who** was that?</td></tr>\n",
        "<tr><td>NUM</td><td>CD</td><td>cardinal number</td><td>I want **three** things.</td></tr>\n",
        "<tr><td>PART</td><td>POS</td><td>possessive ending</td><td>Fred**'s** name is short.</td></tr>\n",
        "<tr><td>PART</td><td>RP</td><td>adverb, particle</td><td>Put it **back**!</td></tr>\n",
        "<tr><td>PART</td><td>TO</td><td>infinitival to</td><td>I want **to** go.</td></tr>\n",
        "<tr><td>PRON</td><td>PRP</td><td>pronoun, personal</td><td>**I** want **you** to go.</td></tr>\n",
        "<tr><td>PROPN</td><td>NNP</td><td>noun, proper singular</td><td>**Kilroy** was here.</td></tr>\n",
        "<tr><td>PROPN</td><td>NNPS</td><td>noun, proper plural</td><td>The **Flintstones** were a pre-historic family.</td></tr>\n",
        "<tr><td>VERB</td><td>MD</td><td>verb, modal auxiliary</td><td>This **could** work.</td></tr>\n",
        "<tr><td>VERB</td><td>VB</td><td>verb, base form</td><td>I want to **go**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBD</td><td>verb, past tense</td><td>This **was** a sentence.</td></tr>\n",
        "<tr><td>VERB</td><td>VBG</td><td>verb, gerund or present participle</td><td>I am **going**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBN</td><td>verb, past participle</td><td>The treasure was **lost**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBP</td><td>verb, non-3rd person singular present</td><td>I **want** to go.</td></tr>\n",
        "<tr><td>VERB</td><td>VBZ</td><td>verb, 3rd person singular present</td><td>He **wants** to go.</td></tr>\n",
        "</table>\n",
        "\n",
        "Veamos cómo calcular las etiquetas de grano fino usando el atributo `tag_` del `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd9718e",
      "metadata": {
        "id": "0bd9718e"
      },
      "outputs": [],
      "source": [
        "tags = [token.tag_ for token in doc]\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fa1dbf",
      "metadata": {
        "id": "53fa1dbf"
      },
      "source": [
        "Veamos una descripción más detallada de cada etiqueta usando el método `explain` y mostrando los datos como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5666f004",
      "metadata": {
        "id": "5666f004"
      },
      "outputs": [],
      "source": [
        "explained_tags = [spacy.explain(tag) for tag in tags]\n",
        "tokens = [token.text for token in doc]\n",
        "pd.DataFrame({\"token\": tokens, \"tag\": tags, \"descripción\": explained_tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a844ff84",
      "metadata": {
        "id": "a844ff84"
      },
      "source": [
        "Con las etiquetas de tipo POS podemos filtrar palabras de acuerdo a su función gramatical dentro del texto. Por ejemplo, podemos filtrar todos los nombres propios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115cb09d",
      "metadata": {
        "id": "115cb09d"
      },
      "outputs": [],
      "source": [
        "propn = list(filter(lambda token: token.tag_ == \"NNP\", doc))\n",
        "print(propn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09316763",
      "metadata": {
        "id": "09316763"
      },
      "source": [
        "### **5.5. Parser**\n",
        "---\n",
        "\n",
        "El componente `parser` hace uso de la clase `DependencyParser` para estimar dependencias entre palabras de acuerdo a su función gramatical, en especial, se crean los siguientes atributos sobre un `Token`:\n",
        "\n",
        "* `Token.dep_`: tipo de relación.\n",
        "* `Token.head`: token con el que hay una relación.\n",
        "\n",
        "El proceso de extracción de dependencias busca determinar relaciones y dependencias entre palabras en una oración con el fin de analizar su estructura gramatical.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BgTWAed6WN57C-mQrN55-D8PWBDsVedJ\" width=\"100%\">\n",
        "\n",
        "Existen distintos tipos de dependencias, las cuales puede encontrar en: [https://universaldependencies.org/u/dep/](https://universaldependencies.org/u/dep/). Por lo general, las etiquetas de dependencias se calculan a partir de las etiquetas POS y otros tipos de atributos. Veamos como listar las dependencias en `spacy` usando los atributos `dep_` y `head`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc63321",
      "metadata": {
        "id": "6bc63321"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for token in doc:\n",
        "    data.append(\n",
        "            (token.text, token.dep_, token.head.text)\n",
        "            )\n",
        "pd.DataFrame(\n",
        "        data,\n",
        "        columns=[\"Token 1\", \"Relación\", \"Token 2\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb07bba3",
      "metadata": {
        "id": "cb07bba3"
      },
      "source": [
        "`spacy` cuenta con el módulo `displacy` para generar gráficas y entender las dependencias como un grafo.\n",
        "\n",
        "Veamos un ejemplo donde usamos la función `render`, especificando el documento `doc`, el estilo del gráfico como árbol de dependencias `style = \"dep\"`, especificamos que el gráfico se mostrará en un notebook `jupyter=True` y algunas opciones que permiten modificar la forma en la que se muestra el diagrama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85999151",
      "metadata": {
        "id": "85999151"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=True,\n",
        "        options={\"distance\": 110}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b473eeed",
      "metadata": {
        "id": "b473eeed"
      },
      "source": [
        "Es posible configurar algunas opciones adicionales mediante el argumento  `options`:\n",
        "\n",
        "<table>\n",
        "<tr><th>Opción</th><th>Tipo</th><th>Descripción</th><th>Valor por defecto</th></tr>\n",
        "<tr><td>`compact`</td><td>bool</td><td>\"Modo compacto\" con flechas cuadradas que ocupan menos espacio.</td><td>`False`</td></tr>\n",
        "<tr><td>`color`</td><td>unicode</td><td>Color del texto (HEX, RGB o nombres de colores).</td><td>`#000000`</td></tr>\n",
        "<tr><td>`bg`</td><td>unicode</td><td>Color del fondo (HEX, RGB o nombres de colores).</td><td>`#ffffff`</td></tr>\n",
        "<tr><td>`font`</td><td>unicode</td><td>Fuente para todo el texto.</td><td>`Arial`</td></tr>\n",
        "</table>\n",
        "\n",
        "Para ver la lista de opciones completa: https://spacy.io/api/top-level#displacy_options\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a8c576",
      "metadata": {
        "id": "06a8c576"
      },
      "outputs": [],
      "source": [
        "options = {\n",
        "        \"distance\": 110,\n",
        "        \"compact\": \"True\",\n",
        "        \"color\": \"yellow\",\n",
        "        \"bg\": \"#09a3d5\",\n",
        "        \"font\": \"Times\"\n",
        "        }\n",
        "\n",
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=True,\n",
        "        options=options\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7eb37ee",
      "metadata": {
        "id": "d7eb37ee"
      },
      "source": [
        "También, es posible exportar el gráfico como una imagen en formato `svg`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e861f7a9",
      "metadata": {
        "id": "e861f7a9"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=False,\n",
        "        options=options\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915d363b",
      "metadata": {
        "id": "915d363b"
      },
      "source": [
        "Creamos el archivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087a53b1",
      "metadata": {
        "id": "087a53b1"
      },
      "outputs": [],
      "source": [
        "with open(\"deps.svg\", \"w\") as f:\n",
        "    f.write(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c2d8c",
      "metadata": {
        "id": "241c2d8c"
      },
      "source": [
        "### **5.6. Lemmatizer**\n",
        "---\n",
        "\n",
        "El componente `lemmatizer` se realiza por medio de la clase `Lemmatizer`, este proceso busca reducir una palabra a su raíz. Este procedimiento puede ser aplicado en aquellos casos donde no es necesario distinguir entre singulares y plurales de palabras o diversas conjugaciones de verbos regulares y deseamos agrupar estos tokens en una única representación, su raíz.\n",
        "\n",
        "La _Lemmatization_ es uno de los procedimientos más sencillos para reducir una palabra a su raíz, consiste en cortar las palabras y agruparlas en una raíz común. Por ejemplo, _eat, eating y eaten_ son agrupadas en la palabra _eat_.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17D7d4bXcTuv4XQCT7-NrViKJgBF6hRED\" width=\"75%\">\n",
        "\n",
        "\n",
        "El `Lemmatizer` agrega el atributo `lemma_` sobre cada token con su versión modificada.\n",
        "\n",
        "Veamos cómo podemos extraer los lemmas de los tokens del documento que estamos manejando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385850e0",
      "metadata": {
        "id": "385850e0"
      },
      "outputs": [],
      "source": [
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af823eb",
      "metadata": {
        "id": "3af823eb"
      },
      "source": [
        "A continuación podemos verificar solo aquellos tokens cuyo lemma sea diferente al texto original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fc110a",
      "metadata": {
        "id": "51fc110a"
      },
      "outputs": [],
      "source": [
        "lemmas = [(token.text, token.lemma_) for token in doc if token.text != token.lemma_]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c819713",
      "metadata": {
        "id": "4c819713"
      },
      "source": [
        "### **5.7. NER**\n",
        "---\n",
        "\n",
        "El componente `ner` hace uso de la clase `EntityRecognizer`, esta busca etiquetar segmentos del texto como entidades en un proceso conocido como reconocimiento de entidades nombradas (_Named Entity Recognition_ - NER).\n",
        "\n",
        "NER es una tarea en procesamiento de lenguaje natural que sirve para extraer información estructurada a partir de un texto, por ejemplo: nombres de organizaciones, personas y lugares. También podemos usar NER para extraer entidades como nombres de productos, conceptos médicos, nombres de autores, nombres de marcas, entre otros.\n",
        "\n",
        "Este componente asigna el atributo `Doc.ents` que nos permite identificar en qué lugar y de qué tipo es cada entidad como un objeto de tipo `Span`. Cada entidad tiene los siguientes atributos:\n",
        "\n",
        "| Atributo | Descripción |\n",
        "| --- | --- |\n",
        "| `ent.text` | Texto de la entidad. |\n",
        "| `ent.label_` | Descripción textual de la entidad. |\n",
        "| `ent.start` | Índice que indica el comienzo de la entidad (tokens). |\n",
        "| `ent.end` | Índice que indica el final de la entidad (tokens). |\n",
        "| `ent.start_char` | Caracter inicial de la entidad (caracteres). |\n",
        "| `ent.end_char` | Caracter final de la entidad (caracteres). |\n",
        "\n",
        "Veamos un listado de las entidades en el documento que estamos usando de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390cb2f3",
      "metadata": {
        "id": "390cb2f3"
      },
      "outputs": [],
      "source": [
        "ents = doc.ents\n",
        "print(ents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d055b5",
      "metadata": {
        "id": "e6d055b5"
      },
      "source": [
        "Podemos extraer información de las mismas y mostrarla como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce5b9a21",
      "metadata": {
        "id": "ce5b9a21"
      },
      "outputs": [],
      "source": [
        "ent_text = [ent.text for ent in ents]\n",
        "ent_type = [ent.label_ for ent in ents]\n",
        "ent_start = [ent.start for ent in ents]\n",
        "ent_end = [ent.end for ent in ents]\n",
        "ent_startc = [ent.start_char for ent in ents]\n",
        "ent_endc = [ent.end_char for ent in ents]\n",
        "pd.DataFrame({\n",
        "    \"texto\": ent_text, \"ner\": ent_type, \"palabra_inicial\": ent_start,\n",
        "    \"palabra_final\": ent_end, \"caracter_inicial\": ent_startc,\n",
        "    \"caracter_final\": ent_endc\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576784cb",
      "metadata": {
        "id": "576784cb"
      },
      "source": [
        "El _pipeline_ general en inglés tiene las siguientes opciones para detección de entidades nombradas:\n",
        "\n",
        "<table>\n",
        "<tr><th>Tipo</th><th>Descripción</th><th>Ejemplo</th></tr>\n",
        "<tr><td>PERSON</td><td>Personas, incluyendo personajes ficticios.</td><td>Fred Flintstone</td></tr>\n",
        "<tr><td>NORP</td><td>Nacionalidades o grupos políticos o religiosos.</td><td>The Republican Party</td></tr>\n",
        "<tr><td>FAC</td><td>Edificios, aeropuertos, autopistas, puentes, etc.</td><td>Logan International Airport, The Golden Gate</td></tr>\n",
        "<tr><td>ORG</td><td>Compañías, agencias, instituciones, etc.</td><td>Microsoft, FBI, MIT</td></tr>\n",
        "<tr><td>GPE</td><td>Países, ciudades, estados.</td><td>France, UAR, Chicago, Idaho</td></tr>\n",
        "<tr><td>LOC</td><td>Ubicaciones no geopolíticas, cadenas montañosas, cuerpos de agua.</td><td>Europe, Nile River, Midwest</td></tr>\n",
        "<tr><td>PRODUCT</td><td>Objetos, vehículos, comidas, etc. (No servicios.)</td><td>Formula 1</td></tr>\n",
        "<tr><td>EVENT</td><td>Huracanes nombrados, batallas, guerras, eventos deportivos, etc.</td><td>Olympic Games</td></tr>\n",
        "<tr><td>WORK_OF_ART</td><td>Títulos de libros, canciones, pinturas, etc.</td><td>The Mona Lisa</td></tr>\n",
        "<tr><td>LAW</td><td>Documentos nombrados convertidos en leyes.</td><td>Roe v. Wade</td></tr>\n",
        "<tr><td>LANGUAGE</td><td>Cualquier idioma.</td><td>English</td></tr>\n",
        "<tr><td>DATE</td><td>Fechas o periodos absolutos o relativos.</td><td>*20 July 1969*</td></tr>\n",
        "<tr><td>TIME</td><td>Periodos de tiempo inferiores a un día.</td><td>Four hours</td></tr>\n",
        "<tr><td>PERCENT</td><td>Porcentaje, incluyendo \"%\".</td><td>Eighty percent</td></tr>\n",
        "<tr><td>MONEY</td><td>Valores monetarios, incluyendo unidades.</td><td>Twenty Cents</td></tr>\n",
        "<tr><td>QUANTITY</td><td>Medidas, como de peso o distancia.</td><td>Several kilometers, 55kg</td></tr>\n",
        "<tr><td>ORDINAL</td><td>Valores ordinales (primero, segundo, tercero, etc.).</td><td>9th, Ninth</td></tr>\n",
        "<tr><td>CARDINAL</td><td>Numerales que no entran en otra categoría.</td><td>2, Two, Fifty-two</td></tr>\n",
        "</table>\n",
        "\n",
        "Adicional a esto, podemos usar `displacy` para visualizar entidades nombradas del texto. La única diferencia con respecto a la visualización de dependencias es que en este caso pasamos como argumento `\"ent\"` al parámetro `style`, de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be68fe96",
      "metadata": {
        "id": "be68fe96"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"ent\",\n",
        "        jupyter=True,\n",
        "        options={\"distance\": 110}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b84e732",
      "metadata": {
        "id": "4b84e732"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información útil para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [Guía de uso de Spacy](https://spacy.io/usage).\n",
        "- [Modelos de Spacy](https://spacy.io/models).\n",
        "- [Código fuente de Spacy](https://github.com/explosion/spaCy)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be0ba0b",
      "metadata": {
        "id": "0be0ba0b"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}