{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso1/ciclo4/2_word_tagging_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bd2f51b",
      "metadata": {
        "id": "7bd2f51b"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1Q6vQcIWFPY27isBepABpJ7nroUNKox_Z\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2de99c7b",
      "metadata": {
        "id": "2de99c7b"
      },
      "source": [
        "# **Etiquetado de Palabras**\n",
        "---\n",
        "\n",
        "En este notebook veremos cómo podemos clasificar tokens de forma automática con `spacy`.\n",
        "\n",
        "Comenzamos importando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afdd9744",
      "metadata": {
        "id": "afdd9744"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd\n",
        "from tqdm import tqdm # Sirve para visualizar una barra de progreso\n",
        "from IPython.display import display"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "133b1472",
      "metadata": {
        "id": "133b1472"
      },
      "source": [
        "## **1. Descripción General**\n",
        "---\n",
        "\n",
        "`spacy` nos permite entrenar modelos personalizados y crear nuestros propios _Pipeline_ para tareas específicas. Esto es especialmente útil cuando deseamos realizar tareas específicas en nuestro dominio.\n",
        "\n",
        "El proceso de entrenamiento de `spacy` se puede describir de acuerdo a la siguiente figura:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1a-lk4nZMe7Vqx4cR_v6Cejhneibc3QOr\" width=\"80%\">\n",
        "\n",
        "En este caso, se usan datos de entrenamiento compuestos de textos y etiquetas (normalmente para clasificación de secuencias) y por medio de técnicas de optimización basadas en gradiente se entrena un modelo basado en redes neuronales. Dicho modelo posteriormente se usa para hacer predicciones automáticas y se puede almacenar y exportar.\n",
        "\n",
        "En este notebook veremos un ejemplo de reconocimiento de entidades nombradas (NER) personalizado, donde buscaremos etiquetar de forma automática textos en categorías específicas. Por ejemplo:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=10hr8FWCVUKb-fN3nSiWa0l3jjO-1E0hi\" width=\"80%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404d22d7",
      "metadata": {
        "id": "404d22d7"
      },
      "source": [
        "## **2. Conjunto de Datos de NER**\n",
        "---\n",
        "\n",
        "Es importante saber estructurar el conjunto de datos para poder entrenar un modelo personalizado de `spacy`, en este caso usaremos el conjunto de datos [Entity Annotated Corpus](https://www.kaggle.com/abhinavwalia95/entity-annotated-corpus) de Kaggle.\n",
        "\n",
        "Comenzamos cargándolo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "23790368",
      "metadata": {
        "id": "23790368"
      },
      "outputs": [],
      "source": [
        "data = pd.read_parquet(\"https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u4/ner_dataset.parquet\")\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54daa76a",
      "metadata": {
        "id": "54daa76a"
      },
      "source": [
        "Como podemos ver, tenemos 4 columnas:\n",
        "\n",
        "- `sentence`: identificador de la oración.\n",
        "- `token`: palabra dentro del texto.\n",
        "- `pos`: etiqueta de tipo _POS_.\n",
        "- `ner`: etiqueta de tipo _NER_.\n",
        "\n",
        "Veamos qué etiquetas de tipo _NER_ tiene el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52cee5ca",
      "metadata": {
        "id": "52cee5ca"
      },
      "outputs": [],
      "source": [
        "labels = data.ner.unique()\n",
        "display(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de30975b",
      "metadata": {
        "id": "de30975b"
      },
      "source": [
        "Estas corresponden a:\n",
        "\n",
        "- `o`: ninguna entidad asociada.\n",
        "- `geo`: lugar geográfico.\n",
        "- `gpe`: entidad geopolítica.\n",
        "- `per`: persona.\n",
        "- `org`: organización.\n",
        "- `tim`: indicador de tiempo.\n",
        "- `art`: artefacto.\n",
        "- `nat`: fenómeno natural.\n",
        "- `eve`: evento.\n",
        "\n",
        "Podemos extraer una oración del texto de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d37e4b",
      "metadata": {
        "id": "69d37e4b"
      },
      "outputs": [],
      "source": [
        "sent_id = 1\n",
        "text = \" \".join(\n",
        "        data\n",
        "        .query(f\"sentence == '{sent_id}'\")\n",
        "        .token.to_list()\n",
        "        )\n",
        "display(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b120a523",
      "metadata": {
        "id": "b120a523"
      },
      "source": [
        "También podemos ver las entidades nombradas asociadas a este texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5dee9ba2",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5dee9ba2"
      },
      "outputs": [],
      "source": [
        "sent_id = 1\n",
        "text = \" \".join(\n",
        "        data\n",
        "        .query(f\"sentence == '{sent_id}'\")\n",
        "        .ner.to_list()\n",
        "        )\n",
        "display(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb07f664",
      "metadata": {
        "id": "fb07f664"
      },
      "source": [
        "Para entrenar un modelo de `spacy` debemos extraer el texto completo y asociar cada etiqueta a su posición.\n",
        "\n",
        "Definimos la función `convert_sentence` para transformar cada oración del corpus en un formato compatible con `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "490d12cc",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "490d12cc"
      },
      "outputs": [],
      "source": [
        "def convert_sentence(df):\n",
        "    sentence = \" \".join(df[\"token\"]) # extraemos el texto completo\n",
        "    tags = []\n",
        "    pos = 0\n",
        "    for _, row in df.iterrows():\n",
        "        if row[\"ner\"] != 'o': # Filtramos los tokens con entidad\n",
        "            tags.append(\n",
        "                    (pos, pos + len(row[\"token\"]), row[\"ner\"])\n",
        "                    )\n",
        "        pos += len(row[\"token\"]) + 1 # asignamos la posición de cada token.\n",
        "    return (sentence, {\"entities\": tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b2eccfa",
      "metadata": {
        "id": "0b2eccfa"
      },
      "source": [
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2de8b226",
      "metadata": {
        "id": "2de8b226"
      },
      "outputs": [],
      "source": [
        "sent = data.query(\"sentence == '1'\")\n",
        "sent_conv = convert_sentence(sent)\n",
        "display(sent_conv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45616450",
      "metadata": {
        "id": "45616450"
      },
      "source": [
        "Podemos ver que una oración se codifica como una tupla de dos valores:\n",
        "\n",
        "1. El texto completo.\n",
        "2. Un diccionario donde se establece el inicio, el final y el tipo de una entidad dentro del texto."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b88cd65a",
      "metadata": {
        "id": "b88cd65a"
      },
      "source": [
        "Podemos aplicar esta función a cada una de las oraciones del texto de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d590f1be",
      "metadata": {
        "id": "d590f1be"
      },
      "outputs": [],
      "source": [
        "corpus = (\n",
        "        data\n",
        "        .groupby(\"sentence\")\n",
        "        .apply(convert_sentence)\n",
        "        .tolist()\n",
        "        )\n",
        "display(corpus[:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "357d1bb7",
      "metadata": {
        "id": "357d1bb7"
      },
      "source": [
        "> **Nota**: como puede ver, estamos usando un conjunto de datos con etiquetas personalizadas. Si desea realizar cualquier tarea de clasificación de tokens lo puede hacer siempre que cuente con un conjunto de datos etiquetado de una forma similar al que tenemos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c834344",
      "metadata": {
        "id": "5c834344"
      },
      "source": [
        "## **3. Pipeline Personalizado de Spacy**\n",
        "---\n",
        "\n",
        "Podemos definir un _Pipeline_ personalizado para cualquier lenguaje soportado por `spacy`. Recuerde que hay reglas (como tokenizado, stopwords, entre otras) que son propias del lenguaje y no requieren ningún modelo específico.\n",
        "\n",
        "En este caso definiremos un _Pipeline_ en blanco para inglés:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69b1381b",
      "metadata": {
        "id": "69b1381b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "display(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "523f8a29",
      "metadata": {
        "id": "523f8a29"
      },
      "source": [
        "Veamos los componentes que tiene este _Pipeline_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3225dd05",
      "metadata": {
        "id": "3225dd05"
      },
      "outputs": [],
      "source": [
        "display(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "572acbd8",
      "metadata": {
        "id": "572acbd8"
      },
      "source": [
        "Como podemos ver, hasta el momento no debería tener ningún componente. Vamos a agregar un componente de `ner`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "110448b4",
      "metadata": {
        "id": "110448b4"
      },
      "outputs": [],
      "source": [
        "ner = nlp.add_pipe(\"ner\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3047d9df",
      "metadata": {
        "id": "3047d9df"
      },
      "source": [
        "Ahora, podemos validar que exista este componente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d533909d",
      "metadata": {
        "id": "d533909d"
      },
      "outputs": [],
      "source": [
        "display(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71fe649b",
      "metadata": {
        "id": "71fe649b"
      },
      "source": [
        "El componente `ner` debe saber qué etiquetas debería clasificar. Estas las podemos obtener del `DataFrame`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8155a0c5",
      "metadata": {
        "id": "8155a0c5"
      },
      "outputs": [],
      "source": [
        "labels = data.ner.unique()\n",
        "labels = labels[labels != 'o']\n",
        "display(labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efbbba81",
      "metadata": {
        "id": "efbbba81"
      },
      "source": [
        "Agregamos estas etiquetas al `ner`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a3b868",
      "metadata": {
        "id": "d1a3b868"
      },
      "outputs": [],
      "source": [
        "for label in labels:\n",
        "    ner.add_label(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "86469471",
      "metadata": {
        "id": "86469471"
      },
      "source": [
        "Podemos validarlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "764ded93",
      "metadata": {
        "id": "764ded93"
      },
      "outputs": [],
      "source": [
        "display(ner.labels)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "121ff594",
      "metadata": {
        "id": "121ff594"
      },
      "source": [
        "## **4. Entrenamiento**\n",
        "---\n",
        "\n",
        "Para el entrenamiento del _Pipeline_ personalizado, debemos usar los siguientes tres elementos de `spacy`:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1gXNxwthy-lJtpbcQhabFs_R6Bof10YjN\" width=\"80%\">\n",
        "\n",
        "- `minibatch`: el modelo es entrenado por lotes, es decir, no se utiliza el corpus completo para entrenar el modelo, sino que se usa un grupo pequeño de documentos (batch) para ir ajustándolo de forma iterativa. Esto permite que el modelo pueda ser entrenado con grandes cantidades de datos. El `minibatch` nos permite definir este subconjunto de datos.\n",
        "- `compounding`: se trata de una estrategia para modificar de forma iterativa el tamaño del batch en el entrenamiento. En este caso, requiere un número mínimo y máximo de oraciones en lote y una tasa para modificar el tamaño del batch de forma incremental.\n",
        "- `Example`: define una muestra en un formato compatible con `spacy`.\n",
        "\n",
        "Importamos estos elementos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d311983",
      "metadata": {
        "id": "2d311983"
      },
      "outputs": [],
      "source": [
        "from spacy.util import minibatch, compounding\n",
        "from spacy.training import Example"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c386f1a2",
      "metadata": {
        "id": "c386f1a2"
      },
      "source": [
        "Ahora, creamos los batches:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b425c2f5",
      "metadata": {
        "id": "b425c2f5"
      },
      "outputs": [],
      "source": [
        "batches = minibatch(\n",
        "        items = corpus,\n",
        "        size = compounding(\n",
        "            start=4, stop=32, compound=1.01\n",
        "            )\n",
        "        )\n",
        "display(batches)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dfab59f",
      "metadata": {
        "id": "0dfab59f"
      },
      "source": [
        "El resultado es un `generator` de _Python_, este objeto nos permite extraer batches de forma iterativa. Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db1db24e",
      "metadata": {
        "id": "db1db24e"
      },
      "outputs": [],
      "source": [
        "batch = next(batches)\n",
        "display(len(batch))\n",
        "display(batch)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5a58f74a",
      "metadata": {
        "id": "5a58f74a"
      },
      "source": [
        "Como podemos ver, obtenemos 4 oraciones, lo cual corresponde al tamaño inicial del `compounding` que definimos.\n",
        "\n",
        "Ahora, podemos definir el entrenamiento del modelo completo, para ello vamos a definir los siguientes hiperparámetros:\n",
        "\n",
        "- `ITERS`: número de iteraciones de entrenamiento del modelo.\n",
        "- `LR`: tasa de aprendizaje, permite controlar el sobre y subajuste del modelo (de esto se habla con mayor profundidad en el módulo de _Deep Learning_).\n",
        "- `DROP`: regularización de redes neuronales, toma valores entre 0 y 1 (de esto se habla con mayor profundidad en el módulo de _Deep Learning_)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4d05c35a",
      "metadata": {
        "id": "4d05c35a"
      },
      "outputs": [],
      "source": [
        "ITERS = 100\n",
        "LR = 1e-3\n",
        "DROP = 0.3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "609034b2",
      "metadata": {
        "id": "609034b2"
      },
      "source": [
        "Ahora, definimos un optimizador (el que se encarga de entrenar el modelo):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4233884",
      "metadata": {
        "id": "b4233884"
      },
      "outputs": [],
      "source": [
        "optimizer = nlp.begin_training()\n",
        "display(optimizer)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0c51d44c",
      "metadata": {
        "id": "0c51d44c"
      },
      "source": [
        "Especificamos la tasa de aprendizaje como atributo del optimizador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36bf25a9",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "36bf25a9"
      },
      "outputs": [],
      "source": [
        "optimizer.learn_rate = LR"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b169b071",
      "metadata": {
        "id": "b169b071"
      },
      "source": [
        "Finalmente, definimos una función que se encargará del entrenamiento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6db8334",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "b6db8334"
      },
      "outputs": [],
      "source": [
        "def train(nlp, batches, n_iter, optimizer, drop):\n",
        "    # Estructura donde se almacenarán las pérdidas del modelo.\n",
        "    losses = {}\n",
        "    # Pérdida acumulada\n",
        "    loss = 0\n",
        "    # Barra de progreso\n",
        "    pbar = tqdm(range(n_iter))\n",
        "    # Iteramos por un número de iteraciones\n",
        "    for _ in pbar:\n",
        "        # Extraemos un batch de datos:\n",
        "        batch = next(batches)\n",
        "\n",
        "        # Convertimos el corpus a ejemplos de spacy.\n",
        "        examples = []\n",
        "        for case in batch:\n",
        "            example = Example.from_dict(\n",
        "                    nlp.make_doc(case[0]), case[1]\n",
        "                    )\n",
        "            examples.append(example)\n",
        "        # Ajustamos el modelo con el batch de datos\n",
        "        nlp.update(\n",
        "            examples,\n",
        "            sgd=optimizer,\n",
        "            drop=drop,\n",
        "            losses=losses\n",
        "            )\n",
        "\n",
        "        # Imprimimos la pérdida del modelo.\n",
        "        cur_loss = losses[\"ner\"] - loss\n",
        "        pbar.set_description(f\"Loss: {cur_loss}\")\n",
        "        loss = losses[\"ner\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fbbe2993",
      "metadata": {
        "id": "fbbe2993"
      },
      "source": [
        "Ahora, entrenamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a4b461b",
      "metadata": {
        "id": "0a4b461b"
      },
      "outputs": [],
      "source": [
        "train(nlp, batches, ITERS, optimizer, DROP)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6618b2cc",
      "metadata": {
        "id": "6618b2cc"
      },
      "source": [
        "## **5. Aplicación**\n",
        "---\n",
        "\n",
        "Por último, veamos el modelo en funcionamiento, podemos extraer una oración del corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16716a19",
      "metadata": {
        "id": "16716a19"
      },
      "outputs": [],
      "source": [
        "text = corpus[0][0]\n",
        "display(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5d67970",
      "metadata": {
        "id": "c5d67970"
      },
      "source": [
        "Podemos crear un documento de `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5d1a54a",
      "metadata": {
        "id": "b5d1a54a"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce4f68ad",
      "metadata": {
        "id": "ce4f68ad"
      },
      "source": [
        "Finalmente, veamos las entidades:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c7f4c9b",
      "metadata": {
        "id": "0c7f4c9b"
      },
      "outputs": [],
      "source": [
        "doc.ents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ede737b0",
      "metadata": {
        "id": "ede737b0"
      },
      "source": [
        "También podemos ver sus tipos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5deb52cf",
      "metadata": {
        "id": "5deb52cf"
      },
      "outputs": [],
      "source": [
        "for token in doc:\n",
        "    if token.ent_type_ :\n",
        "        display(f\"{token.text} - {token.ent_type_}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119eb6c3",
      "metadata": {
        "id": "119eb6c3"
      },
      "source": [
        "También es posible usar `displacy` para visualizar las entidades:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fb05d6aa",
      "metadata": {
        "id": "fb05d6aa"
      },
      "outputs": [],
      "source": [
        "from spacy import displacy"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0200095",
      "metadata": {
        "id": "d0200095"
      },
      "source": [
        "Veamos un gráfico de entidades:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "613af5c2",
      "metadata": {
        "id": "613af5c2"
      },
      "outputs": [],
      "source": [
        "svg = displacy.render(doc, jupyter=True, style=\"ent\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a1b34b5",
      "metadata": {
        "id": "7a1b34b5"
      },
      "source": [
        "Por último, podemos exportar el _Pipeline_ personalizado con el método `to_disk`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "89d19174",
      "metadata": {
        "id": "89d19174"
      },
      "outputs": [],
      "source": [
        "nlp.to_disk(\"mypipe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38e86c9b",
      "metadata": {
        "id": "38e86c9b"
      },
      "source": [
        "Esto genera una carpeta con todos los componentes del modelo. Podemos visualizarlos con la utilidad `tree`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff280ffa",
      "metadata": {
        "id": "ff280ffa"
      },
      "outputs": [],
      "source": [
        "!apt install tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "098673af",
      "metadata": {
        "id": "098673af"
      },
      "outputs": [],
      "source": [
        "!tree mypipe/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8a9c2bf2",
      "metadata": {
        "id": "8a9c2bf2"
      },
      "source": [
        "Este _Pipeline_ se puede cargar como cualquier otro modelo de `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ed8c33",
      "metadata": {
        "id": "48ed8c33"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"mypipe\")\n",
        "display(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04df5ac9",
      "metadata": {
        "id": "04df5ac9"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [Training Pipelines & models](https://spacy.io/usage/training).\n",
        "- [Thinc for deep learning](https://thinc.ai/).\n",
        "- _Fuente de los íconos_\n",
        "     - Flaticon. Justify free icon [PNG]. https://www.flaticon.com/free-icon/justify_6935287\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b0d31a7",
      "metadata": {
        "id": "0b0d31a7"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}