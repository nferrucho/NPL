{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo3/M5U3_Introducci%C3%B3n_Redes_Neuronales_Convolucionales_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1kl2OFnF2FADAAKjgZUFPo8dsBQUSIJM7\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "metadata": {
        "id": "jYN1jK9zMaq0"
      },
      "id": "jYN1jK9zMaq0"
    },
    {
      "cell_type": "markdown",
      "id": "ec1cd52e",
      "metadata": {
        "id": "ec1cd52e"
      },
      "source": [
        "\n",
        "#**Introducción a las Redes Neuronales Convolucionales (CNN)**\n",
        "----\n",
        "\n",
        "Como vimos en la unidad anterior, **_Keras_** ofrece una gran variedad de capas o *layers* para la construcción de modelos. En esta unidad, nos concentraremos en estudiar **las redes neuronales convolucionales**, las cuales son uno de los modelos más usados para el análisis de imagenes. Inicialmente, se mostrarán dos de los componentes principales de estas redes: **la convolución y el _pooling_**. Finalmente se presentará un ejemplo práctico de este tipo de red.\n",
        "\n",
        "Veremos:\n",
        "\n",
        "- Capas convolucionales\n",
        "- Capas de *Pooling*\n",
        "- Redes Neuronales Convolucionales\n",
        "- *Data augmentation*\n",
        "- Entrenamiento y visualización\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Primero importatemos los paquetes necesarios :"
      ],
      "metadata": {
        "id": "SE0OM-Mw2e-H"
      },
      "id": "SE0OM-Mw2e-H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0db63ca5",
      "metadata": {
        "id": "0db63ca5"
      },
      "outputs": [],
      "source": [
        "# Seleccionamos la versión más reciente de Tensorflow 2.0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "# Seleccionamos una semilla para los RNG\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ddb8af31",
      "metadata": {
        "id": "ddb8af31"
      },
      "source": [
        "# **1. Introducción**\n",
        "----\n",
        "\n",
        "Dos de las tareas más populares en el análisis de imágenes automático son la **identificación y clasificación de objetos dentro de una imagen**.\n",
        "\n",
        "Anteriormente, hemos estudiado las redes neuronales multicapa, las cuales pueden usarse en tareas tanto de regresión como de clasificación. En general, se tratan de modelos basados en transformaciones no líneales (gracias a la función de activación) de sumas ponderadas de un vector de entrada. Sin embargo, cuando trabajamos con **imágenes**, no es muy apropiado utilizar directamente un perceptrón multicapa debido a las siguientes razones :\n",
        "\n",
        "  1) El perceptrón multicapa encuentra **patrones fijos** en las variables, no obstante, no es capaz de capturar propiedades comunes de las imágenes como la invarianza a escala, la rotación y la traslación.\n",
        "\n",
        "  2) Es un modelo que **conecta densamente** cada una de las variables de entrada con la capa siguiente. Esto es un inconveniente cuando los datos que tenemos tienen muchas característica como las imágenes, las cuales se caracterizan por estar compuestas por muchos **pixeles**.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Wn7EQ8Ex0mgca0B07ReW5uyEAf1N4Q3H\" width=\"40%\" /></center>\n",
        "\n",
        "  - Por ejemplo, si tenemos imágenes a color (RGB) de tamaño $256\\times256$, el vector de entrada de la red sería de tamaño $196.608$ ($256\\times256\\times3$). Así mismo, si quisiéramos utilizar una capa intermedia de tamaño 8 (lo cual es un valor muy pequeño para representar toda la información en una imagen) necesitaríamos una matriz de pesos de tamaño $1'572.864$. Es decir, el número de pesos del modelo puede crecer muy fácilmente.\n",
        "\n",
        "Una alternativa a lo expuesto anteriormente es determinar **un conjunto de características** que capturen de forma apropiada la información en las imágenes y que sean de una dimensión apropiada para poder utilizar un perceptrón multicapa.\n",
        "\n",
        "- A este proceso se le conoce como **feature engineering** y es un campo que ha sido muy estudiado en áreas como el procesamiento de imágenes y visión por computador.\n",
        "\n",
        "No obstante, determinar un conjunto de características apropiado para una tarea en específico puede llegar a ser dispendioso y difícil. Por ello, surge un gran interés en métodos que puedan aprender a extraer características de forma automática, lo cual es conocido como **feature learning**.\n",
        "\n",
        "\n",
        "## 1.1 Redes neuronales convolucionales\n",
        "\n",
        "Las **redes neuronales convolucionales** o *Convolutional Neural Networks* (CNN), están inspiradas en los procesos biológicos que tienen lugar en la corteza cerebral, donde las neuronas individuales responden a estímulos de un área restringida del campo visual. Las CNNs son capaces de aprender características visuales de bajo nivel que después son agrupadas en patrones de más alto nivel llevando a cabo un proceso de *feauture learning*.\n",
        "\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1NYS3i5LhYAsrlMyzPf9aJe5QuVtXqYy6\" width=\"90%\" /></center>\n",
        "\n",
        "A diferencia del perceptrón multicapa, en una CNN **los pesos no se asignan a un píxel en específico** sino que son compartidos por varios pixeles por medio de una operación conocida como **convolución** (la cual no se ve afectada por efectos de traslación), además, incluyen etapas de submuestreo que permiten analizar una imagen a varias escalas. Como resultado, las CNNs aprenden a responder a diferentes características en una imagen (bordes, formas, entre otros), tal y como funcionan los bancos de filtros (los cuales requieren ser definidos de forma manual) comúnmente usados en los algoritmos tradicionales.\n",
        "- La capacidad de aprender dichos filtros supone una **ventaja única de las CNNs**, que elimina el esfuerzo manual requerido en el diseño de características.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe906b4d",
      "metadata": {
        "id": "fe906b4d"
      },
      "source": [
        "# **2. Convolución**\n",
        "----\n",
        "\n",
        "La **convolución** es conocida por que es ampliamente usada en distintas aplicaciones como :\n",
        "- Ecuaciones diferenciales\n",
        "- Procesamiento de señales\n",
        "- Procesamiento de imágenes\n",
        "- Visión por computador.\n",
        "\n",
        "Se trata de **una operación matemática** resultante de la suma ponderada entre una imagen $\\mathbf{I}$ y un filtro móvil $\\mathbf{H}$. Este concepto se puede entender más fácilmente de forma gráfica, tal y como se muestra a continuación:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1QIRMoaS4xVBjCRo9XqwK42AGyGeeIqo2\" width=\"50%\" /></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b13cd3b0",
      "metadata": {
        "id": "b13cd3b0"
      },
      "source": [
        "En este caso, un filtro de dimensión $3\\times 3$ recorre la imagen (convolución 2D) dando pasos de un pixel y estará conformado por los siguientes elementos:\n",
        "\n",
        "$$ \\mathbf{H}=\\left(\\begin{array}{cc}\n",
        "0 & -1 & 0 \\\\\n",
        "-1 & 5 & -1 \\\\\n",
        "0 & -1 & 0 \\\\  \n",
        "\\end{array}\\right)$$\n",
        "\n",
        "Los elementos de dicho filtro son multiplicados **elemento a elemento** por una sección de la imágen. Posteriormente, los resultados obtenidos son sumados de forma que 9 pixeles dentro de la imagen original pasan a ser representados por un único pixel.\n",
        "\n",
        "- Adicionalmente, la convolución se puede aplicar en imágenes a color (RGB), repitiendo la operación en cada canal como se muestra a continuación :\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1QNyZEP-Jvc7WfVMsrUoFNmEzyfRXJTho\" width=\"60%\" /></center>\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1dUzEHGNDI6w7-7je_E4NwHzDDaUN66Zb\" width=\"40%\" /></center>\n",
        "\n",
        "La convolución tiene varias propiedades que la hacen bastante útil, por ejemplo :\n",
        "- Permite **aproximar diferencias finitas** de cualquier orden (gradiente, laplaciano, entre otros).\n",
        "- Permite **suavizar imagenes** (para eliminar ruido) e incluso puede ser usada para resaltar detalles o patrones.\n",
        "\n",
        "Tradicionalmente, los filtros se diseñaban manualmente para cada tarea en específico. Veamos algunos ejemplos de filtros y el resultado de una convolución utilizando _TensorFlow_ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "918aa902",
      "metadata": {
        "id": "918aa902"
      },
      "outputs": [],
      "source": [
        "# Utilizaremos una imagen de ejemplo tomada de: Michael Plotke - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=24301122\n",
        "!wget -O animal.png https://upload.wikimedia.org/wikipedia/commons/5/50/Vd-Orig.png?20130129005336"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "78aa63b5",
      "metadata": {
        "id": "78aa63b5"
      },
      "outputs": [],
      "source": [
        "# Cargamos la imagen en formato RGB\n",
        "im = np.array(tf.keras.preprocessing.image.load_img(\"animal.png\"))\n",
        "print(f\"Dimensiones de la imagen a color: {im.shape}\")\n",
        "# Convertimos la imagen a grises\n",
        "im_gray = tf.constant(np.mean(im,axis=2).astype(np.float32))\n",
        "print(f\"Dimensiones de la imagen en escala de grises: {im_gray.shape}\\n\\n\\n\")\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(121)\n",
        "plt.imshow(im)\n",
        "plt.axis(\"off\"); plt.title(\"Imágen de ejemplo\")\n",
        "plt.subplot(122)\n",
        "plt.imshow(im_gray,cmap=\"gray\")\n",
        "plt.axis(\"off\"); plt.title(\"Imágen en grises\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9bd0708",
      "metadata": {
        "id": "e9bd0708"
      },
      "source": [
        "Ahora, veamos el efecto de aplicar una convolución. Para el ejemplo, usaremos filtros **Sobel**, usados para detección de bordes."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definimos un filtro sobel horizontal\n",
        "# -1  0  1\n",
        "# -2  0  2\n",
        "# -1  0  1\n",
        "H1 = tf.constant([[-1, 0, 1],\n",
        "                  [-2, 0, 2],\n",
        "                  [-1, 0, 1]], tf.float32)\n",
        "# Ajustamos las dimensiones del kernel para poder usar la función de Tensorflow\n",
        "H1 = tf.reshape(H1, [3, 3, 1, 1])\n",
        "\n",
        "# Definimos un filtro sobel vertical\n",
        "#  1  2  1\n",
        "#  0  0  0\n",
        "# -1 -2 -1\n",
        "H2 = tf.transpose(tf.constant([[-1, 0, 1],\n",
        "                               [-2, 0, 2],\n",
        "                               [-1, 0, 1]], tf.float32))\n",
        "# Ajustamos las dimensiones del kernel para poder usar la función de Tensorflow\n",
        "H2 = tf.reshape(H2, [3, 3, 1, 1])\n",
        "\n",
        "# Definimos un filtro de derivada\n",
        "#  0  1  0\n",
        "#  1 -4  1\n",
        "#  0  1  0\n",
        "H3 = tf.transpose(tf.constant([[0,  1,  0],\n",
        "                               [1, -4,  1],\n",
        "                               [0,  1,  0]], tf.float32))\n",
        "# Ajustamos las dimensiones del kernel para poder usar la función de Tensorflow\n",
        "H3 = tf.reshape(H3, [3, 3, 1, 1])\n",
        "\n",
        "# Definimos un filtro box blur\n",
        "# 1/9 1/9 1/9\n",
        "# 1/9 1/9 1/9\n",
        "# 1/9 1/9 1/9\n",
        "\n",
        "h4 = np.zeros((3, 3, 3, 3))\n",
        "# Asignamos el filtro para cada canal\n",
        "for i in range(3):\n",
        "    h4[:, :, i, i] = np.ones((3, 3))/9.\n",
        "# Construimos el tensor\n",
        "H4 = tf.constant(h4, tf.float32)\n",
        "\n",
        "# Ajustamos las dimensiones de la imagen\n",
        "image_resized = tf.expand_dims(tf.expand_dims(im_gray, 0), 3)\n",
        "print(f\"Dimensiones de la imagen ajustadas: {image_resized.shape}\\n\\n\")"
      ],
      "metadata": {
        "id": "_VA1wY7rRadC"
      },
      "id": "_VA1wY7rRadC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Aplicamos los primeros tres filtros sobre la imagen en grises y mostramos los resultados. Usaremos la función `tf.nn.conv2d` que nos permite aplicar convoluciones definiendo kernels o filtros particulares. `tf.nn.conv2d` recible los siguientes argumentos:\n",
        "\n",
        "*  `input`: Un tensor de rango al menos 4. Por lo general la primera dimensión se asocia al indice en el batch de imágenes, y las últimas 3 a las dimensiones propias de las imágenes.\n",
        "*  `filters`: Un tensor 4-D de forma `[altura_filtro, anchura_filtro, canales_entrada, canales_salida]`. Es el filtro convolucional.\n",
        "*  `strides`: Un `int` o lista de `int`'s de longitud 1, 2 ó 4. El tamaño de paso de la ventana deslizante para cada dimensión de entrada. Si se da un único valor, se replica en todas las dimensiones.\n",
        "*  `padding`: Puede ser la cadena `\"SAME\"` o `\"VALID\"`, que indica el tipo de algoritmo de relleno que se va a utilizar. `\"VALID\"` indica que no se utilice relleno. Esto hace que el tamaño de salida sea normalmente menor que el tamaño de entrada, incluso cuando el `stride` es 1. Con el relleno `\"SAME\"`, el relleno se aplica a cada dimensión espacial. Cuando el `stride` es 1, la entrada se rellena de forma que el tamaño de salida sea el mismo que el de entrada.\n",
        "\n"
      ],
      "metadata": {
        "id": "uL0t-TI8RZ11"
      },
      "id": "uL0t-TI8RZ11"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c6d10b2",
      "metadata": {
        "id": "5c6d10b2"
      },
      "outputs": [],
      "source": [
        "I1 = tf.nn.conv2d(image_resized, filters=H1,\n",
        "                  strides=[1, 1], padding='SAME')\n",
        "I2 = tf.nn.conv2d(image_resized, filters=H2,\n",
        "                  strides=[1, 1], padding='SAME')\n",
        "I3 = tf.nn.conv2d(image_resized, filters=H3,\n",
        "                  strides=[1, 1], padding='SAME')\n",
        "\n",
        "# Mostramos los resultados\n",
        "\n",
        "ims=[I1[0,:,:,0],I2[0,:,:,0],I3[0,:,:,0]]\n",
        "names=[\"Sobel x\", \"Sobel y\", \"Derivada\"]\n",
        "fig,ax=plt.subplots(1,3,figsize=(10,10))\n",
        "for i in range(3):\n",
        "    ax[i].imshow(ims[i], cmap=\"gray\")\n",
        "    ax[i].set_title(names[i]); ax[i].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora aplicamos el último filtro sobre la imagen a color:"
      ],
      "metadata": {
        "id": "wEX6Qs19S7fc"
      },
      "id": "wEX6Qs19S7fc"
    },
    {
      "cell_type": "code",
      "source": [
        "image_resized = tf.expand_dims(tf.constant(im, tf.float32), 0)\n",
        "I4 = tf.nn.conv2d(image_resized, filters=H4,\n",
        "                  strides=[1, 1], padding='SAME')\n",
        "\n",
        "# Mostramos los resultados\n",
        "\n",
        "ims = tf.cast(I4[0, :, :, :],tf.uint8)\n",
        "name = \"Box Blur\"\n",
        "fig,ax=plt.subplots(1,1,figsize=(5,5))\n",
        "ax.imshow(ims, cmap=\"gray\")\n",
        "ax.set_title(name); ax.axis(\"off\")"
      ],
      "metadata": {
        "id": "yy4CyzEjRhMe"
      },
      "id": "yy4CyzEjRhMe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "fae3af01",
      "metadata": {
        "id": "fae3af01"
      },
      "source": [
        "_Keras_ provee una capa especial de convolución `tf.keras.layers.Conv2D` para ser utilizada en conjunto con las redes neuronales. Se trata de un bloque funcional que contiene unos pesos (varios filtros) y permite realizar la operación de convolución. A diferencia de `tf.nn.conv2d`, con `tf.keras.layers.Conv2D` no definimos explicitamente los filtros; definimos cuántos filtros vamos a usar, y se aprenden luego durante el entrenamiento. `tf.keras.layers.Conv2D` tiene los siguientes parámetros:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1kikAVAeIowl-uvT6xIDP5mGotFFocFvt\" width=\"80%\" /></center>\n",
        "\n",
        "* ```filters```: El número de filtros. Una capa convolucional se compone de varias operaciones de convolución, es decir, contiene varios filtros.\n",
        "* ```kernel_size```: Define el tamaño de los filtros que se utilizarán en las convoluciones, por ejemplo, la tupla ```(3,3)``` define una matríz de tamaño $3\\times 3$.\n",
        "* ```strides```: Se trata del tamaño del salto en la convolución. Es decir, permite controlar el número de pixeles que se desplaza el filtro en cada iteración de la convolución. Por ejemplo, la tupla ```(2,1)``` define que el filtro se mueve de a dos pixeles en el primer eje y de a un pixel en el segundo.\n",
        "* ```padding```: Define cómo se manejan los bordes de la imagen resultante. Por un lado, una convolución de tamaño igual (```\"same\"```) mantendrá las dimensiones de la imagen resultante del mismo tamaño de la imagen original al agregar ceros alrededor de los límites de la imagen de entrada cuando sea necesario. Por otro lado, una convolución válida (```\"valid\"```) sólo realiza la convolución en los pixeles donde sea posible, es decir, la convolución no se aplica en los límites y por ende el tamaño de la imagen resultante es menor.\n",
        "* ```activation```: Al igual que en las redes multicapa, se puede utilizar una función de activación para agregar no-linealidad a las representaciones.\n",
        "\n",
        "_Keras_ inicializa los filtros de una capa convolucional de forma aleatoria para posteriormente aprenderlos, veamos un ejemplo de esto:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24c1fcd7",
      "metadata": {
        "id": "24c1fcd7"
      },
      "outputs": [],
      "source": [
        "# Definimos una capa de entrada\n",
        "inp = tf.keras.layers.Input(shape=(100, 100, 3))\n",
        "# Definimos una capa convolucional de ejemplo conectada a la entrada\n",
        "conv = tf.keras.layers.Conv2D(filters=4,\n",
        "                              kernel_size=(5,5),\n",
        "                              strides=(1,1),\n",
        "                              padding=\"same\",\n",
        "                              activation=tf.nn.sigmoid)(inp)\n",
        "# Definimos un modelo\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[conv])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb47fa35",
      "metadata": {
        "id": "fb47fa35"
      },
      "source": [
        "Veamos los filtros de la capa convolucional :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0757636",
      "metadata": {
        "id": "a0757636"
      },
      "outputs": [],
      "source": [
        "model.layers[1].weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50f7e7a2",
      "metadata": {
        "id": "50f7e7a2"
      },
      "outputs": [],
      "source": [
        "model.layers[1].weights[0].numpy()[:, :, 0, 0]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ed21645",
      "metadata": {
        "id": "3ed21645"
      },
      "source": [
        "Podemos ver que los filtros tienen tamaño ```(5, 5, 3, 4)```. Los primeros dos números corresponden al tamaño del filtro, el tercero corresponde al número de canales de la imágen de entrada y el cuarto al número de filtros en la capa convolucional.\n",
        "- Veamos un ejemplo de las convoluciones iniciales (pesos aleatorios)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5af5c284",
      "metadata": {
        "id": "5af5c284"
      },
      "outputs": [],
      "source": [
        "res = model(tf.cast(tf.expand_dims(im, 0), tf.float32))\n",
        "print(f\"Dimensiones del resultado de la convolución: {res.shape}\\n\\n\\n\")\n",
        "\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(4):\n",
        "    plt.subplot(221+i)\n",
        "    plt.imshow(res[0,:,:,i], \"gray\")\n",
        "    plt.title(f\" Filtro {i+1}\")\n",
        "    plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e19ca697",
      "metadata": {
        "id": "e19ca697"
      },
      "source": [
        "# **3. Pooling**\n",
        "----\n",
        "\n",
        "Otro componente fundamental en las redes convolucionales es el **_pooling_** o submuestreo, el cual se utiliza para **reducir la resolución de una imagen** con el fin de evitar que la red aprenda patrones muy detallados (sobreajuste). Además, permite que la red aprenda patrones en distintas escalas y disminuya el número total de parámetros en un modelo.\n",
        "\n",
        "Generalmente, la operación de *pooling* se realiza utilizando una función que reduce el tamaño de la imagen original siguiendo una regla en específico, algunos ejemplos de esto son :\n",
        "\n",
        "* ```tf.keras.layers.AveragePooling2D```: Reduce la dimensión de una imagen al obtener el promedio en determinados vecindarios.\n",
        "* ```tf.keras.layers.MaxPooling2D```: Reduce la dimensión de una imagen al representar cada vecindario por el pixel de mayor intensidad.\n",
        "\n",
        "Un ejemplo de diferentes funciones de pooling se ilustra en la siguiente figura :\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1iYFdNoS1iqo_X2kpm9o6DsgqUC5vkiur\" width=\"60%\" /></center>\n",
        "\n",
        "Las funciones de Pooling reciben parámetros similares a las capas de convolución, que controlan factores como el tamaño de la ventana y el tamaño del paso. Esto a su vez, define el tamaño de la salida de la capa. Veamos en detalle:\n",
        "\n",
        "*   `pool_size`: es el tamaño del vecindario o de la ventana.\n",
        "*   `strides`: el tamaño de paso en cada dimensión que debe moverse la ventana.\n",
        "*   `padding`: define el manejo de los bordes de la entrada a la capa. Analogo a lo que sucede en las capas convolucionales, un padding válido o `\"valid\"` indica que no se hace relleno sobre los bordes del tensor, resultando en una reducción de tamaño en la salida, y  `\"same\"` indica que el relleno se ajusta para que la salida conserve las dimensines de la entrada.\n",
        "\n",
        "En el caso del ejemplo anterior, se utilizan vecindarios (`pool_size`) de tamaño `(2,2)`, se utilizan pasos (`strides`) de tamaño `(2,2)` y se utiliza un *padding* válido.\n",
        "\n",
        "- Otros tipos de pooling pueden ser consultados en la documentación y se encuentran en ```tf.keras.layers```.\n",
        "\n",
        "Ahora veamos un ejemplo de pooling en _Tensorflow_ :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2601c3b3",
      "metadata": {
        "id": "2601c3b3"
      },
      "outputs": [],
      "source": [
        "# Definimos una capa de entrada\n",
        "inp = tf.keras.layers.Input(shape=(100, 100, 3))\n",
        "# Definimos una capa de average pooling\n",
        "pool = tf.keras.layers.AveragePooling2D(pool_size=(5,5),\n",
        "                                        strides=(5,5),\n",
        "                                        padding=\"valid\")(inp)\n",
        "# Definimos un modelo\n",
        "model = tf.keras.models.Model(inputs=[inp], outputs=[pool])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En el ejemplo anterior, el tensor de entrada tiene tamaño `(100, 100, 3)`. Al apliacar un Pooling con `pool_size=(5,5)` y `strides=(5,5)` quiere decir que la ventana de tamaño `(5,5)` se mueve cada vez 5 posiciones en ambas direcciones. Por tanto, caben 20 filtros a lo alto y ancho del tensor de entrada, lo que arroja una salida de tamaño `(20,20,3)`.\n",
        "\n",
        "En el siguiente ejemplo, cambia el `pool_size` a `(10,10)`, pero se mantiene el `strides`, entonces ya no caben 20 filtros sino 19 a lo largo y ancho del tensor de entrada:"
      ],
      "metadata": {
        "id": "wFnkgBkFoVnM"
      },
      "id": "wFnkgBkFoVnM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867be3de",
      "metadata": {
        "id": "867be3de"
      },
      "outputs": [],
      "source": [
        "# Definimos una capa de entrada\n",
        "inp = tf.keras.layers.Input(shape=(100, 100, 3))\n",
        "# Definimos una capa de max pooling\n",
        "pool = tf.keras.layers.MaxPooling2D(pool_size=(10,10), strides=(5,5),\n",
        "                                    padding=\"valid\")(inp)\n",
        "# Definimos un modelo\n",
        "model2 = tf.keras.models.Model(inputs=[inp], outputs=[pool])\n",
        "model2.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30b44293",
      "metadata": {
        "id": "30b44293"
      },
      "outputs": [],
      "source": [
        "# Obtenemos el resultado del primer modelo\n",
        "res_avg = model(tf.cast(tf.expand_dims(im, 0), tf.float32))\n",
        "# Obtenemos el resultado del segundo modelo\n",
        "res_max = model2(tf.cast(tf.expand_dims(im, 0), tf.float32))\n",
        "\n",
        "plt.figure(figsize=(12,4))\n",
        "plt.subplot(131)\n",
        "plt.imshow(im); plt.title(\"Imagen Original\"); plt.axis(\"off\")\n",
        "plt.subplot(132)\n",
        "plt.imshow(tf.cast(res_avg[0], tf.uint8)); plt.title(\"Average Pooling\"); plt.axis(\"off\")\n",
        "plt.subplot(133)\n",
        "plt.imshow(tf.cast(res_max[0], tf.uint8)); plt.title(\"Maximum Pooling\"); plt.axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d5e574f4",
      "metadata": {
        "id": "d5e574f4"
      },
      "source": [
        "# **4. Redes neuronales convolucionales**\n",
        "----\n",
        "\n",
        "Una arquitectura de red neuronal convolucional se compone principalmente de **capas convoluciones y _poolings_** realizados de forma secuencial. Con esto, la red puede obtener varias representaciones intermedias (*feature maps*) hasta llegar a un nivel de abstracción suficiente para realizar una predicción apropiada (clasificación o regresión). Más específicamente, este tipo de arquitecturas incluyen un predictor al final (generalmente una red neuronal multicapa) que utiliza una representación más simple de una imagen (análogo al enfoque clásico donde se extraían características de una imagen para entrenar un clasificador).\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=12xLj0Tfc7Df-n0HIenyX_7QGPNv2q81r\" width=\"90%\" /></center>\n",
        "\n",
        "\n",
        "Veamos cómo construír una red neuronal convolucional en _TensorFlow_:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos un modelo en keras"
      ],
      "metadata": {
        "id": "qjR0D2B7FP0_"
      },
      "id": "qjR0D2B7FP0_"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "Qevqo2W6FP7g"
      },
      "id": "Qevqo2W6FP7g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Definimos una capa de entrada, especificamos que el tamaño de nuestras imágenes será de 150x150 y que tendrá tres canales."
      ],
      "metadata": {
        "id": "PV79wshcFSq4"
      },
      "id": "PV79wshcFSq4"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.add(tf.keras.layers.Input(shape=(150, 150, 3)))"
      ],
      "metadata": {
        "id": "iculNR0MFS-v"
      },
      "id": "iculNR0MFS-v",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos bloques de convolución seguidos de max pooling\n",
        "\n",
        "- Primer bloque"
      ],
      "metadata": {
        "id": "UlYIwxgxFX3o"
      },
      "id": "UlYIwxgxFX3o"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.add(tf.keras.layers.Conv2D(filters=36,\n",
        "                                    kernel_size=3,\n",
        "                                    activation='relu'))\n",
        "\n",
        "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))"
      ],
      "metadata": {
        "id": "s80KnyU1FYCu"
      },
      "id": "s80KnyU1FYCu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Segundo bloque"
      ],
      "metadata": {
        "id": "OXkPO9tVFf11"
      },
      "id": "OXkPO9tVFf11"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.add(tf.keras.layers.Conv2D(filters=36,\n",
        "                                    kernel_size=3,\n",
        "                                    activation='relu'))\n",
        "\n",
        "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))"
      ],
      "metadata": {
        "id": "OBhfy-vhFicF"
      },
      "id": "OBhfy-vhFicF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Tercer bloque"
      ],
      "metadata": {
        "id": "Sv8pEzOrFmOk"
      },
      "id": "Sv8pEzOrFmOk"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.add(tf.keras.layers.Conv2D(filters=36,\n",
        "                                    kernel_size=3,\n",
        "                                    activation='relu'))\n",
        "\n",
        "conv_net.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))"
      ],
      "metadata": {
        "id": "m_W4oDLqFmYk"
      },
      "id": "m_W4oDLqFmYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agregamos una capa `flatten`, que transforma cualquier arreglo multidimensional en un vector unidimensional."
      ],
      "metadata": {
        "id": "tXrkJRqaFren"
      },
      "id": "tXrkJRqaFren"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "MW4h1usYF0aA"
      },
      "id": "MW4h1usYF0aA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y finalmente agregamos un clasificador, en este caso una red neuronal multicapa:"
      ],
      "metadata": {
        "id": "pquA_HVxF34G"
      },
      "id": "pquA_HVxF34G"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fca1ad",
      "metadata": {
        "id": "f1fca1ad"
      },
      "outputs": [],
      "source": [
        "# Capa densa intermedia\n",
        "conv_net.add(tf.keras.layers.Dense(units=512,\n",
        "                                   activation='relu'))\n",
        "# Capa de salida\n",
        "conv_net.add(tf.keras.layers.Dense(units=1,\n",
        "                                   activation='sigmoid'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Visualicemos:"
      ],
      "metadata": {
        "id": "WmKJnjO3GAQR"
      },
      "id": "WmKJnjO3GAQR"
    },
    {
      "cell_type": "code",
      "source": [
        "conv_net.summary()\n",
        "tf.keras.utils.plot_model(conv_net,show_shapes=True)"
      ],
      "metadata": {
        "id": "N7SAkf6eGAb4"
      },
      "id": "N7SAkf6eGAb4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "40c2f983",
      "metadata": {
        "id": "40c2f983"
      },
      "source": [
        "# **5. Aumentación de datos (_Data Augmentation_)**\n",
        "----\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1mUGU94zRIuMMVhfhdsHwsAOtqOmI5IC1\" width=\"60%\" /></center>\n",
        "\n",
        "Una de las características de las redes neuronales convolucionales es que **necesitan una gran cantidad de datos para aprender**. No obstante, existen alternativas para hacer que este tipo de modelos aprendan en conjuntos de datos pequeños. La idea es extender el conjunto de entrenamiento por medio de la generación de datos o *data augmentation*, en el caso específico de imágenes, consiste en generar nuevas imágenes transformadas (cambios en traslación, rotación, intensidad, entre otros) a partir del conjunto de imágenes original.\n",
        "\n",
        "_Keras_ provee herramientas que nos permiten realizar *data augmentation* y que pueden ser utilizadas en conjunto con los modelos. Para ilustrar esto, utilizaremos un dataset con imágenes de perros y gatos. Usaremos el siguiente código para descargar el dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a6691bf",
      "metadata": {
        "id": "0a6691bf"
      },
      "outputs": [],
      "source": [
        "# limpiamos el dataset\n",
        "!rm -rf /tmp/cats_and_dogs_*\n",
        "# descargamos el dataset\n",
        "!wget --no-check-certificate \\\n",
        "     https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip \\\n",
        "     -O /tmp/cats_and_dogs_filtered.zip\n",
        "# descomprimimos\n",
        "!unzip /tmp/cats_and_dogs_filtered.zip -d /tmp/"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "15f58f9e",
      "metadata": {
        "id": "15f58f9e"
      },
      "source": [
        "Los contenidos del zip se extraen al directorio base ```/tmp/cats_and_dogs_filtered```, el cual contiene subdirectorios para los conjuntos de datos de entrenamiento ```/tmp/cats_and_dogs_filtered/train/```y validación ```/tmp/cats_and_dogs_filtered/validation/```. De igual forma, cada uno de estos subdirectorios contiene carpetas con imágenes por cada categoría, es decir, una para perros y una para gatos.\n",
        "\n",
        "Comencemos de definiendo las rutas del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8fbb6df4",
      "metadata": {
        "id": "8fbb6df4"
      },
      "outputs": [],
      "source": [
        "base_dir = '/tmp/cats_and_dogs_filtered'\n",
        "train_dir = os.path.join(base_dir, 'train')\n",
        "validation_dir = os.path.join(base_dir, 'validation')\n",
        "\n",
        "# Directorio con las imágenes de gatos para entrenamiento\n",
        "train_cats_dir = os.path.join(train_dir, 'cats')\n",
        "\n",
        "# Directorio con las imágenes de perros para entrenamiento\n",
        "train_dogs_dir = os.path.join(train_dir, 'dogs')\n",
        "\n",
        "# Directorio con las imágenes de gatos para validación\n",
        "validation_cats_dir = os.path.join(validation_dir, 'cats')\n",
        "\n",
        "# Directorio con las imágenes de perros para validación\n",
        "validation_dogs_dir = os.path.join(validation_dir, 'dogs')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45e1b923",
      "metadata": {
        "id": "45e1b923"
      },
      "source": [
        "Veamos un ejemplo de los nombres algunas imágenes de cada categoría:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a4fc8923",
      "metadata": {
        "id": "a4fc8923"
      },
      "outputs": [],
      "source": [
        "train_cat_fnames = os.listdir(train_cats_dir)\n",
        "print(train_cat_fnames[:10])\n",
        "\n",
        "train_dog_fnames = os.listdir(train_dogs_dir)\n",
        "train_dog_fnames.sort()\n",
        "print(train_dog_fnames[:10])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1664e578",
      "metadata": {
        "id": "1664e578"
      },
      "source": [
        "El dataset está constituído de la siguiente forma :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60e33b70",
      "metadata": {
        "id": "60e33b70"
      },
      "outputs": [],
      "source": [
        "print('Imágenes de gatos en entrenamiento:', len(os.listdir(train_cats_dir)))\n",
        "print('Imágenes de perros en entrenamiento:', len(os.listdir(train_dogs_dir)))\n",
        "print('Imágenes de gatos en validación:', len(os.listdir(validation_cats_dir)))\n",
        "print('Imágenes de perros en validación:', len(os.listdir(validation_dogs_dir)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "20bd532d",
      "metadata": {
        "id": "20bd532d"
      },
      "source": [
        "Ahora, veamos algunos ejemplos de las imágenes originales :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b37e5f42",
      "metadata": {
        "id": "b37e5f42"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(2, 8, figsize=(15,10))\n",
        "\n",
        "# seleccionamos las primeras 8 imágenes de cada categoría\n",
        "next_cat_pix = [os.path.join(train_cats_dir, fname)\n",
        "                for fname in train_cat_fnames[:8]]\n",
        "next_dog_pix = [os.path.join(train_dogs_dir, fname)\n",
        "                for fname in train_dog_fnames[:8]]\n",
        "\n",
        "# mostramos las imágenes\n",
        "for i, img_path in enumerate(next_cat_pix+next_dog_pix):\n",
        "    img = tf.keras.preprocessing.image.load_img(img_path)\n",
        "    ax[i//8,i%8].imshow(img)\n",
        "    ax[i//8,i%8].axis(\"off\")\n",
        "plt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bb33df9c",
      "metadata": {
        "id": "bb33df9c"
      },
      "source": [
        "Para realizar *data augmentation* utilizamos el objeto ```tf.keras.preprocessing.image.ImageDataGenerator```, el cual requiere los siguientes argumentos:\n",
        "\n",
        "* ```rescale```: Define una normalización de intensidad utilizando un factor dado.\n",
        "* ```rotation_range```: Define un rango (a nivel de ángulos) en el que se generarán rotaciones aleatorias de las imágenes.\n",
        "* ```width_shift_range```: Define un rango en el que se generarán traslaciones horizontales de las imágenes.\n",
        "* ```height_shift_range```: Define un rango en el que se generarán traslaciones verticales de las imágenes.\n",
        "* ```shear_range```: Define un rango en el que se generará el efecto de cizallamiento o *shear* .\n",
        "* ```zoom_range```: Define un rango en el que se generará el efecto aumento o *zoom*.\n",
        "* ```horizontal_flip```: Se especifica si las imágenes generadas se pueden invertir horizontalmente.\n",
        "* ```vertical_flip```: Se especifica si las imágenes generadas se pueden invertir verticalmente.\n",
        "* ```fill_mode```: en algunas transformaciones aparecen espacios que no estaban contenidos en la imagen original, este parámetro permite especificar si estos espacios se llenarán con el pixel más cercano ```\"nearest\"``` o con un valor constante ```\"constant\"```.\n",
        "\n",
        "Veamos un ejemplo de esto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d9f8a892",
      "metadata": {
        "id": "d9f8a892"
      },
      "outputs": [],
      "source": [
        "# Definimos las transformaciones para el conjunto de train\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
        "                                                                rotation_range=40,\n",
        "                                                                width_shift_range=0.2,\n",
        "                                                                height_shift_range=0.2,\n",
        "                                                                shear_range=0.2,\n",
        "                                                                zoom_range=0.2,\n",
        "                                                                horizontal_flip=True,\n",
        "                                                                fill_mode='constant')\n",
        "\n",
        "# Definimos las transformaciones para el conjunto de test\n",
        "val_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "32403786",
      "metadata": {
        "id": "32403786"
      },
      "source": [
        "Note que para el conjunto de validación no hacemos transformaciones más allá de `rescale`. Esto es porque el conjunto de validación lo usamos para medir el desempeño del modelo, y esto debe hacerse siempre con los datos inalterados, tal cual como se espera que lleguen en una aplicación real. De igual manera, no se hace augmentation en el conjunto de prueba final.\n",
        "\n",
        "Ahora, obtenemos generadores de _keras_ que permitirán el entrenamiento por batch. `ImageDataGenerator` tiene una función llamada `flow_from_directory` que se encarga de pasarle al modelo grupos de muestras o _batches_. `flow_from_directory` requiere los siguientes argumentos:\n",
        "\n",
        "*   `directory`: la ruta al directorio donde se encuentran los datos.\n",
        "*   `target_size`: el tamaño (`altura`,`ancho`) al que deben reformarce las imágenes para entrar al modelo.\n",
        "*   `batch_size`: el número de muestras por batch.\n",
        "*   `class_mode`: puede ser `categorical`, `binary`, `sparse`, `input`, o `None`. Determina el tipo de etiqueta que se usará en el modelo. `categorical` hará que las etiquetas tengan una codificación _one_hot_. `binary` o `sparse` hará que las etiquetas sean números enteros. `None` no alimentará al modelo con etiquetas.\n",
        "\n",
        "Con esto definimos entonces dos _generators_, uno para entrenamiento y otro para validación:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245fc0ad",
      "metadata": {
        "id": "245fc0ad"
      },
      "outputs": [],
      "source": [
        "# Especificamos el tamaño del batch, número de imagenes que genera en cada iteración\n",
        "batch_size = 128\n",
        "\n",
        "# Obtenemos un generador que realiza las transformaciones y carga\n",
        "# las imágenes de entrenamiento\n",
        "train_generator = train_datagen.flow_from_directory(directory=train_dir,\n",
        "                                                    target_size=(150, 150),\n",
        "                                                    batch_size=batch_size,\n",
        "                                                    class_mode='binary')\n",
        "\n",
        "# Obtenemos un generador que realiza las transformaciones y carga\n",
        "# las imágenes de validación\n",
        "validation_generator = val_datagen.flow_from_directory(directory=validation_dir,\n",
        "                                                       target_size=(150, 150),\n",
        "                                                       batch_size=batch_size,\n",
        "                                                       class_mode='binary')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f60ccb66",
      "metadata": {
        "id": "f60ccb66"
      },
      "source": [
        "Usando la función `next()`, que recibe como argumento un **generador**, veamos un ejemplo de las imágenes generadas :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "158644b8",
      "metadata": {
        "id": "158644b8"
      },
      "outputs": [],
      "source": [
        "# Extraemos un batch\n",
        "ims, lab = next(train_generator)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "# Mostramos 9 ejemplos\n",
        "for i in range(9):\n",
        "    plt.subplot(331+i)\n",
        "    plt.imshow(ims[i]); plt.axis(\"off\"); plt.title(f\"Label: {lab[i]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c1395c9",
      "metadata": {
        "id": "3c1395c9"
      },
      "source": [
        "# **6. Entrenamiento y visualización**\n",
        "----\n",
        "\n",
        "Para entrenar la red neuronal convolucional con los generadores que realizan el **_data augmentation_** utilizaremos la función usual ```fit```, que puede recibir un **generador** en lugar de arreglos multidimensionales explícitos. En este caso, al igual que con los datasets de _TensorFlow_, debemos especificar el número de batches en cada época.\n",
        "\n",
        "Primero, compilemos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "355fee17",
      "metadata": {
        "id": "355fee17"
      },
      "outputs": [],
      "source": [
        "conv_net.compile(loss=\"binary_crossentropy\",\n",
        "                 optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "                 metrics=[\"accuracy\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note que en el siguiente `fit()` definimos explícitamente el número de itaraciones que suceden en cada epoch: `steps_per_epoch`, y lo hacemos tanto para el conjunto de entrenmiento como para el de validación. En cualquier caso bebe calcularse simplemente por medio de la división entre el número de muestras y el tamaño del _batch_."
      ],
      "metadata": {
        "id": "AOU3RdiDdz_X"
      },
      "id": "AOU3RdiDdz_X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "356e872a",
      "metadata": {
        "id": "356e872a"
      },
      "outputs": [],
      "source": [
        "# Entrenamos el modelo\n",
        "history = conv_net.fit(train_generator,\n",
        "                                 steps_per_epoch=2000//batch_size,\n",
        "                                 epochs=10,\n",
        "                                 validation_data=validation_generator,\n",
        "                                 validation_steps=1000//batch_size,\n",
        "                                 verbose=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "101f6358",
      "metadata": {
        "id": "101f6358"
      },
      "source": [
        "Ahora veamos las pérdidas del modelo y la progresión del _accuracy_ a lo largo de las épocas :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "821b7243",
      "metadata": {
        "id": "821b7243"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(history.history[\"loss\"], label=\"entrenamiento\")\n",
        "plt.plot(history.history[\"val_loss\"], label=\"validación\")\n",
        "plt.title(\"Pérdida\"); plt.xlabel(\"Época\"); plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.plot(history.history[\"accuracy\"], label=\"entrenamiento\")\n",
        "plt.plot(history.history[\"val_accuracy\"], label=\"validación\")\n",
        "plt.title(\"Accuracy\"); plt.xlabel(\"Época\"); plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "42038926",
      "metadata": {
        "id": "42038926"
      },
      "source": [
        "Podemos observar las representaciones intermedias o *feature maps* que fueron aprendidos por la red.\n",
        "- Veamos un ejemplo de esto para  imágenes  del conjunto de entrenamiento:\n",
        "\n",
        "> Puete tomar al rededor de 3 minutos en ejecutar con GPU"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dummy_data = np.random.rand(1, 150, 150, 3)  # Example with shape (1, 150, 150, 3)\n",
        "conv_net.predict(dummy_data)"
      ],
      "metadata": {
        "id": "8WRSN48qgk4y"
      },
      "id": "8WRSN48qgk4y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da31222a",
      "metadata": {
        "id": "da31222a"
      },
      "outputs": [],
      "source": [
        "# Definimos un modelo que mostrará la salida de cada una de las capas de\n",
        "# la red convolucional original, excepto las últimas tres (densas).\n",
        "successive_outputs = [layer.output for layer in conv_net.layers[:-3]]\n",
        "visualization_model = tf.keras.models.Model(inputs=conv_net.layers[0].input, outputs=successive_outputs)\n",
        "\n",
        "# Seleccionamos al azar y cargamos la imagen de un perro\n",
        "dog_img_files = [os.path.join(train_dogs_dir, f) for f in train_dog_fnames]\n",
        "img_path = random.choice(dog_img_files)\n",
        "img = np.array(tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150)))\n",
        "img = np.expand_dims(img, axis=0)\n",
        "\n",
        "# Aplicamos el preprocesamiento que usamos en el entrenamiento\n",
        "img = img/255\n",
        "\n",
        "# Obtenemos los feature maps\n",
        "feature_maps = visualization_model.predict(img)\n",
        "# Extraemos los nombres de cada capa\n",
        "layer_names = [layer.name for layer in conv_net.layers[:-3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8a5e7aad",
      "metadata": {
        "id": "8a5e7aad"
      },
      "outputs": [],
      "source": [
        "# Mostramos la imagen original\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.imshow(img[0])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Imagen original\")\n",
        "\n",
        "# Mostramos los feature maps:\n",
        "for lab, fm in zip(layer_names, feature_maps):\n",
        "    fig, ax = plt.subplots(6, 6, figsize=(15, 15))\n",
        "    fig.suptitle(lab)\n",
        "    for filter_i in range(fm.shape[-1]):\n",
        "        ax[filter_i//6, filter_i%6].imshow(fm[0,:,:,filter_i])\n",
        "        ax[filter_i//6, filter_i%6].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5f157f1",
      "metadata": {
        "id": "b5f157f1"
      },
      "source": [
        "Ahora aplicamos la red completa para obtener la predicción :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4586e9d",
      "metadata": {
        "id": "b4586e9d"
      },
      "outputs": [],
      "source": [
        "prob = conv_net(img).numpy()[0,0]\n",
        "print(f\"Probabilidad de ser perro: {prob}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La única salida de la red neuronal se interpreta como la probabilidad de ser perro, pues el generador de datos asignó a la clase perro la clase positiva (1), como se puede observar en el atributo `class_indices` del generador `train_generator`:"
      ],
      "metadata": {
        "id": "IFemXF8b610_"
      },
      "id": "IFemXF8b610_"
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_generator.class_indices)"
      ],
      "metadata": {
        "id": "QvMvmC2r60cL"
      },
      "id": "QvMvmC2r60cL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "2d92f2f3",
      "metadata": {
        "id": "2d92f2f3"
      },
      "source": [
        "Realizamos el mismo proceso para una imagen de gato:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "82a5fb1e",
      "metadata": {
        "id": "82a5fb1e"
      },
      "outputs": [],
      "source": [
        "# Seleccionamos al azar y cargamos la imagen de un gato\n",
        "cat_img_files = [os.path.join(train_cats_dir, f) for f in train_cat_fnames]\n",
        "img_path = random.choice(cat_img_files)\n",
        "img = np.array(tf.keras.preprocessing.image.load_img(img_path, target_size=(150, 150)))\n",
        "img = np.expand_dims(img, axis=0)\n",
        "\n",
        "# Aplicamos el preprocesamiento que usamos en el entrenamiento\n",
        "img = img/255\n",
        "\n",
        "# Obtenemos los feature maps\n",
        "feature_maps = visualization_model.predict(img)\n",
        "# Extraemos los nombres de cada capa\n",
        "layer_names = [layer.name for layer in conv_net.layers[:-3]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45964c11",
      "metadata": {
        "id": "45964c11"
      },
      "outputs": [],
      "source": [
        "# Mostramos la imagen original\n",
        "plt.figure(figsize=(7,7))\n",
        "plt.imshow(img[0])\n",
        "plt.axis(\"off\")\n",
        "plt.title(\"Imagen original\")\n",
        "\n",
        "# Mostramos los feature maps:\n",
        "for lab, fm in zip(layer_names, feature_maps):\n",
        "    fig, ax = plt.subplots(6, 6, figsize=(15, 15))\n",
        "    fig.suptitle(lab)\n",
        "    for filter_i in range(fm.shape[-1]):\n",
        "        ax[filter_i//6, filter_i%6].imshow(fm[0,:,:,filter_i])\n",
        "        ax[filter_i//6, filter_i%6].axis(\"off\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dead0013",
      "metadata": {
        "id": "dead0013"
      },
      "source": [
        "Ahora aplicamos la red completa para obtener la predicción, en este caso usamos el complemento de la salida, pues la clase gato corresponde a la clase negativa (0):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "84f68d50",
      "metadata": {
        "id": "84f68d50"
      },
      "outputs": [],
      "source": [
        "prob = 1.0 - conv_net(img).numpy()[0,0]\n",
        "print(f\"Probabilidad de ser gato: {prob}\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5cc33588",
      "metadata": {
        "id": "5cc33588"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "----\n",
        "Los siguientes enlaces corresponden a sitios en donde encontrará información muy útil para profundizar las redes convolucionales :\n",
        "\n",
        "* https://machinelearningmastery.com/how-to-visualize-filters-and-feature-maps-in-convolutional-neural-networks/\n",
        "* https://en.wikipedia.org/wiki/Kernel_(image_processing)\n",
        "* https://www.tensorflow.org/tutorials/images/classification\n",
        "* https://medium.com/@Joocheol_Kim/take-a-look-inside-the-cnn-eb65d0dfdf94\n",
        "* _Origen de los íconos_\n",
        "    - Flaticon. Pixels free icon [PNG]. https://www.flaticon.com/free-icon/pixels_1881511?_gl\n",
        "    - Blog. ConvNet diagram [PNG]. https://4.bp.blogspot.com/-uHgA5CQk22A/VT3_7lLLXtI/AAAAAAAAOC0/YMAyuywwaQ0/s1600/convnet.png\n",
        "    - Wikipedia. Discrete 2D Convolution Animation [GIF]. https://en.m.wikipedia.org/wiki/Convolution#/media/File%3A2D_Convolution_Animation.gif\n",
        "    - Nadeem Qazi. The window incorporates the depth, but it only moves along two dimensions of the image [GIF]. https://miro.medium.com/max/738/1*Q7NXeOlDkm4xlNrNQOS67g.gif\n",
        "    - Bouvet. A Pooling example with a stride of 2 and a filter size of 2x2 [GIF]. https://www.bouvet.no/bouvet-deler/understanding-convolutional-neural-networks-part-1/_/attachment/inline/e60e56a6-8bcd-4b61-880d-7c621e2cb1d5:6595a68471ed37621734130ca2cb7997a1502a2b/Pooling.gif    \n",
        "    - Kaggle.  What is Image Data Augmentation [JPG]. https://www.kaggle.com/getting-started/190280\n",
        "\n",
        "\n",
        "   "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "  * [Juan Sebastián Lara](https://http://juselara.com/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](mailto:mrodrigueztr@unal.edu.co).\n",
        "\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "metadata": {
        "id": "jcPDlmpLZkzr"
      },
      "id": "jcPDlmpLZkzr"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}