{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo4/M5U4_Taller_4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6vF8WHtOmqKG",
      "metadata": {
        "id": "6vF8WHtOmqKG"
      },
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1VV2e_u46fNm_ewns8QW2HGRZAPHh-e2t\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca9e82b0",
      "metadata": {
        "id": "ca9e82b0"
      },
      "source": [
        "# **Taller 4 - *Transformers***\n",
        "---\n",
        "\n",
        "En este taller usted tendrá que usar un modelo de Transformer de [Hugginface](https://huggingface.co/) para hacer un proceso de *Fine Tunning*. El modelo final será un clasificador de sarcasmo. Usaremos un conjunto de datos que consiste en encabezados de periódicos (textos en inglés).\n",
        "\n",
        "Importemos las librerías requeridas para solucionar la tarea:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ugzb1JKXAZHe",
      "metadata": {
        "id": "Ugzb1JKXAZHe"
      },
      "source": [
        "> **Importante:  Recomendamos utilizar GPU para la ejecución de este notebook, ya que puede tomar mucho tiempo la ejeución de algunos casos de prueba en caso de que no se utilice.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gRusQKhlAd7j",
      "metadata": {
        "id": "gRusQKhlAd7j"
      },
      "outputs": [],
      "source": [
        "!pip install rlxcrypt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5z-Y0-O6AfG9",
      "metadata": {
        "id": "5z-Y0-O6AfG9"
      },
      "outputs": [],
      "source": [
        "!wget --no-cache -O session.pye -q https://raw.githubusercontent.com/JuezUN/INGInious/master/external%20libs/session.pye"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MNycOhRkAggS",
      "metadata": {
        "id": "MNycOhRkAggS"
      },
      "outputs": [],
      "source": [
        "import rlxcrypt\n",
        "import session\n",
        "\n",
        "grader = session.LoginSequence(\"DLIAAPCP-GroupMLDS-5-2024-2@4aab2a5b-740f-4fb9-94d4-163161c86a1e\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade transformers\n",
        "!pip install tf-keras"
      ],
      "metadata": {
        "id": "R5yiXBShPUG3"
      },
      "id": "R5yiXBShPUG3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1' # Establece la versión legacy"
      ],
      "metadata": {
        "id": "h0pZ-h_ZPbVG"
      },
      "id": "h0pZ-h_ZPbVG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_tbd6UioOken",
      "metadata": {
        "id": "_tbd6UioOken"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "import transformers\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OHuWA9eJrzVU",
      "metadata": {
        "id": "OHuWA9eJrzVU"
      },
      "outputs": [],
      "source": [
        "# Versiones de las librerías usadas.\n",
        "!python --version\n",
        "print('Numpy', np.__version__)\n",
        "print('Transformers', transformers.__version__)\n",
        "print('Tensorflow', tf.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aPNlud8Or71E",
      "metadata": {
        "id": "aPNlud8Or71E"
      },
      "source": [
        "Esta actividad se realizó con las siguientes versiones:\n",
        "*  Python 3.9.16\n",
        "*  Numpy 1.22.4\n",
        "*  Transformers 4.28.1\n",
        "*  Tensorflow 2.12.0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3aed3ce8",
      "metadata": {
        "id": "3aed3ce8"
      },
      "source": [
        "## **Cargar Datos**\n",
        "---\n",
        "\n",
        "El conjunto de datos esta compuesto por titulares de periódicos. Los datos fueron preparados por _Laurence Moroney_ (código fuente disponible en [GitHub](https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%203%20-%20NLP/Course%203%20-%20Week%203%20-%20Lesson%202c.ipynb))."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qlFk1VOKprO6",
      "metadata": {
        "id": "qlFk1VOKprO6"
      },
      "outputs": [],
      "source": [
        "!wget -O sarcasm.json https://raw.githubusercontent.com/mindlab-unal/mlds5-dataset-unit4-Tarea4-Transformers/main/sarcasm.json?raw=true\n",
        "with open(\"sarcasm.json\", 'r') as f:\n",
        "    datastore = json.load(f)\n",
        "sentences = []\n",
        "labels = []\n",
        "for item in datastore:\n",
        "    sentences.append(item['headline'])\n",
        "    labels.append(item['is_sarcastic'])\n",
        "print(len(sentences))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sAPABEWoqLHp",
      "metadata": {
        "id": "sAPABEWoqLHp"
      },
      "source": [
        "Tenemos 26709 muestras. Démosle un vistazo a los datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDwPKeVvqNyE",
      "metadata": {
        "id": "XDwPKeVvqNyE"
      },
      "outputs": [],
      "source": [
        "print('label','\\t','sentence','\\n','----\\t----------')\n",
        "for i in np.random.choice(26709, 10, replace=False):\n",
        "  print(labels[i],'\\t',sentences[i],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nMqdRuiQtXf_",
      "metadata": {
        "id": "nMqdRuiQtXf_"
      },
      "source": [
        "La etiqueta `0` corresponde a _no sarcasmo_ y la `1` a _sarcasmo_. Es decir, _sarcasmo_ es la clase positiva."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04054323",
      "metadata": {
        "id": "04054323"
      },
      "source": [
        "## **Particiones de entrenamiento y prueba**\n",
        "---\n",
        "Como siempre, es importante separar una parte de los datos que no serán usados en el entrenamiento para evaluar el desempeño del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06264f5e",
      "metadata": {
        "id": "06264f5e"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_temp, X_test, y_temp, y_test = train_test_split(sentences, labels, test_size=0.2, stratify = labels, random_state = 30)\n",
        "X_train, X_val,  y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.25, stratify = y_temp, random_state = 30)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FwL41WOci0_k",
      "metadata": {
        "id": "FwL41WOci0_k"
      },
      "source": [
        "Hacemos una codificación _one hot_ de las etiquetas de entrenamiento y validación:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "InJpDpJSBaKe",
      "metadata": {
        "id": "InJpDpJSBaKe"
      },
      "outputs": [],
      "source": [
        "y_train = tf.keras.utils.to_categorical(y_train)\n",
        "y_val = tf.keras.utils.to_categorical(y_val)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5SDtEX4Ki-Om",
      "metadata": {
        "id": "5SDtEX4Ki-Om"
      },
      "source": [
        "Y contamos cuántos textos hay en cada partición:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVu3tqJIT0XB",
      "metadata": {
        "id": "cVu3tqJIT0XB"
      },
      "outputs": [],
      "source": [
        "len(X_train), len(X_val), len(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gi_p-YIjxG8-",
      "metadata": {
        "id": "Gi_p-YIjxG8-"
      },
      "source": [
        "Tenemos entonces 16025 muestras para entrenamiento, 5342 para validación y la misma cantidad para prueba."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OV-aLBdExviM",
      "metadata": {
        "id": "OV-aLBdExviM"
      },
      "source": [
        "# **DistilBERT**\n",
        "----\n",
        "[DistilBERT](https://huggingface.co/docs/transformers/model_doc/distilbert) es un modelo Transformer pequeño, rápido, poco costoso y ligero, entrenado a partir BERT, con una técnica llamada [_Destilación de conocimiento_](https://en.wikipedia.org/wiki/Knowledge_distillation).\n",
        "\n",
        "La destilación de conocimiento es el proceso de transferencia de conocimiento de un modelo grande a otro más pequeño. Aunque los modelos grandes (como BERT) tienen mayor capacidad de aprendizaje que los modelos pequeños, es posible que esta capacidad no se utilice plenamente. Evaluar un modelo puede ser igual de costoso desde el punto de vista computacional aunque utilice muy poco de su capacidad de conocimiento. La destilación de conocimiento transfiere conocimientos de un modelo grande a otro más pequeño sin pérdida de validez. Como los modelos más pequeños son menos costosos de evaluar, se pueden implantar en hardware menos potente (como un dispositivo móvil, o en este caso, un entorno de Colab)\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=15uxvIrnqiJJmkqhet_eLGud7CKlQ-145\" alt =\"DistilBERT\" width=\"80%\" /></center>\n",
        "\n",
        "DistilBERT tiene un 40% menos de parámetros que el modelo BERT, se ejecuta un 60% más rápido y conserva más del 95% del rendimiento de BERT. Originalmente se propuso en el artículo: [_DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter_](https://arxiv.org/abs/1910.01108). Para nuestra fortuna, el modelo se encuentra implementado y disponible en HuggingFace: https://huggingface.co/docs/transformers/model_doc/distilbert.\n",
        "\n",
        "Vamos entonces a aplicar lo aprendido en la unidad. Recordemos cuál es el pipeline estandar de ejecución de un Transformer:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1yaxcm5ceXkGuMZ5u6iQaZ0sGyYXGU3Pu\" alt =\"Gráfico ilustrativo de la pipeline de ejecución\" width=\"80%\" /></center>\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "TTzBI8m2ki_2",
      "metadata": {
        "id": "TTzBI8m2ki_2"
      },
      "source": [
        "> **La tarea es incremental, por lo tanto es recomendable resolver los puntos en orden.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "119d73dc",
      "metadata": {
        "id": "119d73dc"
      },
      "source": [
        "# **1. Tokenización**\n",
        "---\n",
        "Aplique el preprocesamiento requerido para usar una arquitectura DistilBERT sobre los conjuntos definidos de entrenamiento, validación y prueba. Complete la función `tokenize`, que recibe como argumentos un conjnuto de datos y los parámetros necesarios para tokenizarlos. La función debe retornar un objeto tipo `BatchEncoding` como lo vimos en el taller guiado. Utilice `AutoTokenizer` y la tokenización del modelo `\"distilbert-base-uncased\"`.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "* **`model_name`**: un `str` que representa el nombre del tokenizer del modelo pre-entrenado a definir.\n",
        "* **`X`**, `list`, una lista de secuencuas de texto.\n",
        "* **`truncate`**, `boolean`, variable booleana para definir si los textos se truncan o no.  \n",
        "* **`padd`**, `boolean`, variable booleana para definir si los textos se rellenan o no.\n",
        "* **`tensor`**, un `str` que puede ser `np`, `tf` o `pt` para indicar el tipo de tensor que debe devolver el tokenizador.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "* **`encodings`**: un objeto tipo `BatchEncoding` para tokenizar textos para `DistilBERT`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Z3DHSL8_QT-T",
      "metadata": {
        "id": "Z3DHSL8_QT-T"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA tokenize\n",
        "from transformers import AutoTokenizer\n",
        "def tokenize(model_name, X, truncate, padd, tensor):\n",
        "    encodings = -1\n",
        "    return encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zb-y2Ngz1LdS",
      "metadata": {
        "id": "zb-y2Ngz1LdS"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "train_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                           X = X_train,\n",
        "                           truncate=True,\n",
        "                           padd=True,\n",
        "                           tensor= \"np\")\n",
        "val_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                         X = X_val,\n",
        "                         truncate=True,\n",
        "                         padd=True,\n",
        "                         tensor= \"np\")\n",
        "test_encodings = tokenize(model_name='distilbert-base-uncased',\n",
        "                          X = X_test,\n",
        "                          truncate=True,\n",
        "                          padd=True,\n",
        "                          tensor= \"np\")\n",
        "\n",
        "print(\"Encoding tipo:\", type(train_encodings))\n",
        "print(\"Primeros cinco tokens de test:\", test_encodings['input_ids'][0][:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "W3n6wvGxRjaz",
      "metadata": {
        "id": "W3n6wvGxRjaz"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "Encoding tipo: <class 'transformers.tokenization_utils_base.BatchEncoding'>\n",
        "Primeros cinco tokens de test: [ 101 5712 2022 4974 2075]\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zyKTa9oJEaHy",
      "metadata": {
        "id": "zyKTa9oJEaHy"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Uvm6y5zHQzZC",
      "metadata": {
        "id": "Uvm6y5zHQzZC"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 1_1_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9_fvBC0dW8m8",
      "metadata": {
        "id": "9_fvBC0dW8m8"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 1_1_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "89599031",
      "metadata": {
        "id": "89599031"
      },
      "source": [
        "# **2. _Fine Tuning_ con _DistilBERT_**\n",
        "---\n",
        "Vamos a llevar a cabo el flujo de trabajo neesario para hacer _Fine Tuning_ con un modelo preentrenado."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VSDnmpD_t-gv",
      "metadata": {
        "id": "VSDnmpD_t-gv"
      },
      "source": [
        "## **2.1 Definir el modelo**\n",
        "---\n",
        "Primero vamos a definir el modelo. Implemente una función llamada `pretrained_model` que reciba como entrada un nombre de modelo y retorne una instancia de ese modelo pre-entrenado utilizando la biblioteca Transformers de HuggingFace. La función debe utilizar `TFAutoModelForSequenceClassification` para cargar el modelo y devolverlo.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "* **`model_name`**: un `str` que representa el nombre del modelo pre-entrenado a cargar.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "* **`model`**: una instancia del modelo pre-entrenado cargado utilizando `TFAutoModelForSequenceClassification`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4EYskRy4ADjt",
      "metadata": {
        "id": "4EYskRy4ADjt"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA pretrained_model\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "def pretrained_model(model_name):\n",
        "    model = -1\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QBRQVMIdBvJi",
      "metadata": {
        "id": "QBRQVMIdBvJi"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "model = pretrained_model('distilbert-base-uncased')\n",
        "print(type(model))\n",
        "print(model.summary())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "610N2ALPBkqZ",
      "metadata": {
        "id": "610N2ALPBkqZ"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "<class 'transformers.models.distilbert.modeling_tf_distilbert.TFDistilBertForSequenceClassification'>\n",
        "Model: \"tf_distil_bert_for_sequence_classification\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_19 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 66,955,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Otg-YvCucLdY",
      "metadata": {
        "id": "Otg-YvCucLdY"
      },
      "source": [
        "### **Nota importante**\n",
        "---\n",
        "Veamos detalladamente la arquitectura del modelo anterior. Tenemos 66,955,010 de parámetros repartidos en cuatro bloques. El primer bloque es el modelo base `distilbert`. Los parámetros de ese bloque (66,362,880) son los que se deben congelar al momento de hacer el calentamiento o _warming up_."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2xh-TZB8D0kf",
      "metadata": {
        "id": "2xh-TZB8D0kf"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zq5F7gD8pOaG",
      "metadata": {
        "id": "zq5F7gD8pOaG"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_1_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "psjh0i7FrXQB",
      "metadata": {
        "id": "psjh0i7FrXQB"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_1_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AY2YRFzhUE8a",
      "metadata": {
        "id": "AY2YRFzhUE8a"
      },
      "source": [
        "## **2.2 Compilar**\n",
        "---\n",
        "\n",
        "Igual que con BERT, DistilBERT se define por defecto con dos neuronas de salida con activación linea. Complete entonces la función `compile` que prepara el modelo para el entrenamiento. La función debe recibir como entrada un modelo, un optimizador y una tasa de aprendizaje, y luego compila el modelo con una función de pérdida `CategoricalCrossentropy`.\n",
        "\n",
        "**Entradas:**\n",
        "\n",
        "*    **`model`**: una instancia del modelo a entrenar tipo `TFAutoModelForSequenceClassification`.\n",
        "*    **`optimizer`**: una instancia del optimizador de tipo `keras.optimizers` a utilizar durante el entrenamiento.\n",
        "*    **`l_r`**: `float`, un número flotante que representa la tasa de aprendizaje.\n",
        "\n",
        "**Salida:**\n",
        "\n",
        "* **`model`**: una instancia compilada del modelo tipo `TFAutoModelForSequenceClassification`.\n",
        "\n",
        "> **Notas:**\n",
        "  * Utilice el método `.assign()` para ajustar la tasa de aprendizaje del optimizador al valor proporcionado.\n",
        "  * Recuerde definir `from_logits=True` dentro de la función de pérdida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SLtRWuN5A1N9",
      "metadata": {
        "id": "SLtRWuN5A1N9"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA compile_model\n",
        "\n",
        "def compile_model(model, optimizer, l_r):\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cEv0ylZTBiDs",
      "metadata": {
        "id": "cEv0ylZTBiDs"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "model = compile_model(model=model,\n",
        "                   optimizer=tf.keras.optimizers.Adam(),\n",
        "                   l_r=1e-3\n",
        "                   )\n",
        "model.get_compile_config()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "je6S-JHMBnGB",
      "metadata": {
        "id": "je6S-JHMBnGB"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "{'optimizer': {'module': 'keras.optimizers',\n",
        "  'class_name': 'Adam',\n",
        "  'config': {'name': 'Adam',\n",
        "   'weight_decay': None,\n",
        "   'clipnorm': None,\n",
        "   'global_clipnorm': None,\n",
        "   'clipvalue': None,\n",
        "   'use_ema': False,\n",
        "   'ema_momentum': 0.99,\n",
        "   'ema_overwrite_frequency': None,\n",
        "   'jit_compile': True,\n",
        "   'is_legacy_optimizer': False,\n",
        "   'learning_rate': 0.0010000000474974513,\n",
        "   'beta_1': 0.9,\n",
        "   'beta_2': 0.999,\n",
        "   'epsilon': 1e-07,\n",
        "   'amsgrad': False},\n",
        "  'registered_name': None},\n",
        " 'loss': {'module': 'keras.losses',\n",
        "  'class_name': 'CategoricalCrossentropy',\n",
        "  'config': {'reduction': 'auto',\n",
        "   'name': 'categorical_crossentropy',\n",
        "   'from_logits': True,\n",
        "   'label_smoothing': 0.0,\n",
        "   'axis': -1},\n",
        "  'registered_name': None},\n",
        " 'metrics': None,\n",
        " 'loss_weights': None,\n",
        " 'weighted_metrics': None,\n",
        " 'run_eagerly': None,\n",
        " 'steps_per_execution': None,\n",
        " 'jit_compile': None}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vMjJCQgPD8c4",
      "metadata": {
        "id": "vMjJCQgPD8c4"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lTHOp-tvgjMl",
      "metadata": {
        "id": "lTHOp-tvgjMl"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_2_1\", globals())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_0yWvCVZhcuD",
      "metadata": {
        "id": "_0yWvCVZhcuD"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_2_2\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f78f7424",
      "metadata": {
        "id": "f78f7424"
      },
      "source": [
        "## **2.3 Función de entrenamiento**\n",
        "---\n",
        "Complete la función llamada `train_model` que entrena un modelo utilizando un conjunto de datos de entrenamiento. La función debe recibir como un modelo, los datos de entrenamiento, el número de épocas y el tamaño del _batch, y devolver el modelo entrenado.\n",
        "\n",
        "**Entradas**:\n",
        "\n",
        "*    **`model`**: una instancia del modelo a entrenar tipo `TFAutoModelForSequenceClassification`\n",
        "*    **`X_train`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_train`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "*    **`X_val`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_val`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de validación.\n",
        "*    **`epochs`**: `int`, un número entero que representa el número de épocas de entrenamiento.\n",
        "*    **`batch_size`**: `int`, un número entero que representa el tamaño del _batch_.\n",
        "*    **`train_base`**: `boolean`, una variable booleana para definir si se congelan o no las capas del modelo base (dependiendo si se quiere hacer _warming up_ o _fine tuning_).\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "*  **`history`**: un objeto tipo `History` de tensorflow con la información del entrenamiento del modelo.\n",
        "*    **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "\n",
        "Para implementar la función, utilice un ciclo `for` para controlar el atributo `trainable` de las capas del modelo, excepto la capa de salida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYG9FJiVJSpP",
      "metadata": {
        "id": "jYG9FJiVJSpP"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA train_model\n",
        "def train_model(model,\n",
        "                    X_train, y_train,\n",
        "                    X_val, y_val,\n",
        "                    epochs,\n",
        "                    batch_size,\n",
        "                    train_base):\n",
        "    history = -1\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "heBQq44nBxHe",
      "metadata": {
        "id": "heBQq44nBxHe"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "history, model = train_model(model=model,\n",
        "                             X_train=train_encodings,\n",
        "                             y_train=y_train,\n",
        "                             X_val=val_encodings,\n",
        "                             y_val=y_val,\n",
        "                             epochs=1,\n",
        "                             batch_size=32,\n",
        "                             train_base = False)\n",
        "print(model.summary())\n",
        "print(history.history.keys())\n",
        "print('El modelo se ha entrenado durante',len(history.history['val_loss']),'epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2biuU8PBo7j",
      "metadata": {
        "id": "Y2biuU8PBo7j"
      },
      "source": [
        "**Salida esperada:**\n",
        "```\n",
        "Model: \"tf_distil_bert_for_sequence_classification\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_19 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 592,130\n",
        "Non-trainable params: 66,362,880\n",
        "_________________________________________________________________\n",
        "None\n",
        "dict_keys(['loss', 'val_loss'])\n",
        "El modelo se ha entrenado durante 1 epoch\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AJKn_ykLD-mI",
      "metadata": {
        "id": "AJKn_ykLD-mI"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hILzmKwlFgqa",
      "metadata": {
        "id": "hILzmKwlFgqa"
      },
      "source": [
        "> Tiempo estimado: 2:10:00 h sin GPU, 02:35 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ph6cYHHWtZeE",
      "metadata": {
        "id": "ph6cYHHWtZeE"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_3_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bc02068",
      "metadata": {
        "id": "7bc02068"
      },
      "source": [
        "## **2.4 _Fine Tuning_**\n",
        "---\n",
        "Ahora vamos a usar las funciones que definió anteriormente para hacer todo el proceso de ajuste fino, comenzando por hacer un calentamiento o _warming up_ entranando solo la capa final del modelo, y luego entrenando el modelo completo.\n",
        "\n",
        "Complete la función `fine_tuning`. Esta función recible los datos de entrenamiento y valdación, la tasa de aprendizaje del _warming up_ y la tasa de aprendizaje del _fine tuning_, así como las epochs dedicadas a cada etapa del entrenamiento. La función debe crear un modelo, compilarlo para _warming up_, realizar el _warming up_, y luego compilar el modelo de nuevo para _fine tuning_ (liberando los pesos de todas las capas), y entrenar de nuevo.\n",
        "\n",
        "**Entrada**:\n",
        "\n",
        "*    **`X_train`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_train`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "*    **`X_val`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_val`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de validación.\n",
        "*    **`l_r_warming_up`**: `float`, la tasa de aprendizaje a usar durante el calentamiento.\n",
        "*    **`epochs_warming_up`**: `int`, un número entero que representa el número de épocas de entrenamiento usadas en el calentamiento.\n",
        "*    **`l_r_fine_tuning`**: `float`, la tasa de aprendizaje a usar durante el _fine tuning_.\n",
        "*    **`epochs_fine_tuning`**: `int`, un número entero que representa el número de épocas de entrenamiento usadas en el _fine tuning_.\n",
        "\n",
        "**Salida**:\n",
        "\n",
        "*  **`history`**: un objeto tipo `History` de tensorflow con la información del entrenamiento del modelo.\n",
        "*    **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "\n",
        "> **Notas**:\n",
        "  * Debe definir el modelo usando la función `pretrained_model`, cargando `'distilbert-base-uncased'`.\n",
        "  * Debe compilar el modelo dos veces usando la función `compile_model`. La primera vez será para configurar el modelo para el _warming up_. La seguda vez que compile será después de haber hecho el _warming up_, para habilitar el entrenamiento de todas las capas y cambiar la tasa de aprendizaje.\n",
        "  * Analogamente, debe usar dos veces la función `train_model`, una vez para _warming up_, y la segunda vez para _fine tuning_.\n",
        "  * `tf.keras.optimizers.Adam()` debe ser el optimizador de todos los entrenamientos.\n",
        "  * La función debe usar un `batch_size` de 32 en todos los entrenamientos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yDo3-MSLfmFg",
      "metadata": {
        "id": "yDo3-MSLfmFg"
      },
      "outputs": [],
      "source": [
        "# FUNCION CALIFICADA fine_tuning\n",
        "def fine_tuning(X_train, y_train,\n",
        "                    X_val, y_val,\n",
        "                    l_r_warming_up,\n",
        "                    epochs_warming_up,\n",
        "                    l_r_fine_tuning,\n",
        "                    epochs_fine_tuning):\n",
        "\n",
        "    history, model = -1, -1\n",
        "\n",
        "    return history, model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hYmFXsvQBx9F",
      "metadata": {
        "id": "hYmFXsvQBx9F"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "history, model = fine_tuning(X_train=train_encodings,\n",
        "                              y_train=np.asarray(y_train),\n",
        "                              X_val=val_encodings,\n",
        "                              y_val=np.asarray(y_val),\n",
        "                              l_r_warming_up=1e-3,\n",
        "                              epochs_warming_up=1,\n",
        "                              l_r_fine_tuning=5e-5,\n",
        "                              epochs_fine_tuning=1)\n",
        "print(model.summary())\n",
        "print('learnign_rate =',model.get_compile_config()['optimizer']['config']['learning_rate'])\n",
        "print(history.history.keys())\n",
        "print('El modelo se ha entrenado durante',len(history.history['val_loss']),'epoch')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NcfVMuPHBpu0",
      "metadata": {
        "id": "NcfVMuPHBpu0"
      },
      "source": [
        "**Salida esperada:**\n",
        "> **Nota**: los valores de `loss` y `val_loss` pueden cambiar.\n",
        "\n",
        "```\n",
        "501/501 [==============================] - 147s 226ms/step - loss: 0.6918 - val_loss: 0.6861\n",
        "501/501 [==============================] - 54s 95ms/step - loss: 0.6859 - val_loss: 0.6859\n",
        "Model: \"tf_distil_bert_for_sequence_classification_1\"\n",
        "_________________________________________________________________\n",
        " Layer (type)                Output Shape              Param #   \n",
        "=================================================================\n",
        " distilbert (TFDistilBertMai  multiple                 66362880  \n",
        " nLayer)                                                         \n",
        "                                                                 \n",
        " pre_classifier (Dense)      multiple                  590592    \n",
        "                                                                 \n",
        " classifier (Dense)          multiple                  1538      \n",
        "                                                                 \n",
        " dropout_39 (Dropout)        multiple                  0         \n",
        "                                                                 \n",
        "=================================================================\n",
        "Total params: 66,955,010\n",
        "Trainable params: 66,955,010\n",
        "Non-trainable params: 0\n",
        "_________________________________________________________________\n",
        "None\n",
        "learnign_rate = 4.999999873689376e-05\n",
        "dict_keys(['loss', 'val_loss'])\n",
        "El modelo se ha entrenado durante 1 epoch\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QVoYfKroEAJn",
      "metadata": {
        "id": "QVoYfKroEAJn"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "iId0fMP0jbAG",
      "metadata": {
        "id": "iId0fMP0jbAG"
      },
      "outputs": [],
      "source": [
        "## TEACHEAR\n",
        "history.history"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4KDwN-ZFx4ik",
      "metadata": {
        "id": "4KDwN-ZFx4ik"
      },
      "source": [
        "> Tiempo estimado: 4:10:00 h sin GPU, 04:00 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7vltS8gWp3c5",
      "metadata": {
        "id": "7vltS8gWp3c5"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_4_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aVP3vDm6vRKn",
      "metadata": {
        "id": "aVP3vDm6vRKn"
      },
      "source": [
        "## **2.5 Evaluar**\n",
        "---\n",
        "Complete la función `evaluate_model` que recible un modelo entrenado y lo evalua en un conjunto de prueba.\n",
        "\n",
        "**Entrada**:\n",
        "\n",
        "*  **`model`**: una instancia del modelo entrenado tipo `TFAutoModelForSequenceClassification`\n",
        "*    **`X_test`**: un objeto tipo `BatchEncoding` para tokenizar textos.\n",
        "*    **`y_test`**: un arreglo de numpy `np.array` que representa las etiquetas de los datos de entrenamiento.\n",
        "\n",
        "**Salida**:\n",
        "* **`report`**: un `str` dado por la función `classification_report` de Scikit-Learn.\n",
        "\n",
        "> Nota: recuerde aplicar `tf.math.softmax` a las predicciones del modelo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nv6ktYTTZ_Jl",
      "metadata": {
        "id": "nv6ktYTTZ_Jl"
      },
      "outputs": [],
      "source": [
        "# FUNCIÓN CALIFICADA evaluate\n",
        "from sklearn.metrics import *\n",
        "def evaluate(model, X_test, y_test):\n",
        "    report = -1\n",
        "    return report"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "L7I3JxY8BzSg",
      "metadata": {
        "id": "L7I3JxY8BzSg"
      },
      "outputs": [],
      "source": [
        "# TEST_CELL\n",
        "print(evaluate(model=model,\n",
        "               X_test=test_encodings,\n",
        "               y_test=y_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1fk0ryES4j62",
      "metadata": {
        "id": "1fk0ryES4j62"
      },
      "source": [
        "**Salida esperada**:\n",
        "\n",
        "```\n",
        "167/167 [==============================] - 13s 65ms/step\n",
        "              precision    recall  f1-score   support\n",
        "\n",
        "           0       0.56      1.00      0.72      2997\n",
        "           1       0.00      0.00      0.00      2345\n",
        "\n",
        "    accuracy                           0.56      5342\n",
        "   macro avg       0.28      0.50      0.36      5342\n",
        "weighted avg       0.31      0.56      0.40      5342\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lyryG_KlEB6Y",
      "metadata": {
        "id": "lyryG_KlEB6Y"
      },
      "source": [
        "### **Evaluar código**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6TFGQe_jPRGk",
      "metadata": {
        "id": "6TFGQe_jPRGk"
      },
      "source": [
        "> Tiempo estimado: 1:00 m sin GPU, 00:29 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ezHjXU3mINRx",
      "metadata": {
        "id": "ezHjXU3mINRx"
      },
      "outputs": [],
      "source": [
        "grader.run_test(\"Test 2_5_1\", globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4yNMUWeUqwMF",
      "metadata": {
        "id": "4yNMUWeUqwMF"
      },
      "source": [
        "## **Entrenando durante más _epochs_**\n",
        "----\n",
        "En este punto usted ha debido programar con éxito todos los pasos necesairios para hacer _Fine Tuning_ con _DistilBERT_ en usando _HuggingFace_, pero no hemos entrenado lo suficiente el modelo. A continuación use su código para el entrenar el modelo durante algunas epochs de calentamiento y otras más de _fine tuning_, cambiando en cada caso las tasas de aprendizaje y evaluando los resultados.\n",
        "\n",
        "> **Nota: este código no es calificable.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yahBNV4RqxIF",
      "metadata": {
        "id": "yahBNV4RqxIF"
      },
      "outputs": [],
      "source": [
        "history, model = fine_tuning(X_train=train_encodings,\n",
        "                              y_train=np.asarray(y_train),\n",
        "                              X_val=val_encodings,\n",
        "                              y_val=np.asarray(y_val),\n",
        "                              l_r_warming_up=5e-3,\n",
        "                              epochs_warming_up=5,\n",
        "                              l_r_fine_tuning=5e-5,\n",
        "                              epochs_fine_tuning=20)\n",
        "print(evaluate(model=model,\n",
        "               X_test=test_encodings,\n",
        "               y_test=np.array(y_test)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MKw7uZ33EMIS",
      "metadata": {
        "id": "MKw7uZ33EMIS"
      },
      "source": [
        "# **Evaluación**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ViRJ8e5vPbbT",
      "metadata": {
        "id": "ViRJ8e5vPbbT"
      },
      "source": [
        "> Tiempo estimado: 06:00:00 h sin GPU, 10:30 m con GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jgeaNWRhEOSc",
      "metadata": {
        "id": "jgeaNWRhEOSc"
      },
      "outputs": [],
      "source": [
        "grader.submit_task(globals())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_eRlyPGbJtsJ",
      "metadata": {
        "id": "_eRlyPGbJtsJ"
      },
      "source": [
        "# **Referencias**\n",
        "---\n",
        "* [*Fine-tuning with custom datasets*](https://huggingface.co/transformers/v3.2.0/custom_datasets.html)\n",
        "\n",
        "* [*Transformer Models For Custom Text Classification Through Fine-Tuning*](https://towardsdatascience.com/transformer-models-for-custom-text-classification-through-fine-tuning-3b065cc08da1)\n",
        "\n",
        "* [*Fine-Tuning Hugging Face Model with Custom Dataset*](https://towardsdatascience.com/fine-tuning-hugging-face-model-with-custom-dataset-82b8092f5333)\n",
        "\n",
        "* [*Fine-Tuning Bert for Tweets Classification ft. Hugging Face*](https://medium.com/mlearning-ai/fine-tuning-bert-for-tweets-classification-ft-hugging-face-8afebadd5dbf)\n",
        "\n",
        "* [*Coronavirus tweets NLP - Text Classification*](https://www.kaggle.com/datasets/datatattle/covid-19-nlp-text-classification?resource=download&select=Corona_NLP_test.csv)\n",
        "\n",
        "* [*covid_tweet_classification.ipynb*](https://colab.research.google.com/github/codistro/Articles/blob/main/covid_tweet_classification.ipynb#scrollTo=bmeDURiI7Q6M)\n",
        "\n",
        "\n",
        "* _Origen de imágenes_\n",
        "\n",
        "  - Swatimeena. (2021, 16 diciembre). DistilBERT Text classification using Keras - Swatimeena - Medium. Medium. [Imagen] https://miro.medium.com/v2/resize:fit:720/format:webp/1*e5G4Vdt6gSFkRjs3fD36Pg.png\n",
        "\n",
        "  - Davaadorj,M. (2022, 19 marzo).\n",
        "Huggingface-course: Documentation-images - Full Nlp Pipeline [Imagen] https://huggingface.co/datasets/huggingface-course/documentation-images/blob/main/en/chapter2/full_nlp_pipeline.svg\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "APRHBmlSxKGo",
      "metadata": {
        "id": "APRHBmlSxKGo"
      },
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](mailto:mrodrigueztr@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "jupytext": {
      "main_language": "python"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}