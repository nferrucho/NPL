{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/Copia_de_2_intro_spacy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "21d236b4",
      "metadata": {
        "id": "21d236b4"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1AQr9H9bXDeNPchTRufU78g8z0yxHvrmC\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "94699a4a",
      "metadata": {
        "id": "94699a4a"
      },
      "source": [
        "# Introducci√≥n a Spacy\n",
        "---\n",
        "\n",
        "En este taller guiado presentaremos una introducci√≥n pr√°ctica a la librer√≠a `spacy` para procesamiento de lenguaje natural. Comenzamos import√°ndola:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "724943d4",
      "metadata": {
        "id": "724943d4"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b817a40",
      "metadata": {
        "id": "5b817a40"
      },
      "source": [
        "## **1. ¬øQu√© es Spacy?**\n",
        "---\n",
        "\n",
        "Se trata de una librer√≠a para procesamiento de lenguaje natural (NLP) que provee una forma de uso simple y flexible para implementar soluciones de NLP de forma inmediata en aplicaciones industriales.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1_hGDcMnNp0MAS7ERGPcmnGTX-xAs1iiG\" width=\"40%\">\n",
        "\n",
        "`spacy` tiene las siguientes caracter√≠sticas:\n",
        "\n",
        "* Soporta m√°s de 66 lenguajes.\n",
        "* Trae m√°s de 80 _pipelines_ pre-entrenados para m√°s de 24 lenguajes.\n",
        "* Permite usar modelos del estado del arte en NLP.\n",
        "* Embeddings pre-entrenados.\n",
        "* Velocidad competitiva con el estado del arte.\n",
        "* Sistema de entrenamiento de modelos.\n",
        "* Herramientas de tokenizado ling√º√≠stico.\n",
        "* Componentes de distintas tareas generales de NLP como: reconocimiento de entidades nombradas, part-of-speech, segmentaci√≥n de textos, entre otros.\n",
        "* Componentes personalizados.\n",
        "* Soporta modelos personalizados de diversas librer√≠as de machine learning.\n",
        "* Herramientas de visualizaci√≥n para texto.\n",
        "* F√°cil empaquetado de modelos para el despliegue y la gesti√≥n.\n",
        "* Modelos con un buen desempe√±o base y con una rigurosa evaluaci√≥n."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71d760d1",
      "metadata": {
        "id": "71d760d1"
      },
      "source": [
        "## **2. Modelos de Spacy**\n",
        "---\n",
        "\n",
        "`spacy` tiene distintos modelos pre-entrenados y listos para usar en distintos lenguajes. Puedes revisar los distintos lenguajes y modelos, como se muestra a continuaci√≥n, a partir de [este enlace](https://spacy.io/models):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0462281f",
      "metadata": {
        "id": "0462281f",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@markdown ##**Ejecute esta celda para ver el video.**\n",
        "from IPython.display import IFrame\n",
        "IFrame(\n",
        "        src=\"https://drive.google.com/file/d/1ncW1HPrkU7aHw2eUhBl-zeoaukypmIZ0/preview\",\n",
        "        width=\"768px\",\n",
        "        height=\"432px\"\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16154a1f",
      "metadata": {
        "id": "16154a1f"
      },
      "source": [
        "Los modelos pre-entrenados generalmente tienen un c√≥digo de nombramiento como `{lang}_{from}_{corpus}_{size}`:\n",
        "\n",
        "* `lang`: c√≥digo del idioma de los modelos a cargar.\n",
        "* `from`: repositorio de d√≥nde vienen los modelos.\n",
        "* `corpus`: conjunto de datos donde fueron entrenados los modelos.\n",
        "* `size`: tama√±o del modelo, generalmente se maneja `sm` (small - peque√±o), `md` (medium - mediano), `lg` (large - grande).\n",
        "\n",
        "Los modelos los podemos descargar autom√°ticamente usando la funci√≥n `download` dentro del `cli` de `spacy` como mostramos a continuaci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "183ca35b",
      "metadata": {
        "id": "183ca35b"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa92ffee",
      "metadata": {
        "id": "fa92ffee"
      },
      "source": [
        "## **_3. Pipeline_**\n",
        "---\n",
        "\n",
        "Los _pipelines_ de `spacy` nos proveen una interfaz por medio de la cual podemos aplicar distintas t√©cnicas y modelos de NLP de forma ordenada y secuencial. Un _pipeline_ representa una serie de pasos (componentes) que se aplican uno tras otro para obtener informaci√≥n espec√≠fica de un texto. Por lo general siguen la siguiente estructura:\n",
        "\n",
        "<center>\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=12MczaMlyMo12kIbk5FZC0PzA1h5E6fQq\" width=\"60%\">\n",
        "<font size=\"1\" color=\"black\"><i>CNN/CPU pipeline design [Imagen]. Extra√≠da de https://spacy.io/models </i></font>\n",
        "</center>\n",
        "\n",
        "\n",
        "Cada uno de los componentes del _pipeline_ ser√° explicado en detalle posteriormente en este mismo taller guiado. Por ahora, podemos centrarnos en el proceso, el cual consiste en los siguientes pasos:\n",
        "\n",
        "* Los componentes `tagger`, `morphologizer` y `parser` esperan la salida del componente `tok2vec`. Si el `lemmatizer` es entrenable, este tambi√©n espera la salida de `tok2vec`.\n",
        "* El componente `attribute_ruler` utiliza los resultados del `tagger` y valida que los espacios y caracteres especiales est√©n etiquetados correctamente.\n",
        "* El `lemmatizer` utiliza las reglas obtenidas del `attribute_ruler` para transformar los textos.\n",
        "* El componente `ner` es independiente del resto de etapas y puede tener su propio componente de `tok2vec`.\n",
        "\n",
        "Veamos c√≥mo crear un _pipeline_ de `spacy` con la funci√≥n `load` y el _pipeline_ pre-entrenado que descargamos anteriormente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6773e5b",
      "metadata": {
        "id": "a6773e5b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "print(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f4660a9",
      "metadata": {
        "id": "7f4660a9"
      },
      "source": [
        "Como puede se puede ver, cargamos un objeto de tipo `Spanish`, lo que indica que `spacy` por detr√°s define clases personalizadas para los distintos lenguajes que queramos manejar.\n",
        "\n",
        "Podemos obtener el listado de los componentes que tiene el _pipeline_ que carg√≥ usando el atributo `component_names`, como se muestra a continuaci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c893106",
      "metadata": {
        "id": "0c893106"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8028faf3",
      "metadata": {
        "id": "8028faf3"
      },
      "source": [
        "Generalmente, un _pipeline_ de `spacy` aplica todos los componentes que tiene definidos sobre un documento espec√≠fico, no obstante, en algunas aplicaciones √∫nicamente llegamos a necesitar 1 o 2 componentes. Podemos deshabilitarlos usando el par√°metro `exclude` de la funci√≥n `load`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be8df55b",
      "metadata": {
        "id": "be8df55b"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"es_core_news_sm\", exclude=[\"ner\"])\n",
        "print(nlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3cb1e79",
      "metadata": {
        "id": "d3cb1e79"
      },
      "source": [
        "En este caso, cargamos un _pipeline_ con el componente `ner` deshabilitado, veamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6aaada7d",
      "metadata": {
        "id": "6aaada7d"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "66c889a7",
      "metadata": {
        "id": "66c889a7"
      },
      "source": [
        "## **4. Objetos de Spacy**\n",
        "---\n",
        "\n",
        "En `spacy` generalmente estamos trabajando sobre tres tipos de objetos: los documentos `Doc`, las palabras `Token` y las secuencias `Span`.\n",
        "\n",
        "Primero, vamos a definir un corpus con 3 documentos de prueba:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e986c2c",
      "metadata": {
        "id": "7e986c2c"
      },
      "outputs": [],
      "source": [
        "texts = [\n",
        "        \"Alan Mathison Turing fue un matem√°tico, l√≥gico, inform√°tico te√≥rico, cript√≥grafo, fil√≥sofo y bi√≥logo te√≥rico brit√°nico\",\n",
        "        \"Marvin Lee Minsky fue un cient√≠fico estadounidense. Es considerado uno de los padres de la inteligencia artificial. Fue cofundador del laboratorio de inteligencia artificial del Instituto de Tecnolog√≠a de Massachusetts (MIT).\",\n",
        "        \"Geoffrey Hinton es un inform√°tico brit√°nico. Hinton fue galardonado con el Premio Turing en 2018 junto con Yoshua Bengio y Yann LeCun por su trabajo en deep learning.\"\n",
        "        ]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2f9ffb6d",
      "metadata": {
        "id": "2f9ffb6d"
      },
      "source": [
        "Ahora, podemos convertir cada documento a un `Doc` de `spacy` por medio del m√©todo `pipe` del *pipeline* como se muestra a continuaci√≥n. El par√°metro `n_process` nos permite ejecutar los componentes de forma paralelizada:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "884cbca6",
      "metadata": {
        "id": "884cbca6"
      },
      "outputs": [],
      "source": [
        "corpus = list(nlp.pipe(texts, n_process=4))\n",
        "print(corpus)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b1f5e0f",
      "metadata": {
        "id": "4b1f5e0f"
      },
      "source": [
        "A simple vista, pareciera que los documentos no han cambiado mucho, no obstante, podemos validar que el tipo de un elemento dentro de `corpus` efectivamente es de tipo `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae8790f",
      "metadata": {
        "id": "3ae8790f"
      },
      "outputs": [],
      "source": [
        "doc = corpus[0]\n",
        "print(type(doc))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45151f6e",
      "metadata": {
        "id": "45151f6e"
      },
      "source": [
        "Podemos extraer el texto asociado a un `Doc` con el atributo `text`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1d189fa",
      "metadata": {
        "id": "e1d189fa"
      },
      "outputs": [],
      "source": [
        "print(doc.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaf8da3e",
      "metadata": {
        "id": "eaf8da3e"
      },
      "source": [
        "El tipo `Doc` tiene distintos atributos que posteriormente usaremos para acceder a los resultados de los distintos componentes del modelo. Entre ellos, podemos obtener tokens a nivel de palabra con una simple indexaci√≥n sobre el documento. Es decir, si queremos extraer la primera palabra del documento podemos hacer lo siguiente:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04bde886",
      "metadata": {
        "id": "04bde886"
      },
      "outputs": [],
      "source": [
        "token = doc[0]\n",
        "print(token)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19d4481d",
      "metadata": {
        "id": "19d4481d"
      },
      "source": [
        "De nuevo pareciera que fuera un string, pero en realidad es un objeto de tipo `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60cb72cb",
      "metadata": {
        "id": "60cb72cb"
      },
      "outputs": [],
      "source": [
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7274b851",
      "metadata": {
        "id": "7274b851"
      },
      "source": [
        "El objeto `Token` tiene distintos atributos asociados a cada componente como mostraremos posteriormente.\n",
        "\n",
        "Por √∫ltimo, podemos extraer las oraciones del documento por medio del atributo `sents` para ver el tipo de datos que nos entrega:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36a3f830",
      "metadata": {
        "id": "36a3f830"
      },
      "outputs": [],
      "source": [
        "sents = list(doc.sents)\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99ad88f3",
      "metadata": {
        "id": "99ad88f3"
      },
      "source": [
        "Al igual que en los casos anteriores los resultados no son de tipo string sino que son de tipo `Span` (secuencia):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8897aa80",
      "metadata": {
        "id": "8897aa80"
      },
      "outputs": [],
      "source": [
        "sent = sents[0]\n",
        "print(type(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9ece158f",
      "metadata": {
        "id": "9ece158f"
      },
      "source": [
        "El objeto `Span` tambi√©n tiene distintos atributos asociados a cada componente como lo veremos m√°s adelante. Cada elemento de un `Span` es un `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ea1bc8e1",
      "metadata": {
        "id": "ea1bc8e1"
      },
      "outputs": [],
      "source": [
        "token = sent[0]\n",
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70e07a52",
      "metadata": {
        "id": "70e07a52"
      },
      "source": [
        "## **5. Componentes**\n",
        "---\n",
        "\n",
        "Para ver el detalle de qu√© hace cada componente vamos a cargar el _pipeline_ `en_core_web_sm` en ingl√©s, ya que, es la forma m√°s estandarizada y general para entender los componentes y sus resultados.\n",
        "\n",
        "En este caso no usamos el _pipeline_ en espa√±ol ya que este no posee implementados todos los componentes que el _pipeline_ en ingl√©s s√≠. Por ejemplo, no contiene etiquetas _POS_ de grano fino.\n",
        "\n",
        "El ejemplo en espa√±ol lo retomaremos m√°s adelante, una vez veamos todos los componentes que puede llegar a tener un _pipeline_ de `spacy`.\n",
        "\n",
        "Veamos c√≥mo descargar el _pipeline_ `en_core_web_sm` para an√°lisis del ingl√©s:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb55581c",
      "metadata": {
        "id": "eb55581c"
      },
      "outputs": [],
      "source": [
        "spacy.cli.download(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b696c4ab",
      "metadata": {
        "id": "b696c4ab"
      },
      "source": [
        "Tambi√©n lo cargamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1acdb172",
      "metadata": {
        "id": "1acdb172"
      },
      "outputs": [],
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3043cd43",
      "metadata": {
        "id": "3043cd43"
      },
      "source": [
        "Veamos los componentes que trae este _pipeline_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "02d5663a",
      "metadata": {
        "id": "02d5663a"
      },
      "outputs": [],
      "source": [
        "print(nlp.component_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d70a91f",
      "metadata": {
        "id": "4d70a91f"
      },
      "source": [
        "Estos son los componentes t√≠picos que trae un _pipeline_ de `spacy`. En algunos lenguajes puede cambiar `tagger` por `morphologizer` si no hay muchos modelos disponibles para etiquetado de palabras en un idioma en espec√≠fico (`tagger` es m√°s completo que `morphologizer`).\n",
        "\n",
        "Vamos a ver el detalle de los componentes m√°s importantes de `spacy` con el siguiente texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "48ea083c",
      "metadata": {
        "id": "48ea083c"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"\n",
        "Alan Mathison Turing was an English mathematician, computer scientist, logician, cryptanalyst, philosopher, and theoretical biologist. Turing was highly influential in the development of theoretical computer science, providing a formalisation of the concepts of algorithm and computation with the Turing machine, which can be considered a model of a general-purpose computer. He is widely considered to be the father of theoretical computer science and artificial intelligence.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3980c3cc",
      "metadata": {
        "id": "3980c3cc"
      },
      "source": [
        "Ejecutamos el _pipeline_ para obtener un objeto de tipo `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a439512",
      "metadata": {
        "id": "1a439512"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text.strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c89ca946",
      "metadata": {
        "id": "c89ca946"
      },
      "source": [
        "### **5.1. Tokenizer**\n",
        "---\n",
        "`spacy` trae un componente llamado `Tokenizer` que est√° definido por defecto en un paquete de lenguaje. Se trata de un componente que aplica reglas del lenguaje para separar palabras, detectar signos de puntuaci√≥n y encontrar atributos generales de los objetos de tipo `Token`.\n",
        "\n",
        "El `Tokenizer` nos permite obtener el listado de tokens (incluye signos de puntuaci√≥n como tokens separados) por medio de algunas reglas espec√≠ficas de cada lenguaje.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=114bsjIq5vjLLLQYLwcUh14jObuUzbydy\" width=\"60%\">\n",
        "\n",
        "\n",
        "Veamos c√≥mo podemos obtener una lista de todas las palabras del documento con `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "96749016",
      "metadata": {
        "id": "96749016"
      },
      "outputs": [],
      "source": [
        "tokens = [token.text for token in doc]\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "755f95fe",
      "metadata": {
        "id": "755f95fe"
      },
      "source": [
        "Adicionalmente, desde el `Tokenizer` se calculan atributos en los tokens como:\n",
        "\n",
        "* `is_stop`: si la palabra es un stopword, es decir, una palabra muy frecuente y poco informativa del lenguaje en cuesti√≥n (e.g., is, the, to, entre otras).\n",
        "* `is_punct`: valida si el token es un signo de puntuaci√≥n.\n",
        "* `is_title`: valida si el token est√° en formato de t√≠tulo.\n",
        "* `is_upper`: valida si el token est√° en may√∫sculas.\n",
        "* `is_lower`: valida si el token est√° en min√∫sculas.\n",
        "* `lower_`: versi√≥n en min√∫sculas del token."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "176671a0",
      "metadata": {
        "id": "176671a0"
      },
      "source": [
        "Por ejemplo, podemos validar qu√© tokens son _stopwords_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40dfd2d8",
      "metadata": {
        "id": "40dfd2d8"
      },
      "outputs": [],
      "source": [
        "stops = [token.is_stop for token in doc]\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba790f6b",
      "metadata": {
        "id": "ba790f6b"
      },
      "source": [
        "Tambi√©n podemos filtrar el listado de palabras que son _stopwords_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1389b654",
      "metadata": {
        "id": "1389b654"
      },
      "outputs": [],
      "source": [
        "stops = list(filter(lambda token: token.is_stop, doc))\n",
        "print(stops)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c7aa0dce",
      "metadata": {
        "id": "c7aa0dce"
      },
      "source": [
        "### **5.2. Senter**\n",
        "---\n",
        "\n",
        "El componente `senter` hace uso de la clase `Sentencizer` la cual permite segmentar el texto en distintas oraciones usando un modelo basado en reglas (expresiones regulares), el resultado queda almacenado en dos niveles:\n",
        "\n",
        "* Como el atributo `is_sent_start` de cada `Token`.\n",
        "* Como el atributo `sents` dentro del documento `Doc`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1OtK3KZuKAeAwbAQpcmzGCz-CQxVfb192\" width=\"80%\">\n",
        "\n",
        "Veamos qu√© tokens son inicio de oraci√≥n dentro del documento que estamos manejando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6113c4b6",
      "metadata": {
        "id": "6113c4b6"
      },
      "outputs": [],
      "source": [
        "sent_starts = list(filter(lambda token: token.is_sent_start, doc))\n",
        "print(sent_starts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6c5a206",
      "metadata": {
        "id": "a6c5a206"
      },
      "source": [
        "Tambi√©n podemos extraer un listado de las oraciones del documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa4f5a48",
      "metadata": {
        "id": "aa4f5a48"
      },
      "outputs": [],
      "source": [
        "sents = list(doc.sents)\n",
        "print(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "54123d02",
      "metadata": {
        "id": "54123d02"
      },
      "source": [
        "En total tenemos 3 oraciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59226c6a",
      "metadata": {
        "id": "59226c6a"
      },
      "outputs": [],
      "source": [
        "print(len(sents))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0d843b0d",
      "metadata": {
        "id": "0d843b0d"
      },
      "source": [
        "### **5.3. Tok2vec**\n",
        "---\n",
        "\n",
        "El componente `tok2vec` hace uso de la clase `Tok2Vec` para implementar distintos modelos de **embedding**, de los cuales hablaremos en detalle en la **Unidad 3**.\n",
        "\n",
        "De forma resumida, este tipo de modelos permite extraer caracter√≠sticas num√©ricas a partir de palabras, oraciones o documentos y representarlas por medio de un vector. El vector se almacena como un atributo `vector` dentro de `Doc`, `Span` y `Token`. Esto es √∫til para capturar relaciones sem√°nticas (relacionadas con el significado) en las palabras, oraciones o documentos. Como por ejemplo en el caso de la relaci√≥n existente entre las palabras `Mujer` y `Hombre`, con respecto a las palabras `Reina` y `Rey`.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1w4z22p_X4KQ-sim0kZBnjkAZOaiojUCv\" width=\"80%\">\n",
        "\n",
        "Veamos un ejemplo de c√≥mo extraer caracter√≠sticas de todo un documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4001798d",
      "metadata": {
        "id": "4001798d"
      },
      "outputs": [],
      "source": [
        "vect = doc.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0666992d",
      "metadata": {
        "id": "0666992d"
      },
      "source": [
        "`spacy` extrae un vector de caracter√≠sticas como un arreglo de `numpy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fda7d04a",
      "metadata": {
        "id": "fda7d04a"
      },
      "outputs": [],
      "source": [
        "print(type(vect))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c6c2076",
      "metadata": {
        "id": "1c6c2076"
      },
      "source": [
        "Tambi√©n podemos extraer caracter√≠sticas de una oraci√≥n:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eec9cf7b",
      "metadata": {
        "id": "eec9cf7b"
      },
      "outputs": [],
      "source": [
        "sent = list(doc.sents)[0]\n",
        "print(type(sent))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "556b8079",
      "metadata": {
        "id": "556b8079"
      },
      "source": [
        "Veamos el vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "762539ca",
      "metadata": {
        "id": "762539ca"
      },
      "outputs": [],
      "source": [
        "vect = sent.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c473e3c",
      "metadata": {
        "id": "9c473e3c"
      },
      "source": [
        "El proceso es similar para un `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0932cadd",
      "metadata": {
        "id": "0932cadd"
      },
      "outputs": [],
      "source": [
        "token = sent[0]\n",
        "print(type(token))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef49526b",
      "metadata": {
        "id": "ef49526b"
      },
      "source": [
        "Veamos el vector:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd03a6d1",
      "metadata": {
        "id": "fd03a6d1"
      },
      "outputs": [],
      "source": [
        "vect = token.vector\n",
        "print(vect)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de2e3d66",
      "metadata": {
        "id": "de2e3d66"
      },
      "source": [
        "En estos casos el vector tiene 96 caracter√≠sticas num√©ricas para representar el `Token`, `Sent` o el `Doc`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "459355ff",
      "metadata": {
        "id": "459355ff"
      },
      "outputs": [],
      "source": [
        "print(vect.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d7da91e",
      "metadata": {
        "id": "1d7da91e"
      },
      "source": [
        "### **5.4. Tagger**\n",
        "---\n",
        "\n",
        "El componente `tagger` permite extraer etiquetas de tipo part-of-speech (POS) a cada `Token`. El proceso consiste en asignarle a cada palabra de una oraci√≥n una etiqueta de acuerdo con su funci√≥n gramatical dentro de la oraci√≥n. Con POS podemos determinar si un token es un sustantivo, una preposici√≥n, un adjetivo, un adverbio, un verbo, complemento directo o indirecto, entre otros.\n",
        "\n",
        "`spacy` maneja dos etiquetas POS dentro del texto:\n",
        "\n",
        "* Para ver la etiqueta POS gruesa, use `token.pos_`.\n",
        "* Para ver la etiqueta de grano fino, use `token.tag_`.\n",
        "\n",
        "Para ver la descripci√≥n de cualquier tipo de etiqueta, use `spacy.explain(tag)`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee63c30",
      "metadata": {
        "id": "bee63c30"
      },
      "source": [
        "#### **5.4.1. Etiquetas POS de Grano Grueso**\n",
        "---\n",
        "\n",
        "A cada token se le asigna una etiqueta POS de la siguiente lista si usamos el _pipeline_ estandarizado en ingl√©s:\n",
        "\n",
        "<table><tr><th>POS</th><th>Descripci√≥n</th><th>Ejemplos</th></tr>\n",
        "    \n",
        "<tr><td>ADJ</td><td>adjective</td><td>big, old, green, incomprehensible, first</td></tr>\n",
        "<tr><td>ADP</td><td>adposition</td><td>in, to, during</td></tr>\n",
        "<tr><td>ADV</td><td>adverb</td><td>very, tomorrow, down, where, there</td></tr>\n",
        "<tr><td>AUX</td><td>auxiliary</td><td>is, has (done), will (do), should (do)</td></tr>\n",
        "<tr><td>CONJ</td><td>conjunction</td><td>and, or, but</td></tr>\n",
        "<tr><td>CCONJ</td><td>coordinating conjunction</td><td>and, or, but</td></tr>\n",
        "<tr><td>DET</td><td>determiner</td><td>a, an, the</td></tr>\n",
        "<tr><td>INTJ</td><td>interjection</td><td>psst, ouch, bravo, hello</td></tr>\n",
        "<tr><td>NOUN</td><td>noun</td><td>girl, cat, tree, air, beauty</td></tr>\n",
        "<tr><td>NUM</td><td>numeral</td><td>1, 2017, one, seventy-seven, IV, MMXIV</td></tr>\n",
        "<tr><td>PART</td><td>particle</td><td>'s, not,</td></tr>\n",
        "<tr><td>PRON</td><td>pronoun</td><td>I, you, he, she, myself, themselves, somebody</td></tr>\n",
        "<tr><td>PROPN</td><td>proper noun</td><td>Mary, John, London, NATO, HBO</td></tr>\n",
        "<tr><td>PUNCT</td><td>punctuation</td><td>., (, ), ?</td></tr>\n",
        "<tr><td>SCONJ</td><td>subordinating conjunction</td><td>if, while, that</td></tr>\n",
        "<tr><td>SYM</td><td>symbol</td><td>$, %, ¬ß, ¬©, +, ‚àí, √ó, √∑, =, :), üòù</td></tr>\n",
        "<tr><td>VERB</td><td>verb</td><td>run, runs, running, eat, ate, eating</td></tr>\n",
        "<tr><td>X</td><td>other</td><td>sfpksdpsxmsa</td></tr>\n",
        "<tr><td>SPACE</td><td>space</td></tr>\n",
        "</table>\n",
        "\n",
        "Veamos c√≥mo extraer las etiquetas de grano grueso:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afcc9231",
      "metadata": {
        "id": "afcc9231"
      },
      "outputs": [],
      "source": [
        "pos_tags = [token.pos_ for token in doc]\n",
        "print(pos_tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9aec377",
      "metadata": {
        "id": "f9aec377"
      },
      "source": [
        "Veamos una descripci√≥n m√°s detallada de cada etiqueta usando el m√©todo `explain` y mostrando los datos como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e336f8b",
      "metadata": {
        "id": "3e336f8b"
      },
      "outputs": [],
      "source": [
        "explained_tags = [spacy.explain(tag) for tag in pos_tags]\n",
        "tokens = [token.text for token in doc]\n",
        "pd.DataFrame({\"token\": tokens, \"pos\": pos_tags, \"descripci√≥n\": explained_tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9d34127",
      "metadata": {
        "id": "a9d34127"
      },
      "source": [
        "#### **5.4.1. Etiquetas POS de Grano Fino**\n",
        "---\n",
        "\n",
        "Las etiquetas de grano fino dan m√°s detalles del tipo de palabra que estamos manejando, se pueden ver como subcategor√≠as sobre las etiquetas de grano grueso (mayor detalle). Con el _pipeline_ general en ingl√©s de `spacy` tenemos las siguientes etiquetas:\n",
        "\n",
        "<table>\n",
        "<tr><th>Grano Grueso</th><th>Grano Fino</th><th>Descripci√≥n</th><th>Ejemplo</th></tr>\n",
        "<tr><td>ADJ</td><td>AFX</td><td>affix</td><td>The Flintstones were a **pre**-historic family.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJ</td><td>adjective</td><td>This is a **good** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJR</td><td>adjective, comparative</td><td>This is a **better** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>JJS</td><td>adjective, superlative</td><td>This is the **best** sentence.</td></tr>\n",
        "<tr><td>ADJ</td><td>PDT</td><td>predeterminer</td><td>Waking up is **half** the battle.</td></tr>\n",
        "<tr><td>ADJ</td><td>PRP\\$</td><td>pronoun, possessive</td><td>**His** arm hurts.</td></tr>\n",
        "<tr><td>ADJ</td><td>WDT</td><td>wh-determiner</td><td>It's blue, **which** is odd.</td></tr>\n",
        "<tr><td>ADJ</td><td>WP\\$</td><td>wh-pronoun, possessive</td><td>We don't know **whose** it is.</td></tr>\n",
        "<tr><td>ADP</td><td>IN</td><td>conjunction, subordinating or preposition</td><td>It arrived **in** a box.</td></tr>\n",
        "<tr><td>ADV</td><td>EX</td><td>existential there</td><td>**There** is cake.</td></tr>\n",
        "<tr><td>ADV</td><td>RB</td><td>adverb</td><td>He ran **quickly**.</td></tr>\n",
        "<tr><td>ADV</td><td>RBR</td><td>adverb, comparative</td><td>He ran **quicker**.</td></tr>\n",
        "<tr><td>ADV</td><td>RBS</td><td>adverb, superlative</td><td>He ran **fastest**.</td></tr>\n",
        "<tr><td>ADV</td><td>WRB</td><td>wh-adverb</td><td>**When** was that?</td></tr>\n",
        "<tr><td>CONJ</td><td>CC</td><td>conjunction, coordinating</td><td>The balloon popped **and** everyone jumped.</td></tr>\n",
        "<tr><td>DET</td><td>DT</td><td>determiner</td><td>**This** is **a** sentence.</td></tr>\n",
        "<tr><td>INTJ</td><td>UH</td><td>interjection</td><td>**Um**, I don't know.</td></tr>\n",
        "<tr><td>NOUN</td><td>NN</td><td>noun, singular or mass</td><td>This is a **sentence**.</td></tr>\n",
        "<tr><td>NOUN</td><td>NNS</td><td>noun, plural</td><td>These are **words**.</td></tr>\n",
        "<tr><td>NOUN</td><td>WP</td><td>wh-pronoun, personal</td><td>**Who** was that?</td></tr>\n",
        "<tr><td>NUM</td><td>CD</td><td>cardinal number</td><td>I want **three** things.</td></tr>\n",
        "<tr><td>PART</td><td>POS</td><td>possessive ending</td><td>Fred**'s** name is short.</td></tr>\n",
        "<tr><td>PART</td><td>RP</td><td>adverb, particle</td><td>Put it **back**!</td></tr>\n",
        "<tr><td>PART</td><td>TO</td><td>infinitival to</td><td>I want **to** go.</td></tr>\n",
        "<tr><td>PRON</td><td>PRP</td><td>pronoun, personal</td><td>**I** want **you** to go.</td></tr>\n",
        "<tr><td>PROPN</td><td>NNP</td><td>noun, proper singular</td><td>**Kilroy** was here.</td></tr>\n",
        "<tr><td>PROPN</td><td>NNPS</td><td>noun, proper plural</td><td>The **Flintstones** were a pre-historic family.</td></tr>\n",
        "<tr><td>VERB</td><td>MD</td><td>verb, modal auxiliary</td><td>This **could** work.</td></tr>\n",
        "<tr><td>VERB</td><td>VB</td><td>verb, base form</td><td>I want to **go**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBD</td><td>verb, past tense</td><td>This **was** a sentence.</td></tr>\n",
        "<tr><td>VERB</td><td>VBG</td><td>verb, gerund or present participle</td><td>I am **going**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBN</td><td>verb, past participle</td><td>The treasure was **lost**.</td></tr>\n",
        "<tr><td>VERB</td><td>VBP</td><td>verb, non-3rd person singular present</td><td>I **want** to go.</td></tr>\n",
        "<tr><td>VERB</td><td>VBZ</td><td>verb, 3rd person singular present</td><td>He **wants** to go.</td></tr>\n",
        "</table>\n",
        "\n",
        "Veamos c√≥mo calcular las etiquetas de grano fino usando el atributo `tag_` del `Token`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bd9718e",
      "metadata": {
        "id": "0bd9718e"
      },
      "outputs": [],
      "source": [
        "tags = [token.tag_ for token in doc]\n",
        "print(tags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53fa1dbf",
      "metadata": {
        "id": "53fa1dbf"
      },
      "source": [
        "Veamos una descripci√≥n m√°s detallada de cada etiqueta usando el m√©todo `explain` y mostrando los datos como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5666f004",
      "metadata": {
        "id": "5666f004"
      },
      "outputs": [],
      "source": [
        "explained_tags = [spacy.explain(tag) for tag in tags]\n",
        "tokens = [token.text for token in doc]\n",
        "pd.DataFrame({\"token\": tokens, \"tag\": tags, \"descripci√≥n\": explained_tags})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a844ff84",
      "metadata": {
        "id": "a844ff84"
      },
      "source": [
        "Con las etiquetas de tipo POS podemos filtrar palabras de acuerdo a su funci√≥n gramatical dentro del texto. Por ejemplo, podemos filtrar todos los nombres propios:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "115cb09d",
      "metadata": {
        "id": "115cb09d"
      },
      "outputs": [],
      "source": [
        "propn = list(filter(lambda token: token.tag_ == \"NNP\", doc))\n",
        "print(propn)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09316763",
      "metadata": {
        "id": "09316763"
      },
      "source": [
        "### **5.5. Parser**\n",
        "---\n",
        "\n",
        "El componente `parser` hace uso de la clase `DependencyParser` para estimar dependencias entre palabras de acuerdo a su funci√≥n gramatical, en especial, se crean los siguientes atributos sobre un `Token`:\n",
        "\n",
        "* `Token.dep_`: tipo de relaci√≥n.\n",
        "* `Token.head`: token con el que hay una relaci√≥n.\n",
        "\n",
        "El proceso de extracci√≥n de dependencias busca determinar relaciones y dependencias entre palabras en una oraci√≥n con el fin de analizar su estructura gramatical.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1BgTWAed6WN57C-mQrN55-D8PWBDsVedJ\" width=\"100%\">\n",
        "\n",
        "Existen distintos tipos de dependencias, las cuales puede encontrar en: [https://universaldependencies.org/u/dep/](https://universaldependencies.org/u/dep/). Por lo general, las etiquetas de dependencias se calculan a partir de las etiquetas POS y otros tipos de atributos. Veamos como listar las dependencias en `spacy` usando los atributos `dep_` y `head`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6bc63321",
      "metadata": {
        "id": "6bc63321"
      },
      "outputs": [],
      "source": [
        "data = []\n",
        "for token in doc:\n",
        "    data.append(\n",
        "            (token.text, token.dep_, token.head.text)\n",
        "            )\n",
        "pd.DataFrame(\n",
        "        data,\n",
        "        columns=[\"Token 1\", \"Relaci√≥n\", \"Token 2\"]\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb07bba3",
      "metadata": {
        "id": "cb07bba3"
      },
      "source": [
        "`spacy` cuenta con el m√≥dulo `displacy` para generar gr√°ficas y entender las dependencias como un grafo.\n",
        "\n",
        "Veamos un ejemplo donde usamos la funci√≥n `render`, especificando el documento `doc`, el estilo del gr√°fico como √°rbol de dependencias `style = \"dep\"`, especificamos que el gr√°fico se mostrar√° en un notebook `jupyter=True` y algunas opciones que permiten modificar la forma en la que se muestra el diagrama:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "85999151",
      "metadata": {
        "id": "85999151"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=True,\n",
        "        options={\"distance\": 110}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b473eeed",
      "metadata": {
        "id": "b473eeed"
      },
      "source": [
        "Es posible configurar algunas opciones adicionales mediante el argumento  `options`:\n",
        "\n",
        "<table>\n",
        "<tr><th>Opci√≥n</th><th>Tipo</th><th>Descripci√≥n</th><th>Valor por defecto</th></tr>\n",
        "<tr><td>`compact`</td><td>bool</td><td>\"Modo compacto\" con flechas cuadradas que ocupan menos espacio.</td><td>`False`</td></tr>\n",
        "<tr><td>`color`</td><td>unicode</td><td>Color del texto (HEX, RGB o nombres de colores).</td><td>`#000000`</td></tr>\n",
        "<tr><td>`bg`</td><td>unicode</td><td>Color del fondo (HEX, RGB o nombres de colores).</td><td>`#ffffff`</td></tr>\n",
        "<tr><td>`font`</td><td>unicode</td><td>Fuente para todo el texto.</td><td>`Arial`</td></tr>\n",
        "</table>\n",
        "\n",
        "Para ver la lista de opciones completa: https://spacy.io/api/top-level#displacy_options\n",
        "\n",
        "Veamos un ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06a8c576",
      "metadata": {
        "id": "06a8c576"
      },
      "outputs": [],
      "source": [
        "options = {\n",
        "        \"distance\": 110,\n",
        "        \"compact\": \"True\",\n",
        "        \"color\": \"yellow\",\n",
        "        \"bg\": \"#09a3d5\",\n",
        "        \"font\": \"Times\"\n",
        "        }\n",
        "\n",
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=True,\n",
        "        options=options\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7eb37ee",
      "metadata": {
        "id": "d7eb37ee"
      },
      "source": [
        "Tambi√©n, es posible exportar el gr√°fico como una imagen en formato `svg`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e861f7a9",
      "metadata": {
        "id": "e861f7a9"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"dep\",\n",
        "        jupyter=False,\n",
        "        options=options\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "915d363b",
      "metadata": {
        "id": "915d363b"
      },
      "source": [
        "Creamos el archivo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "087a53b1",
      "metadata": {
        "id": "087a53b1"
      },
      "outputs": [],
      "source": [
        "with open(\"deps.svg\", \"w\") as f:\n",
        "    f.write(graph)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "241c2d8c",
      "metadata": {
        "id": "241c2d8c"
      },
      "source": [
        "### **5.6. Lemmatizer**\n",
        "---\n",
        "\n",
        "El componente `lemmatizer` se realiza por medio de la clase `Lemmatizer`, este proceso busca reducir una palabra a su ra√≠z. Este procedimiento puede ser aplicado en aquellos casos donde no es necesario distinguir entre singulares y plurales de palabras o diversas conjugaciones de verbos regulares y deseamos agrupar estos tokens en una √∫nica representaci√≥n, su ra√≠z.\n",
        "\n",
        "La _Lemmatization_ es uno de los procedimientos m√°s sencillos para reducir una palabra a su ra√≠z, consiste en cortar las palabras y agruparlas en una ra√≠z com√∫n. Por ejemplo, _eat, eating y eaten_ son agrupadas en la palabra _eat_.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=17D7d4bXcTuv4XQCT7-NrViKJgBF6hRED\" width=\"75%\">\n",
        "\n",
        "\n",
        "El `Lemmatizer` agrega el atributo `lemma_` sobre cada token con su versi√≥n modificada.\n",
        "\n",
        "Veamos c√≥mo podemos extraer los lemmas de los tokens del documento que estamos manejando:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "385850e0",
      "metadata": {
        "id": "385850e0"
      },
      "outputs": [],
      "source": [
        "lemmas = [token.lemma_ for token in doc]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af823eb",
      "metadata": {
        "id": "3af823eb"
      },
      "source": [
        "A continuaci√≥n podemos verificar solo aquellos tokens cuyo lemma sea diferente al texto original:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51fc110a",
      "metadata": {
        "id": "51fc110a"
      },
      "outputs": [],
      "source": [
        "lemmas = [(token.text, token.lemma_) for token in doc if token.text != token.lemma_]\n",
        "print(lemmas)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4c819713",
      "metadata": {
        "id": "4c819713"
      },
      "source": [
        "### **5.7. NER**\n",
        "---\n",
        "\n",
        "El componente `ner` hace uso de la clase `EntityRecognizer`, esta busca etiquetar segmentos del texto como entidades en un proceso conocido como reconocimiento de entidades nombradas (_Named Entity Recognition_ - NER).\n",
        "\n",
        "NER es una tarea en procesamiento de lenguaje natural que sirve para extraer informaci√≥n estructurada a partir de un texto, por ejemplo: nombres de organizaciones, personas y lugares. Tambi√©n podemos usar NER para extraer entidades como nombres de productos, conceptos m√©dicos, nombres de autores, nombres de marcas, entre otros.\n",
        "\n",
        "Este componente asigna el atributo `Doc.ents` que nos permite identificar en qu√© lugar y de qu√© tipo es cada entidad como un objeto de tipo `Span`. Cada entidad tiene los siguientes atributos:\n",
        "\n",
        "| Atributo | Descripci√≥n |\n",
        "| --- | --- |\n",
        "| `ent.text` | Texto de la entidad. |\n",
        "| `ent.label_` | Descripci√≥n textual de la entidad. |\n",
        "| `ent.start` | √çndice que indica el comienzo de la entidad (tokens). |\n",
        "| `ent.end` | √çndice que indica el final de la entidad (tokens). |\n",
        "| `ent.start_char` | Caracter inicial de la entidad (caracteres). |\n",
        "| `ent.end_char` | Caracter final de la entidad (caracteres). |\n",
        "\n",
        "Veamos un listado de las entidades en el documento que estamos usando de ejemplo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "390cb2f3",
      "metadata": {
        "id": "390cb2f3"
      },
      "outputs": [],
      "source": [
        "ents = doc.ents\n",
        "print(ents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6d055b5",
      "metadata": {
        "id": "e6d055b5"
      },
      "source": [
        "Podemos extraer informaci√≥n de las mismas y mostrarla como un dataframe:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce5b9a21",
      "metadata": {
        "id": "ce5b9a21"
      },
      "outputs": [],
      "source": [
        "ent_text = [ent.text for ent in ents]\n",
        "ent_type = [ent.label_ for ent in ents]\n",
        "ent_start = [ent.start for ent in ents]\n",
        "ent_end = [ent.end for ent in ents]\n",
        "ent_startc = [ent.start_char for ent in ents]\n",
        "ent_endc = [ent.end_char for ent in ents]\n",
        "pd.DataFrame({\n",
        "    \"texto\": ent_text, \"ner\": ent_type, \"palabra_inicial\": ent_start,\n",
        "    \"palabra_final\": ent_end, \"caracter_inicial\": ent_startc,\n",
        "    \"caracter_final\": ent_endc\n",
        "    })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "576784cb",
      "metadata": {
        "id": "576784cb"
      },
      "source": [
        "El _pipeline_ general en ingl√©s tiene las siguientes opciones para detecci√≥n de entidades nombradas:\n",
        "\n",
        "<table>\n",
        "<tr><th>Tipo</th><th>Descripci√≥n</th><th>Ejemplo</th></tr>\n",
        "<tr><td>PERSON</td><td>Personas, incluyendo personajes ficticios.</td><td>Fred Flintstone</td></tr>\n",
        "<tr><td>NORP</td><td>Nacionalidades o grupos pol√≠ticos o religiosos.</td><td>The Republican Party</td></tr>\n",
        "<tr><td>FAC</td><td>Edificios, aeropuertos, autopistas, puentes, etc.</td><td>Logan International Airport, The Golden Gate</td></tr>\n",
        "<tr><td>ORG</td><td>Compa√±√≠as, agencias, instituciones, etc.</td><td>Microsoft, FBI, MIT</td></tr>\n",
        "<tr><td>GPE</td><td>Pa√≠ses, ciudades, estados.</td><td>France, UAR, Chicago, Idaho</td></tr>\n",
        "<tr><td>LOC</td><td>Ubicaciones no geopol√≠ticas, cadenas monta√±osas, cuerpos de agua.</td><td>Europe, Nile River, Midwest</td></tr>\n",
        "<tr><td>PRODUCT</td><td>Objetos, veh√≠culos, comidas, etc. (No servicios.)</td><td>Formula 1</td></tr>\n",
        "<tr><td>EVENT</td><td>Huracanes nombrados, batallas, guerras, eventos deportivos, etc.</td><td>Olympic Games</td></tr>\n",
        "<tr><td>WORK_OF_ART</td><td>T√≠tulos de libros, canciones, pinturas, etc.</td><td>The Mona Lisa</td></tr>\n",
        "<tr><td>LAW</td><td>Documentos nombrados convertidos en leyes.</td><td>Roe v. Wade</td></tr>\n",
        "<tr><td>LANGUAGE</td><td>Cualquier idioma.</td><td>English</td></tr>\n",
        "<tr><td>DATE</td><td>Fechas o periodos absolutos o relativos.</td><td>*20 July 1969*</td></tr>\n",
        "<tr><td>TIME</td><td>Periodos de tiempo inferiores a un d√≠a.</td><td>Four hours</td></tr>\n",
        "<tr><td>PERCENT</td><td>Porcentaje, incluyendo \"%\".</td><td>Eighty percent</td></tr>\n",
        "<tr><td>MONEY</td><td>Valores monetarios, incluyendo unidades.</td><td>Twenty Cents</td></tr>\n",
        "<tr><td>QUANTITY</td><td>Medidas, como de peso o distancia.</td><td>Several kilometers, 55kg</td></tr>\n",
        "<tr><td>ORDINAL</td><td>Valores ordinales (primero, segundo, tercero, etc.).</td><td>9th, Ninth</td></tr>\n",
        "<tr><td>CARDINAL</td><td>Numerales que no entran en otra categor√≠a.</td><td>2, Two, Fifty-two</td></tr>\n",
        "</table>\n",
        "\n",
        "Adicional a esto, podemos usar `displacy` para visualizar entidades nombradas del texto. La √∫nica diferencia con respecto a la visualizaci√≥n de dependencias es que en este caso pasamos como argumento `\"ent\"` al par√°metro `style`, de la siguiente forma:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be68fe96",
      "metadata": {
        "id": "be68fe96"
      },
      "outputs": [],
      "source": [
        "graph = spacy.displacy.render(\n",
        "        doc,\n",
        "        style=\"ent\",\n",
        "        jupyter=True,\n",
        "        options={\"distance\": 110}\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4b84e732",
      "metadata": {
        "id": "4b84e732"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrar√° informaci√≥n √∫til para profundizar en los temas vistos en este taller guiado:\n",
        "\n",
        "- [Gu√≠a de uso de Spacy](https://spacy.io/usage).\n",
        "- [Modelos de Spacy](https://spacy.io/models).\n",
        "- [C√≥digo fuente de Spacy](https://github.com/explosion/spaCy)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0be0ba0b",
      "metadata": {
        "id": "0be0ba0b"
      },
      "source": [
        "## Cr√©ditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebasti√°n Lara Ram√≠rez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Dise√±o de im√°genes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "* **Coordinador de virtualizaci√≥n:**\n",
        "    - [Edder Hern√°ndez Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingenier√≠a*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}