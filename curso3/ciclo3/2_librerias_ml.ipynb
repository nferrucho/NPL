{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso3/ciclo3/2_librerias_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "deac5bc2",
      "metadata": {
        "id": "deac5bc2"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=14reVO1X6LsjqJ3cFgoeHxxddZVGfZn3t\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "309c2f86",
      "metadata": {
        "id": "309c2f86"
      },
      "source": [
        "# Librerías para Modelamiento\n",
        "---\n",
        "\n",
        "En este notebook veremos algunas librerías típicas para modelamiento en _Python_ y su integración con herramientas como `mlflow`\n",
        "\n",
        "Comenzamos instalando e importando las librerías necesarias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df33fb53",
      "metadata": {
        "id": "df33fb53"
      },
      "outputs": [],
      "source": [
        "!pip install mlflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f8fa01b",
      "metadata": {
        "id": "4f8fa01b"
      },
      "outputs": [],
      "source": [
        "import mlflow\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b547e2bb",
      "metadata": {
        "id": "b547e2bb"
      },
      "source": [
        "Adicionalmente, utilizaremos un servidor de `mlflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a592282",
      "metadata": {
        "id": "0a592282"
      },
      "outputs": [],
      "source": [
        "command = \"\"\"\n",
        "mlflow server \\\n",
        "        --backend-store-uri sqlite:///tracking.db \\\n",
        "        --default-artifact-root file:mlruns \\\n",
        "        -p 5000 &\n",
        "\"\"\"\n",
        "get_ipython().system_raw(command)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1dd691d4",
      "metadata": {
        "id": "1dd691d4"
      },
      "source": [
        "Utilizaremos `ngrok` para acceder al tablero de `mlflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08dddefd",
      "metadata": {
        "id": "08dddefd"
      },
      "outputs": [],
      "source": [
        "!pip install pyngrok"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "19503fbe",
      "metadata": {
        "id": "19503fbe"
      },
      "source": [
        "Ahora debe agregar su token de `ngrok`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46cb85d8",
      "metadata": {
        "id": "46cb85d8"
      },
      "outputs": [],
      "source": [
        "token = \"2RDTkx1xYfhrYr76sGlFIszN9ur_7UDSixSUBcBdW8UmTNuie\" # Agregue el token dentro de las comillas\n",
        "os.environ[\"NGROK_TOKEN\"] = token"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4961a49c",
      "metadata": {
        "id": "4961a49c"
      },
      "source": [
        "Nos autenticamos en ngrok:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "37414843",
      "metadata": {
        "id": "37414843"
      },
      "outputs": [],
      "source": [
        "!ngrok authtoken $NGROK_TOKEN"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "636dde67",
      "metadata": {
        "id": "636dde67"
      },
      "source": [
        "Ahora, lanzamos la conexión con ngrok:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98c9bbdc",
      "metadata": {
        "id": "98c9bbdc"
      },
      "outputs": [],
      "source": [
        "from pyngrok import ngrok\n",
        "ngrok.connect(5000, \"http\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1819c53",
      "metadata": {
        "id": "d1819c53"
      },
      "source": [
        "Especificamos que MLFlow debe usar el servidor que estamos manejando."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a77ac26",
      "metadata": {
        "id": "5a77ac26"
      },
      "outputs": [],
      "source": [
        "mlflow.set_tracking_uri(\"http://localhost:5000\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba09ba8d",
      "metadata": {
        "id": "ba09ba8d"
      },
      "source": [
        "Vamos a crear un experimento en MLFlow para este conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f453859a",
      "metadata": {
        "id": "f453859a"
      },
      "outputs": [],
      "source": [
        "exp_id = mlflow.create_experiment(name=\"iris\", artifact_location=\"mlruns/\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cb764b15",
      "metadata": {
        "id": "cb764b15"
      },
      "source": [
        "## **1. Motivación**\n",
        "---\n",
        "\n",
        "Actualmente, existen distintas librerías para modelamiento en _Python_, esto se debe a las siguientes razones:\n",
        "\n",
        "- **Abundancia de tareas**: Existen muchas tareas diferentes en machine learning, como la clasificación, la regresión, la agrupación, la detección de anomalías, etc. Cada tarea requiere un enfoque diferente.\n",
        "- **Diferentes enfoques y algoritmos**: Hay muchos algoritmos diferentes que se pueden utilizar para resolver una tarea de machine learning, en donde cada uno tiene sus propios fortalezas y debilidades.\n",
        "- **Flexibilidad**: Algunas librerías están diseñadas para ser flexibles y permiten a los usuarios personalizar y crear sus propios algoritmos y modelos.\n",
        "- **Mejoras constantes**: La investigación en machine learning es muy activa, y los investigadores y desarrolladores están continuamente publicando nuevos algoritmos y mejoras en los existentes.\n",
        "- **Diferentes niveles de complejidad**: Algunas librerías están diseñadas para ser fáciles de usar y accesibles para principiantes, mientras que otras están destinadas a expertos y requieren un mayor conocimiento y habilidades.\n",
        "\n",
        "En cuanto a esto, `mlflow` nos ayuda a estandarizar y versionar modelos de distintas librerías y con distintos lenguajes de programación:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1B2i0jW-76YrOZY6rG80OC_6zc01v06pA\" width=\"80%\">\n",
        "\n",
        "Para la demostración de las librerías, utilizaremos el conjunto de datos Iris, la cual trata de un conjunto de datos clásico utilizado en el aprendizaje automático y la investigación en inteligencia artificial. Este conjunto de datos contiene información sobre las características de tres tipos diferentes de flores Iris: Iris setosa, Iris versicolor e Iris virginica.\n",
        "\n",
        "El conjunto de datos incluye información sobre las medidas de cuatro características de las flores: la longitud y el ancho del sépalo y pétalo. Estas características se utilizan para identificar y clasificar los diferentes tipos de flores.\n",
        "\n",
        "Este conjunto de datos se utiliza a menudo como un problema de clasificación para modelos de aprendizaje automático, y es uno de los conjuntos de datos más utilizados para evaluar la precisión y la eficacia de los algoritmos de aprendizaje automático. También se utiliza para explorar y visualizar patrones y relaciones en los datos y para evaluar la importancia de las características en la identificación y clasificación de las flores.\n",
        "\n",
        "Cargamos el conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eeeec2ea",
      "metadata": {
        "id": "eeeec2ea"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_iris\n",
        "data = load_iris()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472b82dd",
      "metadata": {
        "id": "472b82dd"
      },
      "outputs": [],
      "source": [
        "features, labels, labels_desc = data[\"data\"], data[\"target\"], data[\"target_names\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2d39e8c7",
      "metadata": {
        "id": "2d39e8c7"
      },
      "source": [
        "Podemos ver el número (posición) asignado a cada categoría de flor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03bc0d2b",
      "metadata": {
        "id": "03bc0d2b"
      },
      "outputs": [],
      "source": [
        "print(labels_desc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f30aeb96",
      "metadata": {
        "id": "f30aeb96"
      },
      "source": [
        "Vamos a trabajar únicamente con `setosa` y `versicolor`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfa7fb3a",
      "metadata": {
        "id": "cfa7fb3a"
      },
      "outputs": [],
      "source": [
        "mask = (labels == 1) | (labels == 2)\n",
        "features = features[mask]\n",
        "labels = labels[mask] - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "495934a9",
      "metadata": {
        "id": "495934a9"
      },
      "source": [
        "Veamos ejemplos de modelamiento con este conjunto de datos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f099c3a0",
      "metadata": {
        "id": "f099c3a0"
      },
      "source": [
        "## **2. statsmodels**\n",
        "---\n",
        "\n",
        "`statsmodels` es una librería de Python para la estimación y el análisis de modelos estadísticos. Proporciona una amplia gama de modelos y herramientas para la modelación y análisis de datos, incluyendo modelos lineales, modelos de series temporales, modelos de regresión y modelos de análisis de varianza.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Z_tWT8gwISX_y7aPMHnl7ZDeHyGOrTpF\" width=\"80%\">\n",
        "\n",
        "`statsmodels` es una librería de análisis de datos muy completa que ofrece una amplia gama de funciones y métodos para la estimación, validación y visualización de modelos estadísticos. Es utilizado por investigadores, analistas de datos y científicos de la computación para realizar análisis estadísticos rigurosos y explorar patrones y tendencias en los datos.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Ox6TLvcMATIK_j-8p2qytLjCBDHIA9Bv\" width=\"80%\">\n",
        "\n",
        "Entre sus características se encuentran la capacidad de realizar pruebas estadísticas, diagnósticos de modelos, selección de variables y mucho más. Esta librería es una herramienta importante para cualquier persona interesada en la investigación estadística y el análisis de datos.\n",
        "\n",
        "Comenzamos instalándola:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a2596093",
      "metadata": {
        "id": "a2596093"
      },
      "outputs": [],
      "source": [
        "!pip install statsmodels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "abab2fcd",
      "metadata": {
        "id": "abab2fcd"
      },
      "source": [
        "En este caso, implementaremos un modelo de regresión logística con `statsmodels`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a96bb5e",
      "metadata": {
        "id": "5a96bb5e"
      },
      "outputs": [],
      "source": [
        "import statsmodels.api as sm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "286e6e16",
      "metadata": {
        "id": "286e6e16"
      },
      "source": [
        "Vamos a utilizar el `accuracy` como métrica general de desempeño:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "56a1b26c",
      "metadata": {
        "id": "56a1b26c"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f55438af",
      "metadata": {
        "id": "f55438af"
      },
      "source": [
        "Vamos a definir y a entrenar el modelo como una ejecución de `mlflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bf440499",
      "metadata": {
        "id": "bf440499"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(\n",
        "        run_name=\"statsmodels\", experiment_id=exp_id\n",
        "        ):\n",
        "    model = sm.GLM(\n",
        "        endog=labels,\n",
        "        exog=features,\n",
        "        family=sm.families.Binomial(),\n",
        "        ) # regresión logística\n",
        "    results = model.fit() # entrenamiento\n",
        "    y_pred = (results.predict(features) > .5).astype(\"int\") # predicción continúa\n",
        "    acc = accuracy_score(labels, y_pred) # evaluación\n",
        "    mlflow.statsmodels.log_model(results, \"model\") # registro de modelo\n",
        "    mlflow.log_metrics({\n",
        "        \"accuracy\": acc\n",
        "        }) # registro de métrica"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0ff6b51",
      "metadata": {
        "id": "d0ff6b51"
      },
      "source": [
        "## **3. scikit-learn**\n",
        "---\n",
        "\n",
        "`scikit-learn` es una biblioteca de código abierto para aprendizaje automático en Python. Ofrece una amplia gama de algoritmos y técnicas para el análisis de datos y la construcción de modelos de aprendizaje automático, incluyendo regresión, clasificación, agrupamiento, reducción de dimensionalidad y selección de características.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1Dsd4f1PTiz-Tois1TbJVQnX_sh0_V_a9\" width=\"80%\">\n",
        "\n",
        "`scikit-learn` es una biblioteca muy accesible y fácil de usar, con una interfaz consistente para todos los algoritmos. Además, proporciona una amplia documentación y una comunidad activa de desarrolladores y usuarios.\n",
        "\n",
        "La biblioteca se utiliza ampliamente en la industria y en la investigación para la construcción y evaluación de modelos de aprendizaje automático. Es una herramienta esencial para cualquier persona interesada en el análisis de datos y el aprendizaje automático en Python.\n",
        "\n",
        "Veamos cómo entrenar una máquina de soporte vectorial sobre estos datos y registrar el modelo con `mlflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cb5aa234",
      "metadata": {
        "id": "cb5aa234"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "with mlflow.start_run(\n",
        "        run_name=\"sklearn\", experiment_id=exp_id\n",
        "        ):\n",
        "    model = SVC().fit(features, labels) # definición del modelo.\n",
        "    y_pred = model.predict(features) # predicción\n",
        "    acc = accuracy_score(labels, y_pred) # evaluación\n",
        "    mlflow.sklearn.log_model(model, \"model\") # registro de modelo\n",
        "    mlflow.log_metrics({\n",
        "        \"accuracy\": acc\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a46f1530",
      "metadata": {
        "id": "a46f1530"
      },
      "source": [
        "## **4. xgboost**\n",
        "---\n",
        "\n",
        "`xgboost` es una biblioteca de aprendizaje automático de código abierto diseñada para resolver problemas de clasificación y regresión. Se basa en el algoritmo de gradient boosting y proporciona una implementación eficiente y escalable para resolver problemas de aprendizaje automático.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1D1ZqBu5_BfmIQ_XI-dtuXBqehXd0pQot\" width=\"80%\">\n",
        "\n",
        "El algoritmo `xgboost` es conocido por ser uno de los mejores algoritmos de aprendizaje automático en términos de precisión y rendimiento. Se ha utilizado ampliamente en competiciones de aprendizaje automático en línea como Kaggle.\n",
        "\n",
        "Además de ser un algoritmo de alta precisión, `xgboost` también es fácil de usar y se integra fácilmente con otros paquetes de _Python_ como `numpy`, `pandas` y `scikit-learn`. Esto lo hace una herramienta popular para investigadores y profesionales de la industria que buscan resolver problemas de aprendizaje automático de manera efectiva y eficiente.\n",
        "\n",
        "El algoritmo Gradient Boosting es una técnica de aprendizaje automático en la que se combinan varios modelos simples (llamados \"boosters\") para crear un modelo más complejo y preciso. Cada booster es un modelo que aprende de los errores del modelo anterior y trata de corregirlos.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1GERxdURZCpSl4zt4xGnZhyJ43KKfwD31\" width=\"80%\">\n",
        "\n",
        "El proceso funciona de la siguiente manera:\n",
        "\n",
        "1. Se inicializa un modelo básico, por ejemplo, un árbol de decisiones sencillo.\n",
        "2. Se ajusta el modelo a los datos de entrenamiento y se mide el error.\n",
        "3. Se entrena un segundo modelo, o booster, para corregir los errores del primer modelo.\n",
        "4. Se combinan ambos modelos en un modelo más complejo.\n",
        "5. Se repiten los pasos 2 a 4 para entrenar más boosters y mejorar la precisión del modelo.\n",
        "6. El proceso continúa hasta que se alcance un cierto nivel de precisión o se agoten los boosters disponibles.\n",
        "\n",
        "Gradient Boosting es una técnica efectiva para resolver problemas de clasificación y regresión. Se utiliza ampliamente en la industria y en la investigación. Además, XGBoost es una implementación popular y eficiente de Gradient Boosting.\n",
        "\n",
        "Veamos cómo instalar la librería:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e6cffb6",
      "metadata": {
        "id": "2e6cffb6"
      },
      "outputs": [],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0970191",
      "metadata": {
        "id": "c0970191"
      },
      "source": [
        "Importamos el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b93756e2",
      "metadata": {
        "id": "b93756e2"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4d6ba51",
      "metadata": {
        "id": "c4d6ba51"
      },
      "source": [
        "El entrenamiento del modelo es equivalente a cómo normalmente se realiza en `sklearn`. Aquí están algunos de los hiperparámetros más importantes en `XGBClassifier`:\n",
        "\n",
        "- `booster`: Especifica el tipo de modelo base que se utilizará en el ensamblaje de XGBoost. Los valores posibles son `gbtree` (árbol de decisiones) y `dart` (ensamblaje aleatorio).\n",
        "- `max_depth`: Especifica la profundidad máxima del árbol. Un valor más alto significa que el modelo puede tener ramas más profundas, lo que aumenta la complejidad del modelo.\n",
        "- `learning_rate`: Especifica la tasa de aprendizaje utilizada para actualizar los pesos en el modelo. Un valor más alto significa que el modelo aprende más rápido, pero también aumenta el riesgo de sobreajuste.\n",
        "- `n_estimators`: Especifica el número de árboles que se usarán en el ensamblaje. Un valor más alto significa que el modelo será más robusto, pero también aumenta el tiempo de entrenamiento y el tamaño del modelo.\n",
        "- `gamma`: Especifica una penalización en el crecimiento del árbol. Un valor más alto significa que el modelo es menos propenso a crecer ramas innecesarias.\n",
        "- `min_child_weight`: Especifica una penalización en el crecimiento de las hojas en el árbol. Un valor más alto significa que el modelo será menos propenso a crear hojas con pocas muestras.\n",
        "- `subsample`: Especifica la fracción de muestras que se usarán en cada árbol. Un valor más bajo significa que el modelo será más robusto, pero también aumenta el tiempo de entrenamiento.\n",
        "\n",
        "Estos son solo algunos de los hiperparámetros disponibles en `XGBClassifier`. El ajuste adecuado de ellos puede tener un gran impacto en el rendimiento del modelo. Es importante realizar una búsqueda en grid o validación cruzada para encontrar los valores óptimos para su conjunto de datos.\n",
        "\n",
        "Vamos a definir el modelo, lo entrenamos y registramos dentro de `mlflow`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ed66485",
      "metadata": {
        "id": "0ed66485"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af76e8eb",
      "metadata": {
        "id": "af76e8eb"
      },
      "outputs": [],
      "source": [
        "with mlflow.start_run(\n",
        "        run_name=\"xgboost\", experiment_id=exp_id\n",
        "        ):\n",
        "\n",
        "    model = XGBClassifier(\n",
        "            n_estimators = 20,\n",
        "            max_depth = 7,\n",
        "            learning_rate = 1e-4,\n",
        "            ).fit(features, labels) # entrenamiento\n",
        "    y_pred = model.predict(features)\n",
        "    acc = accuracy_score(labels, y_pred) # evaluación\n",
        "    mlflow.xgboost.log_model(model, \"model\") # registro de modelo\n",
        "    mlflow.log_metrics({\n",
        "        \"accuracy\": acc\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4fea103",
      "metadata": {
        "id": "a4fea103"
      },
      "source": [
        "## **5. tensorflow**\n",
        "---\n",
        "\n",
        "`tensorflow` es una biblioteca de software de código abierto para aprendizaje automático y cálculo numérico en general. Fue desarrollada por Google Brain Team y se lanzó en 2015. `tensorflow` es utilizado para resolver una amplia variedad de problemas de aprendizaje automático, como clasificación, regresión, detección de objetos y procesamiento de lenguaje natural.\n",
        "\n",
        "En `tensorflow`, los datos y los cálculos se representan como grafos computacionales, con nodos que representan operaciones matemáticas y arcos que representan arreglos de datos multidimensionales conocidos como tensores. Los usuarios pueden crear y entrenar modelos de aprendizaje automático utilizando estos grafos, mientras que `tensorflow` se encarga de administrar la ejecución de los cálculos en una variedad de plataformas, incluidas CPUs, GPUs y dispositivos móviles.\n",
        "\n",
        "Además de su flexibilidad y potencia, otro aspecto atractivo de `tensorflow` es su amplia comunidad y documentación, lo que significa que hay muchos recursos disponibles para ayudar a los usuarios a resolver problemas y mejorar sus modelos. En general, `tensorflow` es una herramienta muy valiosa para cualquiera interesado en el aprendizaje automático y el cálculo numérico.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1j5q1PNb3St7UIfuqKXcxSZ2npZO905jb\" width=\"80%\">\n",
        "\n",
        "Una de las principales utilidades de `tensorflow` es que nos permite implementar redes neuronales profundas (Deep Neural Networks - DNNs) de forma sencilla por medio de `keras`. Las DNNs son una clase de modelos de aprendizaje automático basados en el concepto de las redes neuronales artificiales. A diferencia de las redes neuronales tradicionales, que suelen tener solo una o pocas capas ocultas, las DNNs tienen muchas capas ocultas y, por lo tanto, una gran cantidad de parámetros entrenables. Esto les permite aprender representaciones más complejas y abstractas de los datos, lo que las hace muy efectivas para una amplia variedad de tareas.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1MEsKrMrLLsqkWPSPnLV0_WaPIw4xvX9w\" width=\"80%\">\n",
        "\n",
        "> **Nota**: el uso en detalle de `tensorflow` lo verá en el módulo 5 de deep learning. En este caso nos enfocaremos en su integración con `mlflow`.\n",
        "\n",
        "Vamos a definir, entrenar y a registrar el modelo de forma equivalente a los casos anteriores:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43b43ddc",
      "metadata": {
        "id": "43b43ddc"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Input, Dropout\n",
        "from keras.optimizers import Adam\n",
        "from keras.losses import BinaryCrossentropy\n",
        "\n",
        "with mlflow.start_run(\n",
        "        run_name=\"tensorflow\", experiment_id=exp_id\n",
        "        ):\n",
        "\n",
        "    model = Sequential([\n",
        "        Input(shape=(4, )),\n",
        "        Dense(units=32, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(units=16, activation=\"relu\"),\n",
        "        Dropout(0.3),\n",
        "        Dense(units=1, activation=\"sigmoid\")\n",
        "        ]) # definición de modelo\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=1e-3),\n",
        "        loss=BinaryCrossentropy(),\n",
        "        ) # compilado de modelo\n",
        "    model.fit(features, labels, epochs=100, batch_size=256) # entrenamiento\n",
        "    y_pred = (model.predict(features) > .5).astype(\"int\") # predicción continúa\n",
        "    acc = accuracy_score(labels, y_pred) # evaluación\n",
        "    mlflow.tensorflow.log_model(model, \"model\") # registro de modelo\n",
        "    mlflow.log_metrics({\n",
        "        \"accuracy\": acc\n",
        "        })"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "75344efa",
      "metadata": {
        "id": "75344efa"
      },
      "source": [
        "## **6. Inferencia**\n",
        "---\n",
        "\n",
        "Recuerde que una de las ventajas de `mlflow` es que nos permite utilizar modelos de la misma forma, sin importar la librería con la que fueron entrenados.\n",
        "\n",
        "Por ejemplo, puede registrar cualquiera de los modelos bajo el nombre `iris_model`:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1mS53DqJCbA2cjM1F_ujRd9asls5YkEeV\" width=\"80%\">\n",
        "\n",
        "También debe asignar el **staging** a `Production` tal y como se realizó en la unidad pasada. Con esto y de forma independiente al modelo que seleccionará, el siguiente código le permitirá obtener predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aca4be70",
      "metadata": {
        "id": "aca4be70"
      },
      "outputs": [],
      "source": [
        "model_name = 'iris_model'\n",
        "model_version = 1\n",
        "model = mlflow.pyfunc.load_model(f\"models:/{model_name}/{model_version}\")\n",
        "display(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3db24f49",
      "metadata": {
        "id": "3db24f49"
      },
      "source": [
        "Obtenemos las predicciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d4b54e2f",
      "metadata": {
        "id": "d4b54e2f"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(features)\n",
        "display(y_pred[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c84e9180",
      "metadata": {
        "id": "c84e9180"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [MLFlow Models](https://mlflow.org/docs/latest/models.html)\n",
        "- [statsmodels](https://www.statsmodels.org/stable/index.html)\n",
        "- [scikit-learn](https://scikit-learn.org/stable/)\n",
        "- [xgboost](https://xgboost.readthedocs.io/en/stable/)\n",
        "- [tensorflow](https://www.tensorflow.org/)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e837b6f",
      "metadata": {
        "id": "6e837b6f"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "**Profesor**\n",
        "\n",
        "- [Jorge E. Camargo, PhD](https://dis.unal.edu.co/~jecamargom/)\n",
        "\n",
        "**Asistente docente**:\n",
        "\n",
        "- [Juan S. Lara MSc](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/)\n",
        "\n",
        "**Diseño de imágenes:**\n",
        "- [Brian Chaparro Cetina](mailto:bchaparro@unal.edu.co).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}