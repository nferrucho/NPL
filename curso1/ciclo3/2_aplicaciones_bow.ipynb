{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso1/ciclo3/2_aplicaciones_bow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b707f3db",
      "metadata": {
        "id": "b707f3db"
      },
      "source": [
        "<img src=\"https://drive.google.com/uc?export=view&id=1e7ctPi8O3bTQoLZaO9ZZjwGr2r8Z93RS\" width=\"100%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d03444f5",
      "metadata": {
        "id": "d03444f5"
      },
      "source": [
        "# **Aplicaciones de las Bolsas de Palabras**\n",
        "---\n",
        "\n",
        "En este notebook veremos algunas aplicaciones que pueden realizarse con representaciones de bolsas de palabras, en específico hablaremos de resúmenes automáticos de texto, léxicos y nubes de palabras. Comenzaremos importando las librerías de ciencia de datos necesarias para manejo de datos, visualización, y manipulación de strings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4a4b9a24",
      "metadata": {
        "id": "4a4b9a24"
      },
      "outputs": [],
      "source": [
        "!pip install unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b8a71f63",
      "metadata": {
        "id": "b8a71f63"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import spacy\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from unidecode import unidecode\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from IPython.display import display\n",
        "plt.style.use(\"ggplot\")\n",
        "spacy.cli.download(\"es_core_news_sm\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63e1932f",
      "metadata": {
        "id": "63e1932f"
      },
      "source": [
        "Vamos a preprocesar los documentos con una función de preprocesamiento similar a la que usamos en el notebook de bolsa de palabras, no obstante, en este caso también eliminaremos las _stopwords_:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43772b4b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "43772b4b"
      },
      "outputs": [],
      "source": [
        "pat = re.compile(r\"[^a-z ]\")\n",
        "spaces = re.compile(r\"\\s{2,}\")\n",
        "nlp = spacy.load(\n",
        "        \"es_core_news_sm\",\n",
        "        exclude=[\n",
        "            \"attribute_ruler\", \"lemmatizer\", \"ner\"\n",
        "            ]\n",
        "        )\n",
        "def preprocess(text, min_len=1, max_len=23):\n",
        "    # Normalizamos el texto\n",
        "    norm_text = unidecode(text).lower()\n",
        "\n",
        "    # Extraemos tokens\n",
        "    tokens = nlp(norm_text)\n",
        "\n",
        "    # Filtramos palabras por longitud\n",
        "    filtered_tokens = filter(\n",
        "            lambda token: (\n",
        "                len(token) >= min_len and\n",
        "                len(token) <= max_len and\n",
        "                not token.is_stop  # Filtramos stopwords\n",
        "                ),\n",
        "            tokens\n",
        "        )\n",
        "    filtered_text = \" \".join(token.text for token in filtered_tokens)\n",
        "    # Eliminamos caracteres especiales\n",
        "    clean_text = re.sub(pat, \"\", filtered_text)\n",
        "    # Eliminamos espacios duplicados\n",
        "    spaces_text = re.sub(spaces, \" \", clean_text)\n",
        "    return spaces_text.strip()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae3c9cf9",
      "metadata": {
        "id": "ae3c9cf9"
      },
      "source": [
        "## **1. Resumen Extractivo de Texto**\n",
        "---\n",
        "\n",
        "Una de las aplicaciones más típicas de las representaciones de bolsas de palabras es el resumen extractivo de texto. Se trata de una aplicación muy típica relacionada con el procesamiento de lenguaje natural, donde se busca resumir la información que hay en un documento, obteniendo una versión corta que conserve la mayor cantidad de información posible del texto original.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1s5Mq_j5QIPatLwoliojU2YvKBgWl003n\" width=\"100%\">\n",
        "\n",
        "Normalmente el resumen automático de texto se aborda desde dos enfoques:\n",
        "\n",
        "- **Extractivo**: consiste en extraer los _chunks_ o fragmentos del texto con mayor importancia.\n",
        "- **Abstractivo**: consiste en extraer una síntesis del texto que no necesariamente tiene un contenido textual.\n",
        "\n",
        "En este caso, veremos un ejemplo con _TF-IDF_. Comenzaremos definiendo el texto que vamos a resumir:\n",
        "\n",
        "> Alan Mathison Turing fue un matemático, lógico, informático teórico, criptógrafo, filósofo y biólogo teórico británico. Está considerado uno de los padres de la ciencia de la computación y precursor de la informática moderna. Proporcionó una influyente formalización de los conceptos de algoritmo y computación: la máquina de Turing. Formuló su propia versión que hoy es ampliamente aceptada como la tesis de Church-Turing (1936). Durante la segunda guerra mundial, trabajó en descifrar los códigos nazis, particularmente los de la máquina Enigma, y durante un tiempo fue el director de la sección Naval Enigma de Bletchley Park. Se ha estimado que su trabajo acortó la duración de esa guerra entre dos y cuatro años. Tras la guerra, diseñó uno de los primeros computadores electrónicos programables digitales en el Laboratorio Nacional de Física del Reino Unido y poco tiempo después construyó otra de las primeras máquinas en la Universidad de Mánchester. En el campo de la inteligencia artificial, es conocido sobre todo por la concepción de la prueba de Turing (1950), un criterio según el cual puede juzgarse la inteligencia de una máquina si sus respuestas en la prueba son indistinguibles de las de un ser humano."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43a2d50f",
      "metadata": {
        "id": "43a2d50f"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"Alan Mathison Turing fue un matemático, lógico, informático teórico, criptógrafo, filósofo y biólogo teórico británico. Está considerado uno de los padres de la ciencia de la computación y precursor de la informática moderna. Proporcionó una influyente formalización de los conceptos de algoritmo y computación: la máquina de Turing. Formuló su propia versión que hoy es ampliamente aceptada como la tesis de Church-Turing (1936). Durante la segunda guerra mundial, trabajó en descifrar los códigos nazis, particularmente los de la máquina Enigma, y durante un tiempo fue el director de la sección Naval Enigma de Bletchley Park. Se ha estimado que su trabajo acortó la duración de esa guerra entre dos y cuatro años. Tras la guerra, diseñó uno de los primeros computadores electrónicos programables digitales en el Laboratorio Nacional de Física del Reino Unido y poco tiempo después construyó otra de las primeras máquinas en la Universidad de Mánchester. En el campo de la inteligencia artificial, es conocido sobre todo por la concepción de la prueba de Turing (1950), un criterio según el cual puede juzgarse la inteligencia de una máquina si sus respuestas en la prueba son indistinguibles de las de un ser humano.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b5041d6",
      "metadata": {
        "id": "6b5041d6"
      },
      "source": [
        "Vamos a dividir el texto por oraciones usando `spacy`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1d9144d",
      "metadata": {
        "id": "a1d9144d"
      },
      "outputs": [],
      "source": [
        "doc = nlp(text)\n",
        "sents = list(map(lambda sent: sent.text, doc.sents))\n",
        "display(sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2b6c8e0",
      "metadata": {
        "id": "d2b6c8e0"
      },
      "source": [
        "Ahora, preprocesamos el texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58eb3bec",
      "metadata": {
        "id": "58eb3bec"
      },
      "outputs": [],
      "source": [
        "prep_sents = list(map(preprocess, sents))\n",
        "display(prep_sents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13c03cf3",
      "metadata": {
        "id": "13c03cf3"
      },
      "source": [
        "Extraemos una representación _TF-IDF_ de las oraciones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "925efeee",
      "metadata": {
        "id": "925efeee"
      },
      "outputs": [],
      "source": [
        "X = (\n",
        "        TfidfVectorizer(norm=None)\n",
        "        .fit_transform(prep_sents)\n",
        "        .toarray()\n",
        "        )\n",
        "display(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76a2309e",
      "metadata": {
        "id": "76a2309e"
      },
      "source": [
        "Podemos saber qué tan importante es cada una de las oraciones al extraer la norma Euclidiana de cada uno de los vectores de documento:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "220611b2",
      "metadata": {
        "id": "220611b2"
      },
      "outputs": [],
      "source": [
        "scores = np.linalg.norm(X, axis=1)\n",
        "display(scores.size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ee458ac6",
      "metadata": {
        "id": "ee458ac6"
      },
      "source": [
        "Como se puede ver, tenemos una importancia para cada una de las oraciones del texto. Vamos a crear un `DataFrame` con las oraciones y los scores para reordenar y extraer las oraciones más importantes del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3901ab4",
      "metadata": {
        "id": "f3901ab4"
      },
      "outputs": [],
      "source": [
        "scored_text = (\n",
        "        pd.DataFrame({\"text\": sents, \"score\": scores})\n",
        "        .sort_values(by=\"score\", ascending=False)\n",
        "        )\n",
        "display(scored_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ef5396d7",
      "metadata": {
        "id": "ef5396d7"
      },
      "source": [
        "Un resumen extractivo podría estar dado por la oración más relevante:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97f0907f",
      "metadata": {
        "id": "97f0907f"
      },
      "outputs": [],
      "source": [
        "display(scored_text.text.head(1).values[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "460505a3",
      "metadata": {
        "id": "460505a3"
      },
      "source": [
        "## **2. Léxicos**\n",
        "---\n",
        "\n",
        "Los léxicos son una forma de incorporar conceptos y semántica en las representaciones de bolsas de palabras. Este proceso consiste en estructurar léxicos (vocabularios) según determinados conceptos (categorías).\n",
        "\n",
        "Un ejemplo típico de esto es [EmoLex](https://saifmohammad.com/WebPages/NRC-Emotion-Lexicon.htm), se trata de un diccionario de palabras en inglés etiquetado en 10 distintas emociones.\n",
        "\n",
        "Veamos cómo podemos cargar el _EmoLex_. Primero lo descargamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0712a4cc",
      "metadata": {
        "id": "0712a4cc"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u3/emolex.json' -O 'emolex.json'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84ccd8fb",
      "metadata": {
        "id": "84ccd8fb"
      },
      "source": [
        "Ahora, lo cargamos con la librería `json`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "420a99c4",
      "metadata": {
        "id": "420a99c4"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "with open(\"emolex.json\") as f:\n",
        "    vocab = json.load(f)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c472de89",
      "metadata": {
        "id": "c472de89"
      },
      "source": [
        "Como se puede ver, tenemos distintos vocabularios para 10 conceptos distintos (emociones). Veamos de qué emociones disponemos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f81b6b9f",
      "metadata": {
        "id": "f81b6b9f"
      },
      "outputs": [],
      "source": [
        "display(vocab.keys())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "459d2569",
      "metadata": {
        "id": "459d2569"
      },
      "source": [
        "Podemos ver la cantidad de palabras por emoción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a75675a",
      "metadata": {
        "id": "5a75675a"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las emociones:\n",
        "emotions = list(vocab.keys())\n",
        "# Obtenemos el número de palabras:\n",
        "counts = [len(vocab[emotion]) for emotion in emotions]\n",
        "# Diagrama de barras:\n",
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "ax.bar(emotions, counts)\n",
        "ax.set_xlabel(\"Emoción\")\n",
        "ax.set_ylabel(\"Número de palabras\")\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_rotation(90)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7611afaf",
      "metadata": {
        "id": "7611afaf"
      },
      "source": [
        "De igual forma, podemos ver algunas palabras asociadas a una emoción en específico:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e4cfbfd",
      "metadata": {
        "id": "0e4cfbfd"
      },
      "outputs": [],
      "source": [
        "emotion = \"joy\"\n",
        "display(vocab[emotion][:100])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "67c5b02b",
      "metadata": {
        "id": "67c5b02b"
      },
      "source": [
        "Usando el léxico podemos conocer las emociones predominantes de un texto al mirar las coincidencias entre palabras. Veamos un ejemplo con el siguiente texto:\n",
        "> La depresión es un trastorno del estado de ánimo que causa una persistente sensación de tristeza y pérdida de interés en actividades que solían ser disfrutables. La depresión puede afectar a la forma en que una persona se siente, piensa y se comporta, y puede interferir en su capacidad para llevar una vida normal. Los síntomas de la depresión pueden incluir cambios en el apetito, el sueño, la energía y la capacidad de concentración, así como sentimientos de desesperanza, irritabilidad y culpa. Si estás sufriendo de depresión, es importante que busques ayuda profesional."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a42ec6d",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "1a42ec6d"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"La depresión es un trastorno del estado de ánimo que causa una persistente sensación de tristeza y pérdida de interés en actividades que solían ser disfrutables. La depresión puede afectar a la forma en que una persona se siente, piensa y se comporta, y puede interferir en su capacidad para llevar una vida normal. Los síntomas de la depresión pueden incluir cambios en el apetito, el sueño, la energía y la capacidad de concentración, así como sentimientos de desesperanza, irritabilidad y culpa. Si estás sufriendo de depresión, es importante que busques ayuda profesional.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fa499db",
      "metadata": {
        "id": "0fa499db"
      },
      "source": [
        "Creamos una función para contar coincidencias por emoción:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "76df973f",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "76df973f"
      },
      "outputs": [],
      "source": [
        "def emotion_count(text, vocab):\n",
        "    # Separamos las palabras por espacios.\n",
        "    words = text.split(\" \")\n",
        "    # Creamos un diccionario donde se guardarán los conteos por cada emoción.\n",
        "    counts = {\n",
        "            emotion: sum(word in vocab[emotion] for word in words)\n",
        "            for emotion in vocab\n",
        "            }\n",
        "    return counts"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "10ccfca8",
      "metadata": {
        "id": "10ccfca8"
      },
      "source": [
        "Aplicamos la función sobre el texto preprocesado para obtener la distribución de conceptos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ab0fcc",
      "metadata": {
        "id": "24ab0fcc"
      },
      "outputs": [],
      "source": [
        "counts = emotion_count(preprocess(text), vocab)\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6cbc07c",
      "metadata": {
        "id": "f6cbc07c"
      },
      "source": [
        "Podemos generar una visualización para mostrar la distribución de emociones (de acuerdo al léxico) del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30942647",
      "metadata": {
        "id": "30942647"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "ax.bar(counts.keys(), counts.values())\n",
        "ax.set_xlabel(\"Emoción\")\n",
        "ax.set_ylabel(\"Número de palabras\")\n",
        "ax.set_title(\"Distribución de emociones en el texto de ejemplo\")\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_rotation(90)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da5b717d",
      "metadata": {
        "id": "da5b717d"
      },
      "source": [
        "Como podemos ver, obtenemos una representación numérica basada en histogramas que es similar a las representaciones BoW. No obstante, en este caso estructuramos la información de acuerdo a conceptos predefinidos. Esto puede tener diversas ventajas al momento de solucionar tareas específicas, como por ejemplo, en manejo de textos clínicos con léxicos médicos, análisis de sentimientos con léxicos de emociones, entre otros.\n",
        "\n",
        "Veamos otro ejemplo con el siguiente texto:\n",
        "\n",
        "> La felicidad es un estado emocional que se caracteriza por sentir satisfacción y bienestar. La felicidad puede ser el resultado de tener relaciones saludables y positivas, lograr metas importantes, experimentar cosas nuevas y emocionantes, o simplemente disfrutar de las cosas simples de la vida. La felicidad es subjetiva y puede significar cosas diferentes para diferentes personas. Algunas personas pueden encontrar la felicidad en el éxito profesional, mientras que para otros puede ser más importante tener una buena salud o disfrutar de actividades al aire libre. Lo importante es encontrar lo que te hace feliz y hacerlo una parte integral de tu vida."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60b4f8a4",
      "metadata": {
        "id": "60b4f8a4"
      },
      "outputs": [],
      "source": [
        "text = \"\"\"La felicidad es un estado emocional que se caracteriza por sentir satisfacción y bienestar. La felicidad puede ser el resultado de tener relaciones saludables y positivas, lograr metas importantes, experimentar cosas nuevas y emocionantes, o simplemente disfrutar de las cosas simples de la vida. La felicidad es subjetiva y puede significar cosas diferentes para diferentes personas. Algunas personas pueden encontrar la felicidad en el éxito profesional, mientras que para otros puede ser más importante tener una buena salud o disfrutar de actividades al aire libre. Lo importante es encontrar lo que te hace feliz y hacerlo una parte integral de tu vida.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "beed7692",
      "metadata": {
        "id": "beed7692"
      },
      "source": [
        "Veamos los conteos de emociones:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f31c566d",
      "metadata": {
        "id": "f31c566d"
      },
      "outputs": [],
      "source": [
        "counts = emotion_count(preprocess(text), vocab)\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caae6919",
      "metadata": {
        "id": "caae6919"
      },
      "source": [
        "Podemos generar una visualización para mostrar la distribución de emociones (de acuerdo al léxico) del texto:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "94443ac9",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "94443ac9"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(15, 10))\n",
        "ax.bar(counts.keys(), counts.values())\n",
        "ax.set_xlabel(\"Emoción\")\n",
        "ax.set_ylabel(\"Número de palabras\")\n",
        "ax.set_title(\"Distribución de emociones en el texto de ejemplo\")\n",
        "for tick in ax.get_xticklabels():\n",
        "    tick.set_rotation(90)\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "664e10d0",
      "metadata": {
        "id": "664e10d0"
      },
      "source": [
        "## **3. Nubes de Palabras**\n",
        "---\n",
        "\n",
        "Una de las herramientas más útiles para el entendimiento de información textual son las nubes de palabras. Se trata de un tipo de visualización donde mostramos la relevancia (puede calcularse como los conteos, _TF-IDF_ u otras) de una palabra tal y como se muestra a continuación:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=16oFlir07J0ULy9jr7nbPnkTtFwd6C_1_\" width=\"80%\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8f5c417f",
      "metadata": {
        "id": "8f5c417f"
      },
      "source": [
        "### **3.1. Corpus y Preprocesamiento**\n",
        "---\n",
        "\n",
        "En este ejemplo usaremos el dataset [Language Detection de Kaggle](https://www.kaggle.com/datasets/basilb2s/language-detection) que usamos en el notebook de introducción a las bolsas de palabras. Comencemos descargándolo y manipulándolo con `pandas` para extraer únicamente los textos en español:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c363ed78",
      "metadata": {
        "id": "c363ed78"
      },
      "outputs": [],
      "source": [
        "data = (\n",
        "        pd.read_csv(\"https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u3/language.csv\")\n",
        "        .query(\"language == 'Spanish'\")\n",
        "        )\n",
        "\n",
        "display(data.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a29d440b",
      "metadata": {
        "id": "a29d440b"
      },
      "source": [
        "Preprocesamos el corpus:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "506cedc4",
      "metadata": {
        "id": "506cedc4"
      },
      "outputs": [],
      "source": [
        "corpus_prep = data.text.apply(preprocess).to_list()\n",
        "display(corpus_prep[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36536049",
      "metadata": {
        "id": "36536049"
      },
      "source": [
        "### **3.2. WordCloud**\n",
        "---\n",
        "\n",
        "Desde _Python_ podemos construir nubes de palabras con la librería `wordcloud`. Veamos cómo instarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57517c03",
      "metadata": {
        "id": "57517c03"
      },
      "outputs": [],
      "source": [
        "!pip install wordcloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "28f4f835",
      "metadata": {
        "id": "28f4f835"
      },
      "source": [
        "Para generar la visualización, podemos importar la clase `WordCloud`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d61a905",
      "metadata": {
        "id": "2d61a905"
      },
      "outputs": [],
      "source": [
        "from wordcloud import WordCloud"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2afda125",
      "metadata": {
        "id": "2afda125"
      },
      "source": [
        "Vamos a generar una nube de palabras a partir del `CountVectorizer` de `sklearn`, para ello usaremos el corpus en español que habíamos filtrado anteriormente.\n",
        "\n",
        "Entrenamos un `CountVectorizer`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "18c9c859",
      "metadata": {
        "id": "18c9c859"
      },
      "outputs": [],
      "source": [
        "vect = (\n",
        "    CountVectorizer(max_features=1000, max_df=0.7)\n",
        "    .fit(corpus_prep)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0217cd43",
      "metadata": {
        "id": "0217cd43"
      },
      "source": [
        "Extraemos la representación de bolsa de palabras y el vocabulario:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e06f08d",
      "metadata": {
        "id": "0e06f08d"
      },
      "outputs": [],
      "source": [
        "X = vect.transform(corpus_prep)\n",
        "vocab = vect.get_feature_names_out()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "817731f2",
      "metadata": {
        "id": "817731f2"
      },
      "source": [
        "Vamos a generar un conteo completo de cada palabra en el corpus (no por documento) al sumar sobre la matriz de bolsa de palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c4ccd7c",
      "metadata": {
        "id": "7c4ccd7c"
      },
      "outputs": [],
      "source": [
        "counts = np.array(X.sum(axis=0)).flatten()\n",
        "display(counts)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b4ee2c14",
      "metadata": {
        "id": "b4ee2c14"
      },
      "source": [
        "Para generar la nube de palabras, debemos crear un diccionario donde las claves sean las palabras y los valores las importancias:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a2fbfd0",
      "metadata": {
        "id": "2a2fbfd0"
      },
      "outputs": [],
      "source": [
        "counts_dict = {word: count for word, count in zip(vocab, counts)}\n",
        "display(counts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "14768f79",
      "metadata": {
        "id": "14768f79"
      },
      "source": [
        "Generamos la nube de palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "001bae22",
      "metadata": {
        "id": "001bae22"
      },
      "outputs": [],
      "source": [
        "wc = (\n",
        "        WordCloud( width=500, height=300)\n",
        "        .generate_from_frequencies(counts_dict)\n",
        "        )\n",
        "display(wc)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90da4249",
      "metadata": {
        "id": "90da4249"
      },
      "source": [
        "Finalmente, la visualizamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "caa89b9f",
      "metadata": {
        "id": "caa89b9f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(wc)\n",
        "ax.axis(\"off\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c8d3531f",
      "metadata": {
        "id": "c8d3531f"
      },
      "source": [
        "Podemos cambiar algunos aspectos en la configuración de la nube de palabras, por ejemplo:\n",
        "\n",
        "- `font_path`: permite cambiar el tipo de letra.\n",
        "- `width`: ancho de la imagen.\n",
        "- `height`: alto de la imagen.\n",
        "- `prefer_horizontal`: una proporción que define la preferencia de las palabras para aparecer de forma vertical (>1) u horizontal (<1).\n",
        "- `mask`: máscara binaria indicando dónde se deben mostrar las palabras.\n",
        "- `max_words`: número máximo de palabras a mostrar.\n",
        "- `mode`: define el tipo de espacio de color a usar, por ejemplo `\"RGBA\"` permite transparencia en el fondo.\n",
        "- `background_color`: color para usar en el fondo de la imagen.\n",
        "- `colormap`: paleta de colores dentro de las [disponibles](https://matplotlib.org/stable/gallery/color/colormap_reference.html) de `matplotlib`.\n",
        "\n",
        "Veamos un ejemplo un poco más elaborado. Primero descargamos una imagen para usar como máscara:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1d64132",
      "metadata": {
        "id": "d1d64132"
      },
      "outputs": [],
      "source": [
        "!wget 'https://raw.githubusercontent.com/mindlab-unal/mlds4-datasets/main/u3/colombia.jpg' -O 'colombia.jpg'"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cba75974",
      "metadata": {
        "id": "cba75974"
      },
      "source": [
        "Ahora, cargamos la imagen usando la librería `cv2` (librería para procesamiento de imágenes):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ce55b10",
      "metadata": {
        "id": "3ce55b10"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "im = cv2.resize(cv2.imread(\"colombia.jpg\", 0), (1000, 1000))\n",
        "display(im)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ffadecf6",
      "metadata": {
        "id": "ffadecf6"
      },
      "source": [
        "Podemos visualizarla:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7ce7cc00",
      "metadata": {
        "id": "7ce7cc00"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(im, cmap=\"gray\")\n",
        "ax.axis(\"off\")\n",
        "fig.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9133dd69",
      "metadata": {
        "id": "9133dd69"
      },
      "source": [
        "Ahora, generamos la nube de palabras:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "88c45d8a",
      "metadata": {
        "id": "88c45d8a"
      },
      "outputs": [],
      "source": [
        "wc = WordCloud(\n",
        "        mask = im,\n",
        "        colormap = \"Blues\",\n",
        "        background_color = \"#FFFFFF\" # color blanco en hex\n",
        "        ).generate_from_frequencies(counts_dict)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2467b468",
      "metadata": {
        "id": "2467b468"
      },
      "source": [
        "Veamos la nube:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae9c4781",
      "metadata": {
        "id": "ae9c4781"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots()\n",
        "ax.imshow(wc)\n",
        "ax.axis(\"off\")\n",
        "fig.savefig(\"wordcloud.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7cfce008",
      "metadata": {
        "id": "7cfce008"
      },
      "source": [
        "Como pudimos ver, las nubes de palabras son un tipo de visualización bastante entendible que permiten mostrar de mejor forma las representaciones basadas en bolsas de palabras."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1d5c949",
      "metadata": {
        "id": "d1d5c949"
      },
      "source": [
        "## Recursos Adicionales\n",
        "---\n",
        "\n",
        "Los siguientes enlaces corresponden a sitios donde encontrará información muy útil para profundizar en los temas vistos en este notebook:\n",
        "\n",
        "- [Understanding Automatic Text Summarization](https://towardsdatascience.com/understanding-automatic-text-summarization-1-extractive-methods-8eb512b21ecc).\n",
        "- [Generating word cloud in Python](https://www.datacamp.com/tutorial/wordcloud-python)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "efa97acb",
      "metadata": {
        "id": "efa97acb"
      },
      "source": [
        "## Créditos\n",
        "---\n",
        "\n",
        "* **Profesor:** [Felipe Restrepo Calle](https://dis.unal.edu.co/~ferestrepoca/)\n",
        "* **Asistentes docentes:**\n",
        "    - [Juan Sebastián Lara Ramírez](https://www.linkedin.com/in/juan-sebastian-lara-ramirez-43570a214/).\n",
        "* **Diseño de imágenes:**\n",
        "    - [Rosa Alejandra Superlano Esquibel](mailto:rsuperlano@unal.edu.co).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "title,-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}