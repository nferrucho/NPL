{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo5/M5U5_Automatic_Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1Hh_G3M13P9xSNgSiQ-WnALg93XwK_hG8\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "metadata": {
        "id": "KO4GsoJ6_Ap4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C354DchoTOkm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **_Automatic Image Captioning_** (Generación Automática de Descripciones de Imágenes)\n",
        "---\n",
        "\n",
        "¿Cómo describiría lo que ve en la siguiente imagen?\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1GN121S6xLJLKyLAbvvKubBLHN9sg7VR4\" alt = \"Ejemplo de Image Captioning con una imagen de un perro \" width=\"70%\" /></center>\n",
        "\n",
        "Algunas personas podrían decir \"Un perro jugando en la nieve\", u otros podrían decir \"Perro blanco con manchas marrones\" y algunos otros podrían decir \"Un perro en un campo abierto\".\n",
        "\n",
        "Sin duda todas estas descripciones son pertinentes para esta imagen y puede que también haya otras. Como seres humanos, nos resulta muy fácil echar un vistazo a una imagen y describirla con un lenguaje apropiado.\n",
        "\n",
        "- ¿Cómo entrenamos un modelo para que haga esta misma tarea?\n"
      ],
      "metadata": {
        "id": "DAmqP9IDUvhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Introducción**\n",
        "----\n",
        "El \"Automatic Image Captioning\" (o Generación Automática de Descripciones de Imágenes) es un problema de aprendizaje profundo que implica **la generación automática de descripciones textuales para imágenes**. El objetivo es que un modelo pueda generar una descripción textual que resuma el contenido de una imagen en lenguaje natural, de manera similar a como lo haría un humano.\n",
        "\n",
        "El proceso de generación automática de descripciones de imágenes se puede dividir en dos partes:\n",
        "\n",
        "- La extracción de características de la imagen\n",
        "- La generación de texto.\n",
        "\n",
        "En la primera parte, se utiliza una red neuronal convolucional (CNN) para extraer características visuales de la imagen, mientras que, en la segunda parte, se utiliza un generador de texto, como una red neuronal recurrente (RNN) o un _Transformer_ para generar la descripción textual a partir de estas características.\n",
        "\n",
        "La generación automática de descripciones de imágenes plantea varios desafíos.\n",
        "\n",
        "- **En primer lugar**, la descripción generada debe ser precisa y coherente con la imagen.\n",
        "- **En segundo lugar**, el modelo debe ser capaz de generar descripciones que sean gramaticalmente correctas y estén en un estilo similar al lenguaje natural.\n",
        "\n",
        "En este notebook veremos un ejemplo de cómo modelar este problema. Usaremos el conjunto de datos [Flickr 8k](https://www.kaggle.com/datasets/adityajn105/flickr8k), una colección de referencia para la descripción y búsqueda de imágenes basada en frases, compuesta por 8000 imágenes emparejadas con cinco pies de foto diferentes que proporcionan descripciones claras de las entidades y acontecimientos más destacados. Las imágenes no suelen contener personas o lugares conocidos, sino que se seleccionaron manualmente para representar escenas y situaciones variadas.\n",
        "\n",
        "- Como es usual, empezamos importando los paquetes necesarios:\n"
      ],
      "metadata": {
        "id": "ReD577kwWLTV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from numpy import array\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import string\n",
        "import os\n",
        "from PIL import Image\n",
        "import glob\n",
        "from pickle import dump, load\n",
        "from time import time\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing import sequence, image\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras import Input, layers\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import LSTM, Embedding, TimeDistributed, Dense,\\\n",
        "                                    RepeatVector, Activation, Flatten, Reshape,\\\n",
        "                                    concatenate, Dropout, BatchNormalization\n",
        "from tensorflow.keras.optimizers import Adam, RMSprop\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.utils import plot_model"
      ],
      "metadata": {
        "id": "niIrRtlpWHgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mindlab-unal/mlds5-dataset-unit5-AutomaticImageCaptioning.git"
      ],
      "metadata": {
        "id": "yMKGGYsEV53f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Descargamos las imágenes y las descripciones. Estas están almacenadas en un archivo de texto plano: `captions.txt`."
      ],
      "metadata": {
        "id": "1WRVbdxWXxBQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf3Jwd2wUOFi"
      },
      "outputs": [],
      "source": [
        "# Definimos una función para abrir un archivo de texto:\n",
        "def load_doc(filename):\n",
        "\t# open the file as read only\n",
        "\tfile = open(filename, 'r')\n",
        "\t# read all text\n",
        "\ttext = file.read()\n",
        "\t# close the file\n",
        "\tfile.close()\n",
        "\treturn text\n",
        "\n",
        "filename = \"/content/mlds5-dataset-unit5-AutomaticImageCaptioning/captions.txt\"\n",
        "\n",
        "# Cargamos las descripciones de las imágenes\n",
        "doc = load_doc(filename)\n",
        "\n",
        "# Imprimimos los primeros 500 caracteres\n",
        "print(doc[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1 Preprocesamiento**\n",
        "----\n",
        "Como vemos, para una misma imagen tenemos cinco descripciones cortas. Vamos a procesar esta información. Creamos un diccionario llamado `descriptions` que contiene el nombre de la imagen (sin la extensión `.jpg`) como llaves, y una lista de los cinco pies de foto de la imagen correspondiente como valores."
      ],
      "metadata": {
        "id": "UWiTa-M1Ybua"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avUaAjRBUOFj"
      },
      "outputs": [],
      "source": [
        "def load_descriptions(doc):\n",
        "\tmapping = dict()\n",
        "\t# Lineas a procesar\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# Dividir la linea según los espacios en blanco\n",
        "\t\ttokens = line.split()\n",
        "\t\tif len(line) < 2:\n",
        "\t\t\tcontinue\n",
        "\t\t# Tomar el primer token como el id de la imagen, el resto como la descripción\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# Extraer el nombre del archivo del id de la imagen\n",
        "\t\timage_id = image_id.split('.')[0]\n",
        "\t\t# Volver a convertir los tokens de la descripción en strings\n",
        "\t\timage_desc = ' '.join(image_desc)\n",
        "\t\t# Crear la lista si es necesario\n",
        "\t\tif image_id not in mapping:\n",
        "\t\t\tmapping[image_id] = list()\n",
        "\t\t# Almacenar la descripción\n",
        "\t\tmapping[image_id].append(image_desc)\n",
        "\treturn mapping\n",
        "\n",
        "# Analizar las descripciones\n",
        "descriptions = load_descriptions(doc)\n",
        "print('Loaded: %d ' % len(descriptions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tenemos en total 8091 muestras para ser exactos. Veamos las descripciones de la imagen `1000268201_693b08cb0e.jpg`:"
      ],
      "metadata": {
        "id": "8s3Xw0rzZvFw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7_g27PdUOFk"
      },
      "outputs": [],
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La imagen que describe es la siguiente:"
      ],
      "metadata": {
        "id": "BZw8RwNpHMx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x=plt.imread('/content/mlds5-dataset-unit5-AutomaticImageCaptioning/Images/1000268201_693b08cb0e.jpg')\n",
        "plt.imshow(x)"
      ],
      "metadata": {
        "id": "iDOTSHbFHShS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Vamos a usar un _Word Embedding_ para representar los textos y hacer el proceso final de generación. Para esto, vamos primero a normalizar los textos. Creamos una función para esto:"
      ],
      "metadata": {
        "id": "Tk-5jLh5aWyl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yfvHHSWtUOFl"
      },
      "outputs": [],
      "source": [
        "def clean_descriptions(descriptions):\n",
        "# Preparar la tabla de traducción para eliminar la puntuación\n",
        "\ttable = str.maketrans('', '', string.punctuation)\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor i in range(len(desc_list)):\n",
        "\t\t\tdesc = desc_list[i]\n",
        "\t\t\t# Tokenizar\n",
        "\t\t\tdesc = desc.split()\n",
        "\t\t\t# Convertir a minúsculas\n",
        "\t\t\tdesc = [word.lower() for word in desc]\n",
        "\t\t\t# Eliminar los signos de puntuación de cada palabra\n",
        "\t\t\tdesc = [w.translate(table) for w in desc]\n",
        "\t\t\t# Eliminar las 's' y 'a' colgantes\n",
        "\t\t\tdesc = [word for word in desc if len(word)>1]\n",
        "\t\t\t# Elimina los tokens con números\n",
        "\t\t\tdesc = [word for word in desc if word.isalpha()]\n",
        "\t\t\t# Almacenar como cadena\n",
        "\t\t\tdesc_list[i] =  ' '.join(desc)\n",
        "\n",
        "# Limpiar descripciones\n",
        "clean_descriptions(descriptions)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limpiamos y veamos de nuevo las descripciones de la imagen `1000268201_693b08cb0e.jpg`:"
      ],
      "metadata": {
        "id": "_WBceNlTagMw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkMTm237UOFm"
      },
      "outputs": [],
      "source": [
        "descriptions['1000268201_693b08cb0e']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Creamos un vocabulario de todas las palabras únicas presentes en todos los ~8000*5 (es decir, ~40000) pies de imagen (corpus) del conjunto de datos:"
      ],
      "metadata": {
        "id": "cZaWDWtJbIrT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvVh-_7HUOFn"
      },
      "outputs": [],
      "source": [
        "# Convertir las descripciones cargadas en un vocabulario de palabras\n",
        "def to_vocabulary(descriptions):\n",
        "\t# Construye una lista de todas las cadenas de descripciones\n",
        "\tall_desc = set()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.update(d.split()) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# Resumir vocabulario\n",
        "vocabulary = to_vocabulary(descriptions)\n",
        "print('Original Vocabulary Size: %d' % len(vocabulary))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Esto significa que tenemos 8680 palabras únicas en los 40000 pies de foto. Escribimos todos estos pies de foto junto con los nombres de las imágenes en un nuevo archivo llamado `'descriptions.txt'` y lo guardamos. Usamos de nuevo una función:"
      ],
      "metadata": {
        "id": "rNbvMwJgbR2i"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-D9WnqaWUOFo"
      },
      "outputs": [],
      "source": [
        "def save_descriptions(descriptions, filename):\n",
        "\tlines = list() # Lista vacia\n",
        "\tfor key, desc_list in descriptions.items():\n",
        "\t\tfor desc in desc_list:\n",
        "\t\t\tlines.append(key + ' ' + desc) # Se concatena el id de la imagen con la\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t # descripción\n",
        "\tdata = '\\n'.join(lines)\n",
        "\tfile = open(filename, 'w')\n",
        "\tfile.write(data)\n",
        "\tfile.close()\n",
        "\n",
        "save_descriptions(descriptions, 'descriptions.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Particiones de entrenamiento y prueba**\n",
        "\n",
        "Vamos a seleccionar las primeras 6000 imágenes para hacer entrenamiento; el resto será para prueba:"
      ],
      "metadata": {
        "id": "erguvV1gb9Vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_names = list(descriptions.keys())[:6000]\n",
        "\n",
        "with open(r'Flickr_8k.trainImages.txt', 'w') as fp:\n",
        "    fp.write('\\n'.join(train_names))"
      ],
      "metadata": {
        "id": "UNElrIGFoBqe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos guardado los identificadores de las imágenes de entrenamiento en un archivo nuevo: `'Flickr_8k.trainImages.txt'`. Esto nos va a ser útil posteriormente durante el entrenamiento. Análogamente, guardamos los identificadores de las imágenes de prueba en el archivo `'Flickr_8k.testImages.txt'`:"
      ],
      "metadata": {
        "id": "_0OAdHQEcP-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_names = list(descriptions.keys())[6000:]\n",
        "\n",
        "with open(r'Flickr_8k.testImages.txt', 'w') as fp:\n",
        "    fp.write('\\n'.join(test_names))"
      ],
      "metadata": {
        "id": "oY0o8OTYoyMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora creamos una lista con la ruta de las imágenes de entrenamiento"
      ],
      "metadata": {
        "id": "pkYnapuvdNoM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oY6y0aKCUOFp"
      },
      "outputs": [],
      "source": [
        "# La siguiente ruta contiene todas las imágenes\n",
        "images = '/content/mlds5-dataset-unit5-AutomaticImageCaptioning/Images/'\n",
        "# Crea una lista con todos los nombres de imágenes del directorio\n",
        "img = glob.glob(images + '*.jpg')\n",
        "\n",
        "# El siguiente archivo contiene los nombres de las imágenes que se utilizarán en\n",
        "# los datos de entrenamiento\n",
        "\n",
        "train_images_file = '/content/mlds5-dataset-unit5-AutomaticImageCaptioning/Flickr_8k.trainImages.txt'\n",
        "\n",
        "# Leer de los nombres de las imágenes de entrenamiento en un conjunto\n",
        "train_images = set(open(train_images_file, 'r').read().strip().split('\\n'))\n",
        "\n",
        "# Crear una lista de todas las imágenes de entrenamiento con sus nombres de ruta\n",
        "# completos\n",
        "train_img = []\n",
        "\n",
        "for i in img: # img es la lista de los nombres completos de todas las imágenes\n",
        "    if i[len(images):-4] in train_images: # Comprueba si la imagen pertenece al conjunto de entrenamiento\n",
        "        train_img.append(i) # Añadir a la lista de imágenes de entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y hacemos lo mismo para la partición de prueba:"
      ],
      "metadata": {
        "id": "tr01LrC2dwui"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wr0DiY_0UOFp"
      },
      "outputs": [],
      "source": [
        "# El siguiente archivo contiene los nombres de las imágenes que se utilizarán en\n",
        "# los datos de prueba\n",
        "test_images_file = 'Flickr_8k.testImages.txt'\n",
        "test_images = set(open(test_images_file, 'r').read().strip().split('\\n'))\n",
        "test_img = []\n",
        "for i in img:\n",
        "    if i[len(images):-4] in test_images:\n",
        "        test_img.append(i)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, para cada muestra juntamos las cinco descripciones en un solo texto:"
      ],
      "metadata": {
        "id": "l9Er9QU4ekIe"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gpGZ7MbUOFq"
      },
      "outputs": [],
      "source": [
        "# Cargar descripciones limpias en memoria\n",
        "def load_clean_descriptions(filename, dataset):\n",
        "\t# Cargar documento\n",
        "\tdoc = load_doc(filename)\n",
        "\tdescriptions = dict()\n",
        "\tfor line in doc.split('\\n'):\n",
        "\t\t# Dividir la línea por espacios en blanco\n",
        "\t\ttokens = line.split()\n",
        "\t\t# Separar id de descripción\n",
        "\t\timage_id, image_desc = tokens[0], tokens[1:]\n",
        "\t\t# Omitir las imágenes que no están en el conjunto\n",
        "\t\tif image_id in dataset:\n",
        "\t\t\t# Crear lista\n",
        "\t\t\tif image_id not in descriptions:\n",
        "\t\t\t\tdescriptions[image_id] = list()\n",
        "\t\t\t# Guardar la descripción en tokens de inicio y fin\n",
        "\t\t\tdesc = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
        "\t\t\t# Almacenar\n",
        "\t\t\tdescriptions[image_id].append(desc)\n",
        "\treturn descriptions\n",
        "\n",
        "# Guardamos los id's en un set\n",
        "train = set(list(descriptions.keys())[:6000])\n",
        "len(train)\n",
        "# Descripciones\n",
        "train_descriptions = load_clean_descriptions('descriptions.txt', train)\n",
        "print('Descriptions: train=%d' % len(train_descriptions))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> Note que hemos incluido dos tokens: `'startseq'` y `' endseq'`, que encapsulan las descripciones. Esto va a ser útil y necesario en el proceso de generación de texto, en el que vamos a usar un _Word Embedding_."
      ],
      "metadata": {
        "id": "7qNgvjB-fRZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Simplificando el vocabulario**\n",
        "\n",
        "Si lo pensamos bien, muchas de las palabras del vocabulario aparecerán muy pocas veces, digamos 1, 2 o 3 veces. Dado que estamos creando un modelo predictivo, no nos gustaría tener todas las palabras presentes en nuestro vocabulario, sino las palabras que tienen más probabilidades de aparecer o que son comunes. Esto ayuda al modelo a ser más robusto frente a los valores atípicos y a cometer menos errores.\n",
        "\n",
        "- Por lo tanto, sólo tenemos en cuenta las palabras que aparecen al menos 10 veces en todo el corpus. A continuación se muestra el código correspondiente:"
      ],
      "metadata": {
        "id": "15dMXwiyWWw9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear una lista de todas las descripciones\n",
        "all_train_captions = []\n",
        "for key, val in train_descriptions.items():\n",
        "    for cap in val:\n",
        "        all_train_captions.append(cap)\n",
        "# Considere sólo las palabras que aparecen al menos 10 veces en el corpus\n",
        "word_count_threshold = 10\n",
        "word_counts = {}\n",
        "nsents = 0\n",
        "for sent in all_train_captions:\n",
        "    nsents += 1\n",
        "    for w in sent.split(' '):\n",
        "        word_counts[w] = word_counts.get(w, 0) + 1\n",
        "\n",
        "vocab = [w for w in word_counts if word_counts[w] >= word_count_threshold]\n",
        "print('Preprocessed words %d -> %d' % (len(word_counts), len(vocab)))"
      ],
      "metadata": {
        "id": "p-GPV96as-CL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nos quedaron entonces 1652 palabras. Algo mucho más manejable que 7603."
      ],
      "metadata": {
        "id": "B32FlqTWukhQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Índices de palabras**\n",
        "\n",
        "Ahora, debemos tener en cuenta que las descripciones de texto son algo que queremos predecir. Entonces, durante el período de entrenamiento, los _captions_ serán las variables objetivo ($y$) que el modelo está aprendiendo a predecir. Pero la predicción de todo el pie de foto, dada la imagen, no sucede de golpe. Predeciremos el _caption_ palabra por palabra. Para esto, empezaremos por crear dos diccionarios de Python, a saber, `wordtoix` (palabra a índice) e `ixtoword` (índice a palabra).\n",
        "\n",
        "En pocas palabras, representaremos cada palabra única en el vocabulario por un número entero (índice).\n",
        "\n",
        "- Como se vio anteriormente, tenemos 1652 palabras únicas en el corpus y, por lo tanto, cada palabra estará representada por un índice entero entre 1 y 1652.\n",
        "\n",
        "Estos dos diccionarios de Python se pueden usar de la siguiente manera:\n",
        "\n",
        "- `wordtoix['abc']` -> devuelve el índice de la palabra `'abc'`.\n",
        "\n",
        "- `ixtopalabra[k]` -> devuelve la palabra cuyo índice es `k`\n",
        "\n",
        "El código utilizado es el siguiente:\n"
      ],
      "metadata": {
        "id": "ilj2cYGllsBv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ixtoword = {}\n",
        "wordtoix = {}\n",
        "\n",
        "ix = 1\n",
        "for w in vocab:\n",
        "    wordtoix[w] = ix\n",
        "    ixtoword[ix] = w\n",
        "    ix += 1"
      ],
      "metadata": {
        "id": "NWZusTqvtByY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(ixtoword) + 1 # uno para los 0 añadidos\n",
        "vocab_size"
      ],
      "metadata": {
        "id": "g8pexRVktE3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hemos agregado un índice `0` que explicaremos más adelante. Mientras tanto, hay un parámetro más que necesitamos calcular: la longitud máxima de las descripciones. Lo hacemos de la siguiente manera:"
      ],
      "metadata": {
        "id": "wC8LFe73nlqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir un diccionario de descripciones limpias en una lista de descripciones\n",
        "def to_lines(descriptions):\n",
        "\tall_desc = list()\n",
        "\tfor key in descriptions.keys():\n",
        "\t\t[all_desc.append(d) for d in descriptions[key]]\n",
        "\treturn all_desc\n",
        "\n",
        "# Calcula la longitud de la descripción con más palabras\n",
        "def max_length(descriptions):\n",
        "\tlines = to_lines(descriptions)\n",
        "\treturn max(len(d.split()) for d in lines)\n",
        "\n",
        "# Determinar la longitud máxima de la secuencia\n",
        "max_length = max_length(train_descriptions)\n",
        "print('Description Length: %d' % max_length)"
      ],
      "metadata": {
        "id": "YNIYN3YCtJbf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Es decir, la descripción más larga del conjunto de entrenamiento es de 33 palabras."
      ],
      "metadata": {
        "id": "gCFVXC8jns3O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.2 Procesamiento de imágenes**\n",
        "----\n",
        "Vamos a usar una CNN para extraer representaciones de las imágenes. Vamos a tomar Inception V3, que recibe entradas de tamaño $299\\times299$. Vamos entonces a definir una función para poder preproprocesar las imágenes una por una. Esto va a ser útil y necesario para el entrenamiento, pero en particular para poder ver resultados de imágenes individuales al final del proceso.\n"
      ],
      "metadata": {
        "id": "pVzqmUqOfxDu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvnGR1csUOFq"
      },
      "outputs": [],
      "source": [
        "def preprocess(image_path):\n",
        "    # Convierte todas las imágenes a tamaño 299x299 como espera el modelo InceptionV3\n",
        "    img = image.load_img(image_path, target_size=(299, 299))\n",
        "    # Convertir imagen PIL a matriz numpy de 3 dimensiones\n",
        "    x = image.img_to_array(img)\n",
        "    # Añadir una dimensión más\n",
        "    x = np.expand_dims(x, axis=0)\n",
        "    # Preprocesar las imágenes usando preprocess_input() del módulo inception\n",
        "    x = preprocess_input(x)\n",
        "    return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cargamos Inception V3 con los pesos de `imagenet`"
      ],
      "metadata": {
        "id": "IijhR6_mhA3j"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SZxHJV-2UOFq"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.applications.InceptionV3(weights='imagenet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "dv_xNPkVhUiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y como nos interesa la representación latente, vamos a tomar la salida de la capa `GlobalAveragePooling2D`, definiendo para esto un modelo nuevo:"
      ],
      "metadata": {
        "id": "_ifTeyqdhEQ8"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XFcR8UCaUOFr"
      },
      "outputs": [],
      "source": [
        "model_new = tf.keras.models.Model(model.input, model.layers[-2].output)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Entonces podemos definir una función que recibe una imagen y retorna el vector de características dado por InceptionV3, que tiene dimensión 2048:"
      ],
      "metadata": {
        "id": "a7sp4YSWhnkX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-LNFJPKfUOFr"
      },
      "outputs": [],
      "source": [
        "# Función para codificar una imagen dada en un vector de tamaño (2048, )\n",
        "def encode(image):\n",
        "    image = preprocess(image) # Preprocesar la imagen\n",
        "    fea_vec = model_new.predict(image) # Obtener el vector de características de la imagen\n",
        "    fea_vec = np.reshape(fea_vec, fea_vec.shape[1]) # Cambiar la forma de (1, 2048) a (2048, )\n",
        "    return fea_vec"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A continuación, calculamos la representación de todas las imágenes, las de entrenamiento y las de prueba. Este procedimiento puede tomar tiempo, por tanto, puede ejecutarlo, o aprovechar que las representaciones de las imágenes están guardadas en los archivos `encoded_train_images.pkl` y `encoded_test_images.pkl`."
      ],
      "metadata": {
        "id": "_3VtEtXyh_2q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OQ91pe9HUOFs"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "# Imágenes de entrenamiento\n",
        "start = time()\n",
        "encoding_train = {}\n",
        "for img in train_img:\n",
        "    encoding_train[img[:-4]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)\n",
        "# Guardamos las representaciones\n",
        "import pickle\n",
        "with open(\"encoded_train_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_train, encoded_pickle)\n",
        "# Imágenes de prueba\n",
        "start = time()\n",
        "encoding_test = {}\n",
        "for img in test_img:\n",
        "    encoding_test[img[:-4]] = encode(img)\n",
        "print(\"Time taken in seconds =\", time()-start)\n",
        "# Guardamos las representaciones\n",
        "with open(\"encoded_test_images.pkl\", \"wb\") as encoded_pickle:\n",
        "    pickle.dump(encoding_test, encoded_pickle)\n",
        "\"\"\"\n",
        "\n",
        "train_features = load(open(\"/content/mlds5-dataset-unit5-AutomaticImageCaptioning/encoded_train_images.pkl\", \"rb\"))\n",
        "print('Photos: train=%d' % len(train_features))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Generador de datos**\n",
        "----\n",
        "\n",
        "Este es uno de los pasos más importantes de este caso práctico. Aquí entenderemos cómo preparar los datos de forma que sea conveniente darlos como entrada al modelo. Consideremos que tenemos 3 imágenes y sus 3 leyendas correspondientes de la siguiente manera:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1qOqi5DUOPppUV_X_vdflTQs4xrrlfqBx\" alt = \"Matriz de resultados a partir de los indices creados para cada palabra  \" width=\"60%\" /></center>\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "T2mQwjxQjxwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, digamos que usamos las dos primeras imágenes y sus leyendas para entrenar el modelo y la tercera imagen para probar nuestro modelo. Las preguntas a las que hay que responder son: ¿Cómo planteamos esto como un problema de aprendizaje supervisado?, ¿Cómo es la matriz de datos?, ¿Cuántos puntos de datos tenemos?, etc.\n",
        "\n",
        "- En **primer lugar**, tenemos que convertir las dos imágenes en sus correspondientes vectores de características de 2048 de longitud, tal y como se ha explicado anteriormente. Sean `Imagen_1` e `Imagen_2` los vectores de características de las dos primeras imágenes, respectivamente.\n",
        "\n",
        "- En **segundo lugar**, vamos a construir el vocabulario para los dos primeros pies de foto (de entrenamiento) añadiendo los dos tokens `startseq` y `endseq` en ambos (supongamos que ya hemos realizado los pasos básicos de limpieza):\n",
        "\n",
        "* Pie de foto 1:\n",
        "```\n",
        "startseq the black cat sat on grass endseq\n",
        "```\n",
        "\n",
        "* Pie de foto 2:\n",
        "```\n",
        "startseq the white cat is walking on road endseq\n",
        "```\n",
        "\n",
        "El vocabulario sería:\n",
        "```\n",
        "vocab =  {black, cat, endseq, grass, is, on, road, sat, startseq, the, walking, white}\n",
        "```\n",
        "Demos un índice a cada palabra del vocabulario:\n",
        "```\n",
        "black: 1, cat: 2, endseq: 3, grass: 4, is: 5, on: 6, road: 7, sat: 8, startseq: 9, the: 10, walking: 11, white: 12\n",
        "```\n",
        "\n",
        "Intentemos ahora plantearlo como un problema de aprendizaje supervisado en el que tenemos un conjunto de puntos de datos $D = \\{X_i, y_i\\}$, donde $X_i$ es el vector de características del punto de datos $i$ y $y_i$ es la variable objetivo correspondiente. Tomemos el primer vector de imágenes `Image_1` y su correspondiente leyenda `startseq the black cat sat on grass endseq`. Recordemos que el vector imagen es la entrada y el título es lo que tenemos que predecir. Pero la forma de predecir el título es la siguiente:\n",
        "\n",
        "La primera vez, proporcionamos el vector imagen y la primera palabra como entrada e intentamos predecir la segunda palabra, es decir\n",
        "\n",
        "* Entrada = `Imagen_1` + `'startseq'`; Salida = `'the'`\n",
        "\n",
        "A continuación, proporcionamos el vector imagen y las dos primeras palabras como entrada e intentamos predecir la tercera palabra, es decir\n",
        "\n",
        "* Entrada = `Imagen_1` + `'startseq the'`; Salida = `'cat'`\n",
        "\n",
        "Y así sucesivamente... Así, podemos resumir la matriz de datos de una imagen y su correspondiente pie de foto de la siguiente manera:\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1i-HVKejF8Lyp3TZciCOPe8bvZZGeMuZo\" alt = \"Matriz de resultados a partir del pie de foto 1 \" width=\"100%\" /></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "654DcIVOlwws"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora debemos entender que, en cada punto de datos, no es solo la imagen la que ingresa al sistema, sino también una leyenda parcial que ayuda a predecir la siguiente palabra en la secuencia.\n",
        "\n",
        "Dado que estamos procesando secuencias, emplearemos una red neuronal recurrente para leer estos subtítulos parciales. Sin embargo, no vamos a pasar el texto real en inglés al modelo, sino que vamos a pasar la secuencia de índices donde cada índice representa una palabra única.\n",
        "\n",
        "- Como ya hemos creado un índice para cada palabra, ahora reemplacemos las palabras con sus índices y comprendamos cómo se verá la matriz de datos:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qZGqirwWouUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1IZSgSo9rFi4Yg566bkq38afdG6Al8bku\" alt = \"Matriz de resultados a partir de los indices creados para cada palabra  \" width=\"100%\" /></center>\n",
        "\n"
      ],
      "metadata": {
        "id": "ZsPlaBw5p9pd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dado que estaríamos realizando un procesamiento por _batches_, debemos asegurarnos de que cada secuencia tenga la misma longitud. Por lo tanto, debemos agregar `0` (relleno con cero, ¿recuerda el índice extra?) al final de cada secuencia. Pero ¿cuántos ceros debemos agregar en cada secuencia?\n",
        "\n",
        "Bueno, esta es la razón por la que calculamos la longitud máxima de un pie de foto, que es 33. Entonces agregaremos esa cantidad de ceros que llevará a que cada secuencia tenga una longitud de 33.\n",
        "\n",
        "- La matriz de datos entonces se verá de la siguiente manera:\n"
      ],
      "metadata": {
        "id": "SVSQ0XUfpGXj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1za4j08mcNmaxpPsb6n7VsklGj2hEfIMZ\" alt = \"Matriz de resultados a partir de los indices creados para la longitud máxima de un pie de foto  \" width=\"100%\" /></center>\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eNiGhwOBqNSw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Todo este proceso esta entonces a cargo del siguiente generador de datos. Esto lo usaremos para alimentar el modelo:"
      ],
      "metadata": {
        "id": "r8BGrQNBpq_g"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4uK2LyvUOFw"
      },
      "outputs": [],
      "source": [
        "# Generador de datos, destinado a ser utilizado en una llamada a model.fit_generator()\n",
        "def data_generator(descriptions, photos, wordtoix, max_length, num_photos_per_batch):\n",
        "    X1, X2, y = list(), list(), list()\n",
        "    n=0\n",
        "    # Bucle para siempre sobre las imágenes\n",
        "    while True:\n",
        "        for key, desc_list in descriptions.items():\n",
        "            n+=1\n",
        "            # Recuperar la foto\n",
        "            photo = photos['/content/mlds5-dataset-unit5-AutomaticImageCaptioning/Images/'+key]\n",
        "            for desc in desc_list:\n",
        "                # Codificar la secuencia\n",
        "                seq = [wordtoix[word] for word in desc.split(' ') if word in wordtoix]\n",
        "                # Dividir una secuencia en varios pares X, y\n",
        "                for i in range(1, len(seq)):\n",
        "                    # Dividir en pares de entrada y salida\n",
        "                    in_seq, out_seq = seq[:i], seq[i]\n",
        "                    # Rellenar secuencia de entrada\n",
        "                    in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
        "                    # Codificar la secuencia de salida\n",
        "                    out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "                    # Almacenar\n",
        "                    X1.append(photo)\n",
        "                    X2.append(in_seq)\n",
        "                    y.append(out_seq)\n",
        "            # Producir los datos del lote\n",
        "            if n==num_photos_per_batch:\n",
        "                yield [[array(X1), array(X2)], array(y)]\n",
        "                X1, X2, y = list(), list(), list()\n",
        "                n=0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.4 _Word Embedding_**\n",
        "---\n",
        "Para representar el texto, vamos a utilizar [**Glove**](https://nlp.stanford.edu/projects/glove/), un método de embedding que permite capturar y representar las características esenciales de las palabras y su contexto.\n",
        "- Glove, que es un acrónimo de \"Global Vectors for Word Representation\", es un modelo de lenguaje diseñado para transformar el texto en vectores de alta dimensión, logrando una representación más rica y precisa.\n",
        "\n",
        "Este enfoque de embedding considera las palabras como objetos globales en un espacio vectorial, donde las relaciones semánticas y gramaticales entre ellas se reflejan en la cercanía y orientación de los vectores correspondientes. En Glove, se presta especial atención a la incorporación de información contextual y la captura de relaciones semánticas a nivel global. Esto permite que el modelo identifique patrones y estructuras en el texto de manera más efectiva que otros enfoques de embedding, lo que resulta en una mejor comprensión del contenido y una mayor capacidad para abordar tareas complejas relacionadas con el lenguaje.\n",
        "\n",
        "- Para utilizar Glove en Keras, primero debemos obtener los embeddings pre-entrenados de Glove y luego cargarlos en una capa de Embedding de Keras. Vamos a usar una representación de 200 dimensiones."
      ],
      "metadata": {
        "id": "Deq03yHVnpZw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ],
      "metadata": {
        "id": "w_6oyeBUGKBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Una capa de Embedding en Keras es una capa especializada que transforma representaciones de palabras basadas en enteros (índices) en representaciones de vectores densos (embeddings)."
      ],
      "metadata": {
        "id": "L_G9yadKut-r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f_SlZtOlUOFw"
      },
      "outputs": [],
      "source": [
        "# Cargar los vectores de Glove\n",
        "embeddings_index = {} # diccionario vacío\n",
        "f = open('glove.6B.200d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    embeddings_index[word] = coefs\n",
        "f.close()\n",
        "print('Found %s word vectors.' % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "GloVe contiene la representación para 400000 palabras. Vamos a seleccionar solo las que nos interesan según nuestro vocabulario:"
      ],
      "metadata": {
        "id": "wksYOdEnbqdd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hAgytdFeUOFx"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 200\n",
        "# Obtener un vector denso de 200 dim para cada una de las 1653 palabras de nuestro vocabulario\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, i in wordtoix.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Las palabras no encontradas en el índice de incrustación serán todos ceros\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos"
      ],
      "metadata": {
        "id": "JrXui-K9b2CO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvdQ7pPmUOFx"
      },
      "outputs": [],
      "source": [
        "embedding_matrix.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "`embedding_matrix` contiene la representación de todas las palabras de nuestro vocabulario. Esta matriz constituirá el conjunto de pesos de la capa `Embedding` de nuestro modelo generador de descripciones."
      ],
      "metadata": {
        "id": "IbAZdDy_bIi7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **2. Descripción Automática de Imágenes**\n",
        "----\n",
        "\n",
        "Ahora sí, vamos a crear nuestro modelo de _automatic image captioning_.\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Oz3GJVWnAYH7Dl9dcYWEnALh1_amq76S\" alt = \"Matriz de resultados a partir del pie de foto 1 \" width=\"80%\" /></center>\n"
      ],
      "metadata": {
        "id": "ixw-e_omnwjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1 Definición del modelo**\n",
        "----\n",
        "\n",
        "**_Embedding Layer_**\n",
        "\n",
        "Una capa de `Embedding` en Keras es una capa especializada que transforma representaciones de palabras basadas en enteros (índices) en representaciones de vectores densos (_embeddings_). Al crear una capa de `Embedding`, es necesario especificar dos parámetros principales:\n",
        "\n",
        "*        `input_dim`: El tamaño del vocabulario, es decir, el número total de palabras únicas en el conjunto de datos, en nuestro caso 1653.\n",
        "*        `output_dim`: La dimensión del espacio vectorial en el que se representarán las palabras (también conocida como la dimensión del embedding); en nuestro caso, 200.\n",
        "\n",
        "La capa de `Embedding` se incluye en la arquitectura de la red neuronal como una de las primeras capas. Esta recibe los índices de las palabras de la secuencia de texto, y genera una representación global a partir de las representaciones individuales de las palabras. Durante el entrenamiento, la capa de `Embedding` ajusta los pesos de los vectores de las palabras para minimizar el error en la tarea específica (por ejemplo, clasificación de texto o generación de texto).\n",
        "\n",
        "Después del entrenamiento, la capa de `Embedding` puede utilizarse para transformar las palabras en sus representaciones vectoriales correspondientes, que luego se utilizan como entrada para las capas posteriores de la red neuronal. Esto es lo que nosotros haremos.\n",
        "\n",
        "- **No queremos entrenar** la capa `Embedding`, porque ya tenemos la representación que nos ofrece **Glove**. Solo vamos a usar la capa como un mecanismo de codificación del texto.\n",
        "\n",
        "Definimos la arquitectura del modelo:"
      ],
      "metadata": {
        "id": "ITG2nK1ln4VH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZz9oCd5UOFx"
      },
      "outputs": [],
      "source": [
        "inputs1 = Input(shape=(2048,))\n",
        "fe1 = Dropout(0.5)(inputs1)\n",
        "fe2 = Dense(256, activation='relu')(fe1)\n",
        "inputs2 = Input(shape=(max_length,))\n",
        "se1 = Embedding(vocab_size, embedding_dim, mask_zero=True)(inputs2)\n",
        "se2 = Dropout(0.5)(se1)\n",
        "se3 = LSTM(256)(se2)\n",
        "decoder1 = tf.keras.layers.Add()([fe2, se3])\n",
        "decoder2 = Dense(256, activation='relu')(decoder1)\n",
        "outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
        "model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identifiquemos la capa de `Embedding`"
      ],
      "metadata": {
        "id": "sImAPxdJdDpd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XeDGnAIfUOFy"
      },
      "outputs": [],
      "source": [
        "model.layers[2]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como mencionamos anteriormente, `embedding_matrix` contiene la representación de todas las palabras de nuestro vocabulario (según GloVe). Debemos entonces cargar esta matriz de pesos a la capa `Embedding` de nuestro modelo generador de descripciones, y además congelamos la capa, para que no se alteren estas representaciones.  "
      ],
      "metadata": {
        "id": "rYNd8YJqdGdb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jjuDSLNUUOFy"
      },
      "outputs": [],
      "source": [
        "model.layers[2].set_weights([embedding_matrix])\n",
        "model.layers[2].trainable = False"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos el diagrama del modelo:"
      ],
      "metadata": {
        "id": "cCZyUQfoeTxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_model(model, show_shapes=True,)"
      ],
      "metadata": {
        "id": "215tv-ddejHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Entendamos el modelo**\n",
        "\n",
        "El modelo recibe dos entradas. En la gráfica, vemos dos ramas.\n",
        "\n",
        "- La parte izquierda corresponde a la entrada de la secuencia de texto. Es de tamaño 33 porque 33 es la longitud de la secuencia más larga en el conjunto de entrenamiento. El texto se codifica con la capa `Embedding`, pasa por una capa de `dropout` y luego por una capa LSTM (que estudiamos en la Unidad 4 de este módulo).\n",
        "\n",
        "- Por la rama de la derecha entra la imagen, codificada ya como un vector de 2048 dimensiones (gracias a InceptionV3). Esta representación pasa por una capa de `dropout` y luego por una capa densa para obtener una representación de 256 dimensiones.\n",
        "\n",
        "Las representaciones de texto e imagen se suman en la capa `add`, obteniendo una representación conjunta de imagen y texto, que entra entonces a un modelo decodificador de dos capas densas, cuya salida indica la probabilidad de ocurrencia de la siguiente palabra de la secuencia de texto.\n"
      ],
      "metadata": {
        "id": "DHj3yOnuezhX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Compilación y entrenamiento**\n",
        "----"
      ],
      "metadata": {
        "id": "D_-8wSfHn-XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos usando `categorical_crossentropy` y `adam` como optimizador, con la tasa de aprendizaje por defecto (0.001):"
      ],
      "metadata": {
        "id": "06pCFIvyhMN7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNm2seInUOFz"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer='adam')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como primera instancia, vamos a entrenar durante 20 epochs. Luego vamos a bajar la tasa de aprendizaje a 0.0001 para ajustar un poco más el modelo durante 10 epochs. Durante el entrenamiento se guardarán los pesos después de cada epoch.\n",
        "\n",
        "> **Nota**: este entrenamiento puede demorarse más de una hora usando una GPU estándar de Colab. Por practicidad, podemos cargar los pesos obtenidos en un entrenamiento anterior.\n"
      ],
      "metadata": {
        "id": "PRZiUXf9hUie"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-l2gxsj9UOFz"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "epochs = 20\n",
        "number_pics_per_bath = 3\n",
        "steps = len(train_descriptions)//number_pics_per_bath\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "    model.save('./model_weights/model_' + str(i) + '.weights.h5')\n",
        "\n",
        "model.optimizer.lr = 0.0001\n",
        "epochs = 10\n",
        "number_pics_per_bath = 6\n",
        "steps = len(train_descriptions)//number_pics_per_bath\n",
        "\n",
        "for i in range(epochs):\n",
        "    generator = data_generator(train_descriptions, train_features, wordtoix, max_length, number_pics_per_bath)\n",
        "    model.fit(generator, epochs=1, steps_per_epoch=steps, verbose=1)\n",
        "\n",
        "model.save_weights('./model_weights/model_30.weights.h5')\n",
        "\"\"\"\n",
        "# Cargando los pesos de un entrenamiento anterior\n",
        "# En caso de entrenar por tu cuenta usar .weights.h5\n",
        "model.load_weights('/content/mlds5-dataset-unit5-AutomaticImageCaptioning/model_30.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.3 Visualización de resultados**\n",
        "----\n",
        "\n",
        "Durante inferencia, el modelo recibe como entrada la imagen, y el token `startseq`. Este es el punto de partida. En la primera iteración el modelo predice la siguiente palabra de la secuencia, la de mayor probabilidad en el vocabulario de 1653 elementos. Supongamos que la siguiente palabra es: `The`. Entonces en la siguiente iteración el modelo recibe la imagen y la secuencia `startseq The`, y calculará la siguiente palabra. El proceso sigue hasta que en un momento el siguiente token en la secuencia sea `endseq`.\n",
        "\n",
        "- Carguemos las representaciones de las imágenes del conjunto de prueba:\n"
      ],
      "metadata": {
        "id": "xgh0XLNVoN9a"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1aJlbAawUOF0"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/mlds5-dataset-unit5-AutomaticImageCaptioning/encoded_test_images.pkl\", \"rb\") as encoded_pickle:\n",
        "    encoding_test = load(encoded_pickle)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Y definimos la función `captioning`. Esta función recibe la representación de la imagen de prueba, e inicial la generación iterativa de texto con `startseq`:"
      ],
      "metadata": {
        "id": "VlcLt-C0j4qc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rJw9XREvUOF1"
      },
      "outputs": [],
      "source": [
        "def captioning(photo):\n",
        "    in_text = 'startseq'\n",
        "    for i in range(max_length):\n",
        "        sequence = [wordtoix[w] for w in in_text.split() if w in wordtoix]\n",
        "        sequence = pad_sequences([sequence], maxlen=max_length, padding='post')\n",
        "        yhat = model.predict([photo,sequence], verbose=0)\n",
        "        yhat = np.argmax(yhat)\n",
        "        word = ixtoword[yhat]\n",
        "        in_text += ' ' + word\n",
        "        if word == 'endseq':\n",
        "            break\n",
        "    final = in_text.split()\n",
        "    final = final[1:-1]\n",
        "    final = ' '.join(final)\n",
        "    return final"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El siguiente código nos permite visualizar uno a uno los resultados:"
      ],
      "metadata": {
        "id": "2j1gi7L0kXnn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "z = 0"
      ],
      "metadata": {
        "id": "l-b8mADq3x0C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cada vez que ejecute la siguiente celda, la imagen y la descripción generada por nuestro modelo va a cambiar:"
      ],
      "metadata": {
        "id": "pIdHNveFkfen"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFN1M9RXUOF1"
      },
      "outputs": [],
      "source": [
        "z+=1\n",
        "pic = list(encoding_test.keys())[z].split('/')[-1]\n",
        "image = encoding_test[list(encoding_test.keys())[z]].reshape((1,2048))\n",
        "x=plt.imread('/content/mlds5-dataset-unit5-AutomaticImageCaptioning/Images/'+pic+'.jpg')\n",
        "plt.imshow(x)\n",
        "plt.show()\n",
        "print(\"Caption:\",captioning(image))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Hemos terminado, buen trabajo!"
      ],
      "metadata": {
        "id": "7FZ-r1zZkvOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Recursos adicionales**\n",
        "---\n",
        "\n",
        "\n",
        "- [*Image Captioning with Keras*](https://towardsdatascience.com/image-captioning-with-keras-teaching-computers-to-describe-pictures-c88a46a311b8)\n",
        "\n",
        "- [*Flickr 8k Dataset*](https://www.kaggle.com/datasets/adityajn105/flickr8k)\n",
        "\n",
        "* _Origen de los íconos_\n",
        "\n",
        "    - Yadav,A. Vishwakarma,A. Panickar,S. Kuchiwale,S (2020). IRJET: Real Time Video to Text Summarization using Neural Network - CNN + LSTM for Image Captioning [Imagen] https://www.irjet.net/archives/V7/i12/IRJET-V7I12333.pdf\n",
        "    - Limited, A. (s. f.). A black cat with a white front patch sat on the grass. Alamy images. https://www.alamy.com/stock-photo-a-black-cat-with-a-white-front-patch-sat-on-the-grass-36422236.html\n",
        "    - The white cat crosses the road. The concept will be: the white cat crossing the road brings good luck.   foto de Stock. (s. f.). Adobe Stock. https://stock.adobe.com/es/images/the-white-cat-crosses-the-road-the-concept-will-be-the-white-cat-crossing-the-road-brings-good-luck/288570621\n",
        "    - Limited, A. (s. f.-b). Black cat walking on grass in the garden. Alamy images. https://www.alamy.com/black-cat-walking-on-grass-in-the-garden-image63426915.html\n",
        "\n"
      ],
      "metadata": {
        "id": "PnYj93q5oeGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](mailto:mrodrigueztr@unal.edu.co).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "metadata": {
        "id": "w-q-u3EeobwC"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}