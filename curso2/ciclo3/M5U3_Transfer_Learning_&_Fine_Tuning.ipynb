{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo3/M5U3_Transfer_Learning_%26_Fine_Tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeqUajBBMM8n"
      },
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1kl2OFnF2FADAAKjgZUFPo8dsBQUSIJM7\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "id": "oeqUajBBMM8n"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cef4586"
      },
      "source": [
        "# **_Transfer Learning_ y _Fine Tuning_ para Clasificación de Objetos en Imágenes**\n",
        "----\n",
        "\n",
        "En este Notebook daremos una introducción a las técnicas de **_Transfer Learning_** y **_Fine Tuning_**. Se tratan de técnicas comúnmente usadas para entrenar redes neuronales de forma más rápida y con resultados más precisos, basándose en modelos previamente construidos y entrenados en tareas similares. En este notebook estudiaremos un ejemplo práctico utilizando redes neuronales convolucionales preentrenadas para un uso de propósito general. Más específicamente veremos:\n",
        "- Modelos pre-entrenados en _Tensorflow_ con _ImageNet_\n",
        "- _Transfer Learning_\n",
        "  - Definición de los modelos\n",
        "  - Preprocesamiento\n",
        "  - Entrenamiento\n",
        "  - Evaluación\n",
        "- _Fine Tuning_\n",
        "  - _Warming Up_\n",
        "  - Entrenamiento\n",
        "- Evaluación de los modelos\n",
        "\n",
        "Primero, como siempre, importamos los paquetes necesarios:"
      ],
      "id": "5cef4586"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "899e53ae"
      },
      "outputs": [],
      "source": [
        "# Seleccionamos la versión más reciente de Tensorflow 2.0\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random\n",
        "\n",
        "plt.style.use(\"ggplot\")\n",
        "# Seleccionamos una semilla para los RNG\n",
        "tf.random.set_seed(0)\n",
        "np.random.seed(0)"
      ],
      "id": "899e53ae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "007baa6d"
      },
      "source": [
        "En este caso utilizaremos un dataset que contiene imágenes de rostros. Algunos de estos rostros son falsos; han sido creados usando herramientas profesionales de edición de imágenes (por expertos humanos). Veamos el conjunto de datos:"
      ],
      "id": "007baa6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttplyxXaWPLG"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/mindlab-unal/mlds5-datasets-unit3-TransferLearning.git"
      ],
      "id": "ttplyxXaWPLG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fTaFVZvbicAT"
      },
      "outputs": [],
      "source": [
        "base_path = '/content/mlds5-datasets-unit3-TransferLearning/real_and_fake_face'"
      ],
      "id": "fTaFVZvbicAT"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5071fe98"
      },
      "source": [
        "Comenzaremos cargando el dataset :"
      ],
      "id": "5071fe98"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0c520129"
      },
      "outputs": [],
      "source": [
        "# Cargamos el conjunto de datos\n",
        "all_images = []\n",
        "labels = []\n",
        "for i,val in enumerate([base_path+\"/fake/\", base_path+\"/real/\"]):\n",
        "    temp_path = val\n",
        "    for im_path in os.listdir(temp_path):\n",
        "        all_images.append(np.array(tf.keras.preprocessing.image.load_img(temp_path+im_path,\n",
        "                                                                         target_size=(224, 224, 3))))\n",
        "        labels.append(i)\n",
        "X = np.array(all_images)\n",
        "y = np.array(labels)"
      ],
      "id": "0c520129"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8bd8eeae"
      },
      "outputs": [],
      "source": [
        "# Miramos el tamaño en cada dimensión del dataset:\n",
        "X.shape"
      ],
      "id": "8bd8eeae"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdgLw9tHz-C8"
      },
      "source": [
        "Es decir, tenemos 2041 imágenes RGB de 224 por 224 pixeles. Hacemos una partición de entrenamiento y prueba:"
      ],
      "id": "qdgLw9tHz-C8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPNIGLUJmJ2f"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=15)"
      ],
      "id": "UPNIGLUJmJ2f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a697c546"
      },
      "source": [
        "Codificamos las etiquetas usando *one-hot representation*, una variable binaria por cada posible clase de salida :"
      ],
      "id": "a697c546"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b12aeb87"
      },
      "outputs": [],
      "source": [
        "Y_train = tf.keras.utils.to_categorical(y_train)\n",
        "Y_val = tf.keras.utils.to_categorical(y_val)"
      ],
      "id": "b12aeb87"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6b77bcec"
      },
      "source": [
        "Veamos algunos ejemplos de imágenes del dataset. La etiqueta 0 representa las imágenes falsas, y 1 las verdaderas:"
      ],
      "id": "6b77bcec"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ce3dc982"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 3, figsize=(10, 10))\n",
        "for i in range(9):\n",
        "    # Seleccionamos una imagen al azar\n",
        "    idx = np.random.randint(X_train.shape[0])\n",
        "    ax[i//3, i%3].imshow(X_train[idx]); ax[i//3, i%3].axis(\"off\")\n",
        "    ax[i//3, i%3].set_title(f\"label {y_train[idx]}\")"
      ],
      "id": "ce3dc982"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f86a088"
      },
      "source": [
        "# **ImageNet**\n",
        "----\n",
        "La popularidad del **_Transfer Learning_** se dio fundamentalmente a partir de los [resultados](https://paperswithcode.com/sota/image-classification-on-imagenet) del [proyecto ImageNet](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwi8uajxj-79AhWDbTABHeRGDXQQFnoECAoQAQ&url=https%3A%2F%2Fwww.image-net.org%2Fchallenges%2FLSVRC%2F&usg=AOvVaw1dIu39wO1j2Vb7VlfjJ9tc), el cual provee una gran base de datos visual diseñada para la tarea de reconocimiento de objetos. - Contiene más de **14 millones de imágenes** que fueron manualmente etiquetadas en más de 20.000 categorías diferentes.\n",
        "\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1kAx9nUlvvgHIz3NcA2cI_TcS5uQD9nuX\" alt = \"ImageNet Set\" width = \"60%\">  </img></center>\n",
        "\n",
        "\n",
        "\n",
        "El proyecto **ImageNet** tiene una competencia anual conocida como *the ImageNet Large Scale Visual Recognition Challenge* (ILSVRC), donde distintos métodos compiten para clasificar y detectar objetos de forma correcta.\n",
        "\n",
        "- A partir del **año 2012**, los métodos de *deep learning* comenzaron a ganar atención cuando una arquitectura de red convolucional conocida como **_AlexNet_** ganó la competencia, superando al ganador del año anterior con un 10% menos de error en la clasificación. Adicionalmente, en el **año 2015** una arquitectura de red neuronal (**_ResNet_**) demostró tener un error menor en comparación con un ser humano, mostrando así, que las máquinas son mejores que las personas en este tipo de tarea.\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1Lke35whYl9sazWlTErLUAe3YJaKT_ZsT\" alt = \"2012 ImageNet Competition\" width = \"70%\">  </img></center>\n",
        "\n",
        "Una de las principales ventajas de estos resultados es que **los pesos de las redes son liberado**s para ser utilizados por cualquier persona. Por ello, el *Transfer Learning* se volvió una práctica típica en la experimentación de *deep learning*.\n",
        "\n",
        "- En general, la idea consiste en utilizar un modelo robusto (que fue entrenado con millones de datos) y aplicarlo a una tarea específica tal y como se ilustra en la siguiente figura :\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1wymIS_mzuoc393UJGEHiIDYem2je-CsO\" alt = \"Transfer Learning\" width = \"70%\">  </img></center>\n"
      ],
      "id": "7f86a088"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV0uxk7mLJXJ"
      },
      "source": [
        "# **1. Modelos Pre-entrenados**\n",
        "----"
      ],
      "id": "CV0uxk7mLJXJ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QD3C3LRSLi7z"
      },
      "source": [
        "_TensorFlow_ nos permite utilizar varias arquitecturas pre-entrenadas con ImageNet; podemos encontrarlas en [`tf.keras.applications`](https://www.tensorflow.org/api_docs/python/tf/keras/applications). Algunas de las más famosas son:\n",
        "\n",
        "* VGG16\n",
        "* ResNet152\n",
        "* InceptionV3\n",
        "* MobileNetV2\n",
        "\n",
        "Si bien todas estas aplicaciones se construyen a partir de bloques convolucionales, cada una tiene una arquitectura particular, lo que influye directamente en aspectos como :\n",
        "- **El tamaño del modelo**, es decir el número de parámetros entrenables,\n",
        "- **La velocidad de cómputo** y su desempeño para distintas tareas específicas.\n",
        "\n",
        "La siguiente tabla nos muestra el tamaño de las aplicaciones mencionadas antes, el número de parámetros, y el desempeño (medido en exactitud) en la tarea de clasificación de ImageNet\n",
        "- La tabla completa se puede consultar en [Keras Applications](https://keras.io/applications/) :\n",
        "\n",
        "| Aplicación  | Tamaño | Parámetros | Exactitud |\n",
        "| ----------- | ----------- | ----------- | ----------- |\n",
        "| VGG16       | 528 MB      | 138.4M      | 90.1%       |\n",
        "| ResNet152   | 232 MB      |  60.4M      | 93.1%       |\n",
        "| InceptionV3 |  92 MB      |  23.9M      | 93.7%       |\n",
        "| MobileNetV2 |  14 MB      |   3.5M      | 90.1%       |\n",
        "\n",
        "Si bien cada aplicación tiene un desempeño específico en la tarea de clasificación de imágenes de ImageNet, no se puede concluir, por ejemplo, que **ResNet152** va a tener un mejor desempeño que **MobileNetV2** en cualquier tarea de clasificación de imágenes.\n",
        "- En general, cuando se busca usar un modelo preentrenado, se debe seleccionar una arquitectura base apropiada para cada problema en particular, buscando un equilibrio entre el costo computacional, tamaño y el desempeño :\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1fKuH0HIgCrJzJlQhHbmhnm1bJlb-5wXO\" alt = \"Top Accuracy\" width = \"50%\">  </img></center>\n",
        "\n",
        "Veamos un ejemplo de cómo cargar a nuestro entorno una aplicación, específicamente VGG16.\n",
        "\n",
        "Lo que vamos a hacer es definir un modelo usando la función `tf.keras.applications.VGG16`. Cada modelo se define usando su respectiva función, que recibe una serie de argumentos:\n",
        "\n",
        "*    `include_top`: `True` o `False`, sirve para incluir la capa totalmente conectada en la parte superior del modelo, como última capa de la red. Como estos modelos se han entrenado con ImageNet, esta capa, si se incluye, tiene tamaño 1000, por las 1000 clases de ImageNet. Por tanto, si se quiere usar el modelo para una tarea diferente con _Transfer Learning_ o _Fine Tuning_, se debe asignar `False` a este parámetro. Por defecto es `True`.\n",
        "*    `weights`: `None` para una inicialización aleatoria, o `imagenet` para cargar los pesos del preentrenamiento en ImageNet, o la ruta al archivo de pesos que se va a cargar. Por defecto es `imagenet`.\n",
        "*    `input_tensor`: puede ser la salida de un `layers.Input()` para utilizar como entrada de imagen para el modelo. Por defecto es `None`.\n",
        "*    `input_shape`: Tupla de forma opcional, sólo debe especificarse si `include_top` es `False` (de lo contrario, la forma de entrada debe ser `(224, 224, 3)` para VGG16. Debe tener exactamente 3 canales de entrada, y la anchura y la altura no deben ser inferiores a 75.\n",
        "*    `pooling`: Modo de agrupación opcional para la extracción de características cuando `include_top` es `False`.\n",
        "*    `classes`: número de clases en las que clasificar las imágenes, sólo debe especificarse si `include_top` es `True`, y si no se especifica ningún argumento de pesos. Por defecto es `1000`.\n",
        "*    `classifier_activation`: La función de activación a utilizar en la capa de salida. Se ignora a menos que `include_top=True`. Por defecto es `softmax`.\n"
      ],
      "id": "QD3C3LRSLi7z"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e70b80c2"
      },
      "outputs": [],
      "source": [
        "# Cargamos una red VGG16\n",
        "model = tf.keras.applications.VGG16(weights='imagenet',\n",
        "                                    include_top=True,\n",
        "                                    input_shape=(224, 224, 3))\n",
        "model.summary()"
      ],
      "id": "e70b80c2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03ae21db"
      },
      "source": [
        "En el modelo pusimos `include_top=True`. Entonces veamos un ejemplo de la clasificación de cuatro imágenes en las categorías originales de ImageNet :"
      ],
      "id": "03ae21db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b24d64b"
      },
      "outputs": [],
      "source": [
        "# Descargamos algunas imágenes de ejemplo\n",
        "!wget -O koala.jpg https://upload.wikimedia.org/wikipedia/commons/4/49/Koala_climbing_tree.jpg\n",
        "!wget -O wolf.jpg https://upload.wikimedia.org/wikipedia/commons/d/d7/Wolf_Kolm%C3%A5rden.jpg\n",
        "!wget -O tiger.jpg https://steemit-production-imageproxy-upload.s3.amazonaws.com/DQmenD8j2rha9SkqpB1gQDrkuE9FmjBTwxb4eGbmHQ81aGM\n",
        "!wget -O cat.jpg https://as.com/epik/imagenes/2017/11/13/portada/1510586807_350031_1510586958_noticia_normal.jpg"
      ],
      "id": "3b24d64b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8e50454b"
      },
      "source": [
        "Comenzamos cargando y preprocesando las imágenes. Note que usamos una función de preprocesamiento **específica** del modelo VGG16: `tf.keras.applications.vgg16.preprocess_input`. Las imágenes se convierten de RGB a BGR y, a continuación, cada canal de color se centra en cero con respecto al conjunto de datos ImageNet, sin escalado."
      ],
      "id": "8e50454b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e813b26"
      },
      "outputs": [],
      "source": [
        "# Cargamos las imágenes\n",
        "ims = [tf.keras.preprocessing.image.load_img(i, target_size=(224, 224, 3))\n",
        "       for i in [\"koala.jpg\", \"wolf.jpg\", \"tiger.jpg\", \"cat.jpg\"]]\n",
        "# Convertimos las imágenes a arreglos de numpy\n",
        "ims = np.array(list(map(np.array, ims)))\n",
        "# Aplicamos el preprocesamiento de la InceptionV3 a las imágenes\n",
        "ims_prep = tf.keras.applications.vgg16.preprocess_input(ims.copy())\n",
        "# Mostramos el tamaño de nuestras imágenes\n",
        "ims_prep.shape"
      ],
      "id": "0e813b26"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c93dd926"
      },
      "source": [
        "Ahora, realizamos las predicciones y mostramos los resultados:"
      ],
      "id": "c93dd926"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "02e60a9e"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las predicciones\n",
        "preds = model.predict(ims_prep)\n",
        "# Obtenemos el nombre de las categorías predichas\n",
        "labs = tf.keras.applications.inception_v3.decode_predictions(preds)\n",
        "# Mostramos las imágenes y las categorías predichas\n",
        "fig, ax = plt.subplots(4, 2, figsize=(10,20))\n",
        "for i in range(4):\n",
        "    ax[i, 0].imshow(ims[i]); ax[i, 0].axis(\"off\")\n",
        "    ax[i, 1].bar(list(map(lambda x:x[1], labs[i])),\n",
        "                 list(map(lambda x:x[2], labs[i])), color=\"k\")\n",
        "    ax[i,1].set_xticklabels(list(map(lambda x:x[1], labs[i])), rotation=45)\n",
        "fig.tight_layout()"
      ],
      "id": "02e60a9e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ff8eb64"
      },
      "source": [
        "Para poder utilizar estas redes en un problema nuevo, podemos seguir dos metodologías dependiendo del tamaño del dataset y de la afinidad del problema con el conjunto de datos en el que se hizo el entrenamiento originalmente :\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=14MQ8XtP6ZgA2lwDcbn4JOWcpXxsqz2uo\" alt = \"Diagrama ilustrativo de las metodologías de Transfer Learning y Fine Tuning\" width = \"70%\">  </img></center>\n",
        "\n",
        "\n",
        "\n",
        "Veamos en detalle cómo realizar *Transfer Learning* y *Fine Tuning*:"
      ],
      "id": "2ff8eb64"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1bf07ecc"
      },
      "source": [
        "# **2. _Transfer Learning_**\n",
        "----\n",
        "\n",
        "Los seres humanos tenemos una habilidad inherente de transferir conocimiento entre distintas tareas, es decir, lo que aprendemos en determinada tarea lo podemos reutilizar para resolver tareas relacionadas. Entre más similares sean las tareas, más fácil es el aprendizaje. Veamos algunos ejemplos sencillos:\n",
        "\n",
        "* Saber cómo montar bicicleta $\\rightarrow$ Aprender a conducir una moto.\n",
        "* Saber cómo tocar música clásica en piano $\\rightarrow$ Aprender a tocar jazz en piano.\n",
        "* Tener conocimiento en matemática, programación y estadística $\\rightarrow$ Aprender *machine learning*.\n",
        "\n",
        "Esta es la base de **_Transfer Learning_**, donde se busca que las redes neuronales aprendan una nueva tarea partiendo de un conocimiento base :\n",
        "\n",
        "\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1kUMODR1rk1i1C9RCj85mGZAI8TKk12Ij\" alt = \"Diagrama ilustrativo de la aplicación de Transfer Learning a un conjunto de datos de imágenes médicas\" width = \"80%\">  </img></center>\n",
        "\n",
        "Como mencionamos anteriormente, esta técnica se utiliza comúnmente cuando tenemos datasets pequeños. La idea es utilizar una representación intermedia de una red profunda para obtener una representación de los datos de entrada.\n",
        "\n",
        "Por ejemplo, en el caso de **las imágenes**, un enfoque típico es utilizar las capas convolucionales como extractores de características para posteriormente entrenar una red neuronal multicapa en la nueva tarea.\n"
      ],
      "id": "1bf07ecc"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55fe0936"
      },
      "source": [
        "## **2.1. Definición del modelo**\n",
        "----\n",
        "\n",
        "Veamos un ejemplo práctico de esto con _TensorFlow_, para ello, utilizaremos una [*EfficientNetB0*](https://arxiv.org/abs/1905.11946). Este modelo lo introdujo Google AI, con la intención de proponer un método eficiente, como sugiere su nombre, al tiempo que mejora los resultados del estado del arte. Generalmente para mejorar en las métricas de clasificación, los modelos se hacen demasiado anchos, profundos o con una resolución muy alta. Aumentar estas características ayuda al modelo inicialmente pero rápidamente se satura y el modelo final sólo tiene más parámetros y por lo tanto no es eficiente. En _EfficientNet_ se escalan de un modo más basado en principios, es decir, se aumenta todo gradualmente.\n",
        "\n",
        "\n",
        "Veamos un ejemplo con el dataset de clasificación de rostros, comenzamos cargando un extractor de características, es decir no vamos a incluir la parte superior del modelo donde se hace la clasificación (`include_top=False`):"
      ],
      "id": "55fe0936"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "99b6d13b"
      },
      "outputs": [],
      "source": [
        "# Cargamos EfficientNet sin el top (capas densas del final)\n",
        "extractor = tf.keras.applications.EfficientNetB0(weights='imagenet',\n",
        "                                            include_top=False,\n",
        "                                            input_shape=(224, 224, 3))\n",
        "extractor.summary()"
      ],
      "id": "99b6d13b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cc7ba0b"
      },
      "source": [
        "Ahora, agregamos un nuevo clasificador (con pesos aleatorios) para transferir el conocimiento de ImageNet al problema con dataset de caras reales y falsas:"
      ],
      "id": "1cc7ba0b"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pDJArg6tNfH"
      },
      "source": [
        "**Congelar Capas**\n",
        "\n",
        "Primero vamos a _congelar_ las capas que pertenecen al modelo original de _EfficientNet_, esto es, definir que los pesos de esas capas no se van a entrenar. Cada capa `layer` del modelo `extractor` tiene un atributo booleano (`True` o `False`) llamado `trainable`. Vamos a definirlo `False` en todas las `layers` de `extractor`:"
      ],
      "id": "4pDJArg6tNfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ac19lJQFtNp3"
      },
      "outputs": [],
      "source": [
        "# Congelamos el extractor de características (Transfer Learning)\n",
        "for layer in extractor.layers:\n",
        "    layer.trainable=False"
      ],
      "id": "ac19lJQFtNp3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UF1Bf0-6tOHu"
      },
      "source": [
        "**Agregar un Clasificador**\n",
        "\n",
        "Ahora agregamos a el modelo las capas que constituyen el clasificador final, un simple perceptrón multicapa que va a actuar sobre la salida del modelo `extractor`:\n"
      ],
      "id": "UF1Bf0-6tOHu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7852e5a4"
      },
      "outputs": [],
      "source": [
        "# Creamos una capa de pooling para consolidar los feature maps de salida en\n",
        "# 1024 valores\n",
        "pool = tf.keras.layers.GlobalAveragePooling2D()(extractor.output)\n",
        "# Agregamos una capa densa\n",
        "dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
        "# Agregamos dropout para regularización\n",
        "drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "# Agregamos una capa de salida\n",
        "dense2 = tf.keras.layers.Dense(units=2, activation=\"softmax\")(drop1)\n",
        "# Definimos nuestro modelo de transfer learning\n",
        "tl_model = tf.keras.models.Model(inputs=[extractor.input], outputs=[dense2])\n",
        "# Compilamos el modelo\n",
        "tl_model.compile(loss=\"categorical_crossentropy\",\n",
        "                 optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "                 metrics=[\"accuracy\"])\n",
        "tl_model.summary()"
      ],
      "id": "7852e5a4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RnKJDM1YoE6"
      },
      "source": [
        "Las últimas cuatro capas del modelo anterior constituyen entonces el clasificador:\n",
        "\n",
        "```\n",
        "global_average_pooling2d (Glob  (None, 1280)        0           ['top_activation[0][0]']         \n",
        " alAveragePooling2D)                                                                              \n",
        "                                                                                                  \n",
        " dense (Dense)                  (None, 32)           40992       ['global_average_pooling2d[0][0]'\n",
        "                                                                 ]                                \n",
        "                                                                                                  \n",
        " dropout (Dropout)              (None, 32)           0           ['dense[0][0]']                  \n",
        "                                                                                                  \n",
        " dense_1 (Dense)                (None, 2)            66          ['dropout[0][0]']                \n",
        "                                                                                                  \n",
        "==================================================================================================\n",
        "Total params: 4,090,629\n",
        "Trainable params: 4,048,606\n",
        "Non-trainable params: 42,023\n",
        "```\n",
        "\n",
        "La salida de `global_average_pooling2d` constituyen las características extraídas de la imagen que entra al modelo completo. Es decir, el extractor de características transforma cada imagen en un vector de tamaño 1280. Este vector entra entonces a un modelo de dos capas densas: una capa oculta de 32 neuronas con _dropout_, y la capa de salida de dos neuronas. Estas capas aportan los 41058 parámetros que se van a entrenar. Note que los parámetros del modelo base no cuentan como parámetros entrenables."
      ],
      "id": "2RnKJDM1YoE6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e54850d8"
      },
      "source": [
        "## **2.2. Data augmentation y preprocesamiento**\n",
        "----\n",
        "\n",
        "Para entrenar el modelo utilizaremos _Data Augmentation_, de la misma forma como lo utilizamos en el notebook anterior (_Introducción a las Redes Neuronales Convolucionales (CNN)_):"
      ],
      "id": "e54850d8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e2be10a5"
      },
      "outputs": [],
      "source": [
        "# Definimos las transformaciones para data augmentation\n",
        "train_datagen = tf.keras.preprocessing.image.ImageDataGenerator(rotation_range=10,\n",
        "                                                                width_shift_range=0.2,\n",
        "                                                                height_shift_range=0.2,\n",
        "                                                                shear_range=0.2,\n",
        "                                                                zoom_range=0.2,\n",
        "                                                                horizontal_flip=True,\n",
        "                                                                fill_mode='constant')"
      ],
      "id": "e2be10a5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe90e3ad"
      },
      "source": [
        "Utilizamos el preprocesamiento de la _EfficientNet_ para transformar los conjuntos:"
      ],
      "id": "fe90e3ad"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b900fb89"
      },
      "outputs": [],
      "source": [
        "X_train_prep = tf.keras.applications.efficientnet.preprocess_input(X_train)\n",
        "X_val_prep = tf.keras.applications.efficientnet.preprocess_input(X_val)"
      ],
      "id": "b900fb89"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "606c782c"
      },
      "source": [
        "Ahora, definimos el generador de entrenamiento. El conjunto de validación y de prueba se dejarán intactos, para poder evaluar sobre datos reales, y como son pequeños y caben en memoria, no ameritan un generador."
      ],
      "id": "606c782c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2d4c39bd"
      },
      "outputs": [],
      "source": [
        "train_gen = train_datagen.flow(x=X_train_prep,\n",
        "                               y=Y_train,\n",
        "                               batch_size=32)"
      ],
      "id": "2d4c39bd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e78f1aef"
      },
      "source": [
        "## **2.3. Entrenamiento**\n",
        "----\n",
        "\n",
        "Para el entrenamiento, utilizaremos un _Callback_ para guardar el modelo con menor pérdida en validación; guardaremos los pesos del mejor modelo en un archivo llamado: `transfer_learning.h5`.\n",
        "\n",
        "> **Nota**:  `transfer_learning.h5` se guardará temporalmente solo mientras el entorno de ejecución está conectado."
      ],
      "id": "e78f1aef"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4b2062a"
      },
      "outputs": [],
      "source": [
        "# Definimos el callback\n",
        "best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"transfer_learning.weights.h5\",\n",
        "                                                   monitor=\"val_loss\",\n",
        "                                                   verbose=True,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=True,\n",
        "                                                   mode=\"min\")\n",
        "# Entrenamos el modelo\n",
        "hist_tl = tl_model.fit(x=train_gen,\n",
        "                       validation_data=(X_val_prep, Y_val),\n",
        "                       epochs=10,\n",
        "                       steps_per_epoch=X_train.shape[0]//32,\n",
        "                       callbacks=[best_callback])"
      ],
      "id": "b4b2062a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f824b25e"
      },
      "source": [
        "## **2.4. Evaluación**\n",
        "----\n",
        "\n",
        "Ahora, evaluaremos el desempeño del modelo en términos de _accuracy_, _recall_, _precision_ y _F1 Score_. Primero cargamos los pesos del mejor modelo usando la función `load_weights`, que recibe directamente el archivo en formato `h5`:"
      ],
      "id": "f824b25e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5b7f8803"
      },
      "outputs": [],
      "source": [
        "# Cargamos el mejor modelo\n",
        "tl_model.load_weights(\"transfer_learning.weights.h5\")"
      ],
      "id": "5b7f8803"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1764083"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, np.argmax(tl_model.predict(X_val_prep,\n",
        "                                                              batch_size=32),\n",
        "                                            axis=1)\n",
        "                            ))"
      ],
      "id": "a1764083"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73b28dff"
      },
      "source": [
        "# **3. _Fine Tuning_**\n",
        "----\n",
        "\n",
        "Cuando el dataset nuevo no es tan parecido al problema original (por ejemplo, cuando se quiere hacer análisis de imágenes médicas), no es muy recomendable utilizar características de tan alto nivel. En este caso, se aplica **_fine tuning_** para poder entrenar otras capas del modelo base y ajustar la red mejor a nuestro problema en partícular :\n",
        "<center><img src = \"https://drive.google.com/uc?export=view&id=1XNazt6CfzxCIAElz0KJotnYh9M8U4ory\" alt = \"Diagrama ilustrativo de la aplicación de Fine Tuning a un conjunto de datos de imágenes de frotis sanguíneo\" width = \"70%\">  </img></center>\n",
        "\n",
        "*Fine Tuning* permite de **aprender mejores parámetros a cambio de un mayor tiempo de entrenamiento**. Así mismo, se trata de un método que requiere una buena experimentación para poder converger. A continuación, se presentan algunos consejos que se pueden aplicar para mejorar el *Fine Tuning*:\n",
        "\n",
        "* **Calentamiento**: Al igual que en *Transfer Learning*, las últimas capas densas son reemplazadas por un nuevo clasificador, no obstante, al iniciar con pesos completamente aleatorios se pueden dañar las representaciones de las capas de bajo nivel. Por ello, es recomendable entrenar primero un poco (calentamiento) las capas del final antes de entrenar todo el modelo.\n",
        "\n",
        "* **Taza de aprendizaje**: Lo recomendable para *Fine Tuning* es utilizar valores pequeños de _learning rate_ (o tasa de aprendizaje), al menos, valores menores de los que se usan en *Transfer Learning*.\n",
        "\n",
        "* **Congelamiento**: En muchos casos no es tan recomendable entrenar toda la red, por ejemplo, en redes convolucionales los filtros de las primeras capas convergen a valores similares (tienden a ser filtros de bordes o de forma), por ello, es necesario explorar a qué porcentaje de capas se les realizará *Fine Tuning*.\n",
        "\n",
        "Veamos un ejemplo de *Fine Tuning*:\n"
      ],
      "id": "73b28dff"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c477fe5f"
      },
      "source": [
        "## **3.1. Definición del modelo**\n",
        "----\n",
        "\n",
        "En este caso utilizaremos la misma arquitectura que usamos en *Transfer Learning*, no obstante, más adelante también entrenaremos las capas convolucionales de la _EfficientNet_:"
      ],
      "id": "c477fe5f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a0645c50"
      },
      "outputs": [],
      "source": [
        "# Cargamos EfficientNet sin el top (capas densas del final)\n",
        "extractor = tf.keras.applications.EfficientNetB0(weights='imagenet',\n",
        "                                            include_top=False,\n",
        "                                            input_shape=(224, 224, 3))\n",
        "\n",
        "# Congelamos el extractor de características\n",
        "for layer in extractor.layers:\n",
        "    layer.trainable=False\n",
        "\n",
        "# Creamos una capa de pooling para eliminar los ejes no deseados\n",
        "pool = tf.keras.layers.GlobalAveragePooling2D()(extractor.output)\n",
        "# Agregamos una capa densa\n",
        "dense1 = tf.keras.layers.Dense(units=32, activation=\"relu\")(pool)\n",
        "# Agregamos dropout para regularización\n",
        "drop1 = tf.keras.layers.Dropout(0.2)(dense1)\n",
        "# Agregamos una capa de salida\n",
        "dense2 = tf.keras.layers.Dense(units=2, activation=\"softmax\")(drop1)\n",
        "# Definimos nuestro modelo de transfer learning\n",
        "ft_model = tf.keras.models.Model(inputs=[extractor.input], outputs=[dense2])\n",
        "ft_model.summary()"
      ],
      "id": "a0645c50"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58b02744"
      },
      "source": [
        "## **3.2. Calentamiento (*Warming Up*)**\n",
        "----\n",
        "Comenzamos entrenando sólo un poco (en este caso máximo 10 _epochs_) las nuevas capas densas del final fijando el learning rate en $10^{-3}$.Guardamos el mejor modelo para luego empezar desde ese punto el entrenamiento de _Fine Tuning_.\n"
      ],
      "id": "58b02744"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "96fc2b0c"
      },
      "outputs": [],
      "source": [
        "# Compilamos el modelo\n",
        "ft_model.compile(loss=\"categorical_crossentropy\",\n",
        "                 optimizer=tf.optimizers.Adam(learning_rate=1e-3),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Definimos el callback\n",
        "best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"warming_up.weights.h5\",\n",
        "                                                   monitor=\"val_loss\",\n",
        "                                                   verbose=True,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=True,\n",
        "                                                   mode=\"min\")\n",
        "\n",
        "# Entrenamos el modelo\n",
        "ft_model.fit(train_gen,\n",
        "                       validation_data=(X_val_prep, Y_val),\n",
        "                       epochs=10,\n",
        "                       steps_per_epoch=X_train.shape[0]//32,\n",
        "                       callbacks=[best_callback])"
      ],
      "id": "96fc2b0c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ce55cab5"
      },
      "source": [
        "## **3.3. Ajuste Fino (_Fine Tuning_)**\n",
        "----\n",
        "\n",
        "Ahora disminuimos el learning rate y permitimos entrenar todas las capas de la red. Esto se logra cambiando el atributo `trainable` a `True` de cada una de las capas correspondientes. Cargamos los pesos obtenidos durante el calentamiento y bajamos la tasa de aprendizaje a $10^{-4}$. Recordemos que esto lo hacemos porque solo queremos hacer ajustes muy ligeros a los pesos del modelo.\n"
      ],
      "id": "ce55cab5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aba0e959"
      },
      "outputs": [],
      "source": [
        "# Hacemos entrenables todas las capas\n",
        "for layer in ft_model.layers:\n",
        "    layer.trainable = True\n",
        "\n",
        "# Disminuímos el learning rate\n",
        "ft_model.compile(loss=\"categorical_crossentropy\",\n",
        "                 optimizer=tf.optimizers.Adam(learning_rate=1e-4),\n",
        "                 metrics=[\"accuracy\"])\n",
        "\n",
        "# Cargamos los pesos del calentamiento\n",
        "ft_model.load_weights(\"warming_up.weights.h5\")\n",
        "\n",
        "# Definimos el callback\n",
        "best_callback = tf.keras.callbacks.ModelCheckpoint(filepath=\"fine_tuning.weights.h5\",\n",
        "                                                   monitor=\"val_loss\",\n",
        "                                                   verbose=True,\n",
        "                                                   save_best_only=True,\n",
        "                                                   save_weights_only=True,\n",
        "                                                   mode=\"min\")"
      ],
      "id": "aba0e959"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dd3eed8e"
      },
      "outputs": [],
      "source": [
        "# Entrenamos\n",
        "hist_ft = ft_model.fit(train_gen,\n",
        "                        validation_data=(X_val_prep, Y_val),\n",
        "                        epochs=20,\n",
        "                        steps_per_epoch=X_train.shape[0]//32,\n",
        "                        callbacks=[best_callback])"
      ],
      "id": "dd3eed8e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d0c43330"
      },
      "source": [
        "## **3.4. Evaluación**\n",
        "----\n",
        "\n",
        "Ahora evaluaremos el modelo en términos de _accuracy, recall, precision_ y f1:"
      ],
      "id": "d0c43330"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8944d985"
      },
      "outputs": [],
      "source": [
        "# Cargamos el mejor modelo\n",
        "ft_model.load_weights(\"fine_tuning.weights.h5\")"
      ],
      "id": "8944d985"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb73688a"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_val, np.argmax(ft_model.predict(X_val_prep,\n",
        "                                                              batch_size=32),\n",
        "                                              axis=1)\n",
        "                            ))"
      ],
      "id": "fb73688a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "057b7260"
      },
      "source": [
        "Por último, compararemos las pérdidas y el accuracy de los dos modelos a lo largo de las iteraciones:"
      ],
      "id": "057b7260"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6bc99250"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.subplot(121)\n",
        "plt.plot(hist_tl.history[\"loss\"], \"r\", label=\"transfer learning - entrenamiento\")\n",
        "plt.plot(hist_tl.history[\"val_loss\"], \"r--\", label=\"transfer learning - validación\")\n",
        "plt.plot(hist_ft.history[\"loss\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
        "plt.plot(hist_ft.history[\"val_loss\"], \"k--\", label=\"fine tuning - validación\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.title(\"Pérdida\")\n",
        "plt.legend()\n",
        "plt.subplot(122)\n",
        "plt.plot(hist_tl.history[\"accuracy\"], \"r\", label=\"transfer learning - entrenamiento\")\n",
        "plt.plot(hist_tl.history[\"val_accuracy\"], \"r--\", label=\"transfer learning - validación\")\n",
        "plt.plot(hist_ft.history[\"accuracy\"], \"k\", label=\"fine tuning - entrenamiento\")\n",
        "plt.plot(hist_ft.history[\"val_accuracy\"], \"k--\", label=\"fine tuning - validación\")\n",
        "plt.xlabel(\"Épocas\")\n",
        "plt.title(\"Accuracy\")\n",
        "plt.legend()"
      ],
      "id": "6bc99250"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mzQxgPikiyu"
      },
      "source": [
        "Compare los resultados obtenidos entre _Transfer Learning_ y _Fine Tuning_. Los últimos son mejores. Sin embargo, aunque parezca fácil, estas técnicas no siempre van a funcionar con cualquier modelo en cualquier conjunto de datos. Todo hay que explorarlo y tener mucha paciencia en el proceso."
      ],
      "id": "6mzQxgPikiyu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b1f5b45"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "----\n",
        "\n",
        "* [*Transfer Learning Introduction*](https://www.hackerearth.com/practice/machine-learning/transfer-learning/transfer-learning-intro/tutorial/)\n",
        "* A. G. Howard et al., “MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications.”\n",
        "* [*Fine-tuning guidelines*](https://image.slidesharecdn.com/stcbwed1730mac205-161230131625/95/aws-reinvent-2016-deep-learning-at-cloud-scale-improving-video-discoverability-by-scaling-up-caffe-on-aws-mac205-28-638.jpg?cb=1483103838)\n",
        "* [*Transfer learning: the dos and don’ts*](https://medium.com/starschema-blog/transfer-learning-the-dos-and-donts-165729d66625)\n",
        "* [*Transfer Learning : Approaches and Empirical Observations*](https://hackernoon.com/transfer-learning-approaches-and-empirical-observations-efeff9dfeca6)\n",
        "* [*A Comprehensive Hands-on Guide to Transfer Learning with Real-World Applications in Deep Learning*](https://towardsdatascience.com/a-comprehensive-hands-on-guide-to-transfer-learning-with-real-world-applications-in-deep-learning-212bf3b2f27a)\n",
        "\n",
        "\n",
        "* _Origen de los íconos_\n",
        "\n",
        "    - Alex Krizhevsky. The CIFAR-10 dataset [JPG]. https://www.cs.toronto.edu/~kriz/cifar.html\n",
        "    - Flaticon. Free Check Icon [PNG]. https://www.flaticon.com/free-icon//cheque_3699516\n",
        "    - Flaticon. Free Cross Icon [PNG]. https://www.flaticon.com/free-icon//cruzar_6711656\n",
        "    - Flaticon. Free New Car Icon [PNG]. https://www.flaticon.com/free-icon//carro-nuevo_5044540\n",
        "    - Flaticon. Free Cart Icon [PNG]. https://www.flaticon.com/free-icon//carro_1574481\n",
        "    - MDPI. Ball chart reporting the Top-1 ImageNet-1k accuracy vs. computational complexity\n",
        " [JPG]. https://www.mdpi.com/electronics/electronics-11-00945/article_deploy/html/images/electronics-11-00945-g001-550.jpg\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "    \n",
        "    "
      ],
      "id": "5b1f5b45"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o8qH9RB6w_ES"
      },
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](https://www.linkedin.com/in/mario-andres-rodriguez-triana-394806145/).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "id": "o8qH9RB6w_ES"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}