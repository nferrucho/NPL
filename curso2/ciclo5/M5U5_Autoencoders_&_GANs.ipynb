{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo5/M5U5_Autoencoders_%26_GANs.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhRJN2bQowUd"
      },
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1Hh_G3M13P9xSNgSiQ-WnALg93XwK_hG8\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "id": "vhRJN2bQowUd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KqxLq6Bo1EX"
      },
      "source": [
        "# **_Autoencoders_ y Redes Generativas Adversarias**\n",
        "---\n",
        "\n",
        "En este Notebook daremos una breve introducción a métodos generativos de Deep Learning. Los **modelos _generativos_** cubren una clase de modelos estadísticos que contrastan con los **modelos _discriminativos_**. Rápidamente podemos resaltar que :\n",
        "\n",
        "- Los modelos _generativos_ pueden generar nuevas instancias de datos.\n",
        "- Los modelos _discriminativos_ discriminan (valga la redundancia) entre distintos tipos de datos.\n",
        "\n",
        "Es decir, un modelo _generativo_ podría generar nuevas fotos de animales que se parecieran a animales reales, mientras que un modelo discriminativo podría distinguir un perro de un gato.\n",
        "\n",
        "Más formalmente, dado un conjunto de instancias de datos $X$ y su respectivo conjunto de etiquetas $y$, tenemos que:\n",
        "\n",
        "- Los modelos _generativos_ capturan la probabilidad conjunta $P(X, y)$, o simplemente $P(X)$ si no hay etiquetas.\n",
        "- Los modelos _discriminativos_ en cambio capturan la probabilidad condicional $P(y | X)$.\n",
        "\n",
        "Un **modelo _generativo_** captura la distribución de los propios datos y es capaz de calcular la probabilidad de ocurrencia de un ejemplo determinado. Por ejemplo, los modelos que predicen la siguiente palabra de una secuencia de texto suelen ser modelos generativos porque pueden **asignar una probabilidad** a una secuencia de palabras.\n",
        "\n",
        "Un **modelo _discriminativo_** en cambio ignora la pregunta de si una instancia dada es probable o no, y simplemente es capaz de **calcular la probabilidad** de que una **etiqueta** se aplique a la instancia.\n",
        "\n",
        "En este Notebook veremos algunos ejemplos de implementaciones de:\n",
        "* _Autoencoders_,\n",
        "* _Autoencoders_ Variacionales,\n",
        "* Redes Generativas Adversarias (GAN's - _Generative Adversarial Networks_),\n",
        "\n",
        "Primero importamos _TensorFlow_ y los paquetes que usaremos :"
      ],
      "id": "2KqxLq6Bo1EX"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "044b86c4"
      },
      "outputs": [],
      "source": [
        "# Seleccionamos la versión más reciente de Tensorflow 2.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "# Seleccionamos una semilla para los RNG (Random Number Generator)\n",
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)\n",
        "import time"
      ],
      "id": "044b86c4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gI2KeGwPtThH"
      },
      "source": [
        "**Tensorflow Probability**\n",
        "\n",
        "_Tensorflow Probability_ (TFP) es un paquete de _Python_ construido sobre _Tensorflow_ que facilita la combinación de modelos probabilísticos y aprendizaje profundo en hardware moderno (TPU, GPU). TFP incluye:\n",
        "\n",
        "- Una amplia selección de distribuciones de probabilidad y biyectores.\n",
        "- Herramientas para construir modelos probabilísticos profundos, incluidas capas probabilísticas y una abstracción `JointDistribution`.\n",
        "- Métodos de inferencia variacional y _cadena de Markov Monte Carlo_.\n",
        "- Optimizadores especiales como _Nelder-Mead, BFGS y SGLD_.\n",
        "\n",
        "Dado que TFP hereda las ventajas de _TensorFlow_, se puede **construir, ajustar y desplegar** un modelo utilizando un único lenguaje.\n",
        "\n",
        "> **Nota**: Por defecto, TFP no viene instalado en el entorno de Google Colab, así que debemos instalarlo manualmente :"
      ],
      "id": "gI2KeGwPtThH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ccf4878"
      },
      "outputs": [],
      "source": [
        "!pip install tensorflow-probability\n",
        "!pip install imageio # Paquete para generar gifs\n",
        "!pip install git+https://github.com/tensorflow/docs"
      ],
      "id": "3ccf4878"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9446d9d8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import PIL\n",
        "import glob\n",
        "import imageio.v2 as imageio\n",
        "import pandas as pd\n",
        "from IPython import display\n",
        "import tensorflow_probability as tfp\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras import layers, losses\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score"
      ],
      "id": "9446d9d8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fb26b45e"
      },
      "source": [
        "# **1. _Autoencoders_**\n",
        "---\n",
        "\n",
        "Los **_Autoencoders_** son un tipo especial de redes neuronales que son típicamente usados en problemas no supervisados y de reducción de dimensionalidad. Se trata de un tipo de arquitecturas que busca beneficiarse del *Representation Learning* o aprendizaje de representación.\n",
        "\n",
        "En general, con los _Autoencoders_ se busca imponer una restricción de *cuello de botella* con el fin de obtener una representación más compacta de los datos. Un ejemplo típico de la arquitectura de un _Autoencoder_ se puede ver a continuación :\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1Kmjwib_nsyLyKYAMt3dBn55VIuj-M--N\" alt = \"Gráfico ilustrativo de la arquitectura de un Autoencoder \" width=\"60%\" /></center>\n",
        "\n",
        "\n",
        "Veamos algunos componentes generales que conforman un _Autoencoder_ :\n",
        "\n",
        "* **Encoder** : o _codificador_, se trata de una red neuronal que transforma los datos a un espacio de menor dimensionalidad o _espacio latente_ (en inglés _latent space_, _latent feature space_ o _embedding space_).\n",
        "* **Decoder** : o _decodificador_, se trata de una red neuronal que transforma los datos del espacio latente al espacio original.\n",
        "\n",
        "En el ejemplo de la imagen, los datos de entrada tienen 6 dimensiones. El _encoder_ transforma los datos hasta llevarlos a un espacio de 3 dimensiones. Esta nueva representación se llama _Representación Latente_.\n",
        "\n",
        "- Para garantizar que esta reducción de dimensionalidad conserve la mayor cantidad de información de la representación original, el _decoder_ debe ser capaz de reconstruir los datos originales a partir de la representación latente.\n",
        "\n",
        "En resumen, un _Autoencoder_ **busca reconstruir  los datos de entrada** utilizando una representación latente. Esto se consigue al optimizar la siguiente función de pérdida :\n",
        "\n",
        "$$\n",
        "L=\\frac{1}{N}\\sum_{i=1}^N ||x_i-\\widetilde{x}_i||^2\n",
        "$$\n",
        "\n",
        "Donde $x_i$ es una observación original y $\\widetilde{x}_i$ está dado por :\n",
        "\n",
        "$$\n",
        "\\widetilde{x}_i = \\text{decoder}(\\text{encoder}(x_i)),\n",
        "$$\n",
        "\n",
        "y lo que se busca es :\n",
        "$$\n",
        "x_i \\approx \\widetilde{x}_i.\n",
        "$$\n",
        "Es decir, el modelo, a través del _encoder_, **transforma los datos originales** a un _espacio latente_, y a partir de ahí debe ser capaz de **reconstruir los datos al espacio original** usando el _decoder_.\n",
        "\n",
        "Es un concepto muy simple pero su aplicación es bastante útil y posee propiedades bastante interesantes. Veamos un ejemplo de esto en _Tensorflow_.\n",
        "\n",
        "- Primero vamos a cargar el dataset de imágenes _MNIST_ usando `tf.keras.datasets`:\n"
      ],
      "id": "fb26b45e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "831ff87d"
      },
      "outputs": [],
      "source": [
        "(train_images, _), (test_images, _) = tf.keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train = train_images.astype('float32') / 255.\n",
        "x_test = test_images.astype('float32') / 255.\n",
        "\n",
        "print (x_train.shape)\n",
        "print (x_test.shape)"
      ],
      "id": "831ff87d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20TuIh3-ww96"
      },
      "source": [
        "- Veamos un ejemplo de las imágenes"
      ],
      "id": "20TuIh3-ww96"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fc9e03f"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(3, 3, figsize=(10, 10))\n",
        "batch = train_images[:11]\n",
        "for i in range(9):\n",
        "    ax[i//3, i%3].imshow(np.squeeze(batch[i]))\n",
        "    ax[i//3, i%3].axis(\"off\")"
      ],
      "id": "7fc9e03f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c17cb218"
      },
      "source": [
        "## **1.1 Autoencoder Básico**\n",
        "----\n",
        "\n",
        "Las imágenes con las que vamos a trabajar se representan originalmente en un espacio de dimensión $28\\times28=784$. Vamos entonces a definir un _Autoencoder_ con dos capas densas :\n",
        "\n",
        "1. Un codificador, que comprime las imágenes en un vector latente de 64 dimensiones.\n",
        "2. Un decodificador, que reconstruye la imagen original a partir de su representación en el espacio latente.\n",
        "\n",
        "Definamos primero la dimensión del espacio latente :"
      ],
      "id": "c17cb218"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bpq_0As4x7yB"
      },
      "outputs": [],
      "source": [
        "latent_dim = 64"
      ],
      "id": "bpq_0As4x7yB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67NAiboHyJx3"
      },
      "source": [
        "Ahora definimos el _Autoencoder_ como una clase a partir de dos modelos secuenciales `tf.keras.Sequential`. Lo vamos a hacer de este modo para que sea más fácil después manipular tanto el modelo completo como el _encoder_ y _decoder_ por separado:"
      ],
      "id": "67NAiboHyJx3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fb2dc635"
      },
      "outputs": [],
      "source": [
        "class Autoencoder(Model):\n",
        "  def __init__(self, latent_dim):\n",
        "    super(Autoencoder, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "    self.encoder = tf.keras.Sequential([\n",
        "                                        layers.Flatten(),\n",
        "                                        layers.Dense(latent_dim,\n",
        "                                                     activation='relu'),\n",
        "                                      ])\n",
        "    self.decoder = tf.keras.Sequential([\n",
        "                                        layers.Dense(784,\n",
        "                                                     activation='sigmoid'),\n",
        "                                        layers.Reshape((28, 28))\n",
        "                                      ])\n",
        "\n",
        "  def call(self, x):\n",
        "    encoded = self.encoder(x)\n",
        "    decoded = self.decoder(encoded)\n",
        "    return decoded"
      ],
      "id": "fb2dc635"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-Sv3s49yYjg"
      },
      "source": [
        "Entendamos el código anterior. El objeto `Autoencoder` recibe como argumento la dimensión latente `latent_dim` y construye el modelo que consta de:\n",
        "\n",
        "*   El codificador `encoder`, que primero aplana la imagen con `layers.Flatten()` y luego aplica una capa densa que tiene tantas neuronas como se indique en `latent_dim`. Esta capa tiene una activación `relu`.\n",
        "*   El decodificador `decoder`, que recibe la salida del `encoder` y usa una capa densa de 784 neuronas con activación `sigmoid` para devolver la representación latente al espacio original. La reconstrucción final de la imagen sucede con un `Reshape((28, 28))`.\n",
        "\n",
        "Con esto en mente podemos definir y nuestro modelo `autoencoder`:"
      ],
      "id": "U-Sv3s49yYjg"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S6pBb--ayZt9"
      },
      "outputs": [],
      "source": [
        "autoencoder = Autoencoder(latent_dim)"
      ],
      "id": "S6pBb--ayZt9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOPdzs_rzecz"
      },
      "source": [
        "Para compilarlo usamos un optimizador `adam` y, como se había explicado anteriormente, usamos `MeanSquaredError` como función de pérdida. En este caso, el optimizador se define con la tasa de aprendizaje por defecto: `learning_rate=0.001`."
      ],
      "id": "iOPdzs_rzecz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1b4144c"
      },
      "outputs": [],
      "source": [
        "autoencoder.compile(optimizer='adam',\n",
        "                    loss=losses.MeanSquaredError())"
      ],
      "id": "e1b4144c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "47b615ed"
      },
      "source": [
        "Entrenamos el modelo utilizando la variable `x_train` tanto como entrada como objetivo.\n",
        "\n",
        "\n",
        "> El codificador aprenderá a comprimir el conjunto de datos al espacio latente y el decodificador aprenderá a reconstruir las imágenes originales."
      ],
      "id": "47b615ed"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8a01bd8d"
      },
      "outputs": [],
      "source": [
        "# Entreamos el Autoencoder\n",
        "autoencoder.fit(x=x_train,\n",
        "                y=x_train,\n",
        "                epochs=10,\n",
        "                shuffle=True,\n",
        "                validation_data=(x_test, x_test))"
      ],
      "id": "8a01bd8d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f425bd0"
      },
      "source": [
        "Ahora podemos probar el _Autoencoder_ entrenado, codificando y decodificando imágenes del conjunto de prueba. Primero codifiquemos las imágenes usando el `encoder` del modelo `autoencoder`:"
      ],
      "id": "1f425bd0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRpm2PZs1kJX"
      },
      "outputs": [],
      "source": [
        "encoded_imgs = autoencoder.encoder(x_test).numpy()"
      ],
      "id": "wRpm2PZs1kJX"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZIjpygi1u54"
      },
      "source": [
        "Veamos la dimensión de estas `encoded_imgs`:"
      ],
      "id": "jZIjpygi1u54"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSqdVkGZ10YC"
      },
      "outputs": [],
      "source": [
        "encoded_imgs.shape"
      ],
      "id": "hSqdVkGZ10YC"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xqY2UwaF13RS"
      },
      "source": [
        "Efectivamente tenemos una representación de 64 dimensiones. Ahora decodifiquemos `encoded_imgs` usando el `decoder` :"
      ],
      "id": "xqY2UwaF13RS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69f71ca9"
      },
      "outputs": [],
      "source": [
        "decoded_imgs = autoencoder.decoder(encoded_imgs).numpy()"
      ],
      "id": "69f71ca9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlCboZ7i2EP-"
      },
      "source": [
        "Y visualicemos algunas de las imágenes originales y su respectiva reconstrucción :"
      ],
      "id": "MlCboZ7i2EP-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f000c0cf"
      },
      "outputs": [],
      "source": [
        "# Visualizar las imágenes originales y las reconstruidas\n",
        "n = 10\n",
        "plt.figure(figsize=(20, 4))\n",
        "for i in range(n):\n",
        "  # Mostrar original\n",
        "  ax = plt.subplot(2, n, i + 1)\n",
        "  plt.imshow(x_test[i])\n",
        "  plt.title(\"original\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "\n",
        "  # Mostrar reconstrucción\n",
        "  ax = plt.subplot(2, n, i + 1 + n)\n",
        "  plt.imshow(decoded_imgs[i])\n",
        "  plt.title(\"reconstruída\")\n",
        "  plt.gray()\n",
        "  ax.get_xaxis().set_visible(False)\n",
        "  ax.get_yaxis().set_visible(False)\n",
        "plt.show()"
      ],
      "id": "f000c0cf"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42a11b7f"
      },
      "source": [
        "Este modelo sirve como un reductor de dimensionalidad bastante efectivo:\n",
        "\n",
        "- Nos permite comprimir la información de cada imagen en vectores de dimensión 64 (casi 12 veces más pequeño que la dimensión original).\n",
        "\n",
        "**Sin embargo** dependiendo del contexto en el que se esté utilizando el modelo, puede ser importante que las representaciones latentes sigan ciertas propiedades estadísticas que en este caso no se cumplen. Por ejemplo, si se está utilizando un autoencoder para generar datos nuevos a partir de la representación latente, puede ser deseable que las nuevas muestras generadas sigan una distribución específica. En este caso, un autoencoder que aprende representaciones que no siguen una distribución específica puede no ser útil.\n",
        "\n",
        "Otra posible desventaja de este modelo es que las representaciones latentes pueden ser difíciles de interpretar si no siguen una distribución de probabilidad específica.\n",
        "\n",
        "- Si se desea comprender cómo el modelo está representando los datos de entrada, puede ser útil tener una idea clara de la distribución de probabilidad subyacente de la representación latente. A continuación vamos a ver cómo mejorar estas limitaciones.\n",
        "\n",
        "\n"
      ],
      "id": "42a11b7f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68cbdab0"
      },
      "source": [
        "## **1.2. Autoencoder Variacional**\n",
        "---\n",
        "\n",
        "El **_Autoencoder_ Variacional** (o _Variational autoencoder_ - VAE) es un modelo generativo basado en _Autoencoders_, el cual busca que el espacio latente represente los **parámetros de una distribución**.\n",
        "\n",
        "Lo más típico es utilizar una **distribución normal**. Para ello, el espacio latente debe representar sus parámetros, es decir, una media $\\mu$ y una desviación típica $\\mathbf{\\sigma}$ (puede ser un vector asumiendo que no hay correlación en el espacio latente).\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=1MC8JVeFj1QjGNdxJAAdoSt1koaPeXL19\" alt = \"Gráfico ilustrativo de un Autoencoder Variacional que utiliza una distribución normal \" width=\"100%\" /></center>\n",
        "\n",
        "Un VAE difiere de un _Autoencoder_ típico en dos aspectos fundamentales:\n",
        "\n",
        "* 1) La representación latente en un VAE está conformada por **dos capas** (media y desviación típica). El decodificador utiliza entonces una muestra generada con los parámetros estimados por el codificador :\n",
        "\n",
        "$$\n",
        "\\mu_i, \\sigma_i = \\text{encoder(x_i)}\\\\\n",
        "z_i \\sim {N}(\\mu_i, \\sigma_i)\\\\\n",
        "\\tilde{x}_i = \\text{decoder}(z_i)\n",
        "$$\n",
        "\n",
        "* 2) La pérdida se conforma de dos partes :\n",
        "  - Una pérdida de reconstrucción ${L}_1$ igual a la de los _Autoencoders_ típicos.\n",
        "  -  Un término de regularización ${L}_2$ el cual busca que las distribuciones generadas por el codificador se parezcan a una distribución normal estándar. Para esto usamos la divergencia de [**_Kullback-Leibler_**](https://es.wikipedia.org/wiki/Divergencia_de_Kullback-Leibler) ($\\text{KL}$).\n",
        "\n",
        "$$\n",
        "{L}= \\lambda_1{L}_1+\\lambda_2{L}_2\\\\\n",
        "{L}_1 = \\text{MSE}(x_i, \\tilde{x}_i)\\\\\n",
        "{L}_2 = \\text{KL}({N}(\\mu_i, \\sigma_i)|| {N}(0, I))\n",
        "$$\n",
        "\n",
        "La divergencia de **_Kullback-Leibler_** es una medida que nos permite ver qué tan parecidas son dos distribuciones. Formalmente, dadas dos distribuciones de variable discreta $P$ y $Q$, la divergencia de Kullback-Leiber está dada por:\n",
        "\n",
        "$$\n",
        "D_{KL}=\\sum_i P(i)\\ln\\dfrac{P(i)}{Q(i)}.\n",
        "$$\n",
        "\n",
        "En nuestro caso, lo que buscamos que el espacio latente aproxime una distribución normal multivariada con media 0 y desviación 1.\n"
      ],
      "id": "68cbdab0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fzdARF2mo2W"
      },
      "source": [
        "- Comencemos con el ejemplo práctico: Definamos la función `preprocess_images` que escala la imagen en el rango $(0,1)$ (dividiendo por $255$) y binariza completamente la imagen haciendo que los pixeles tomen valor `0.0` o `1.0`:"
      ],
      "id": "4fzdARF2mo2W"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "866f70e5"
      },
      "outputs": [],
      "source": [
        "def preprocess_images(images):\n",
        "  images = images.reshape((images.shape[0], 28, 28, 1)) / 255.\n",
        "  return np.where(images > .5, 1.0, 0.0).astype('float32')\n",
        "\n",
        "train_images = preprocess_images(train_images)\n",
        "test_images = preprocess_images(test_images)"
      ],
      "id": "866f70e5"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_j-bKaiDI_8"
      },
      "source": [
        "Definimos variables para la creación de dataset :"
      ],
      "id": "L_j-bKaiDI_8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f1d33cf3"
      },
      "outputs": [],
      "source": [
        "train_size = 60000\n",
        "batch_size = 32\n",
        "test_size = 10000"
      ],
      "id": "f1d33cf3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY1b77HtDMJ4"
      },
      "source": [
        "Y usamos `tf.data.Dataset` para crear _Batches_ y mezclar los datos. Recordemos que la API `tf.data.Dataset` permite escribir cadenas de entrada descriptivas y eficientes. El uso de conjuntos de datos sigue un patrón común :\n",
        "\n",
        "- Crear un conjunto de datos de origen a partir de los datos de entrada.\n",
        "- Aplicar transformaciones al conjunto de datos para preprocesar los datos.\n",
        "- Iterar sobre el conjunto de datos y procesar los elementos.\n",
        "\n",
        "La iteración se produce de **forma continua**, por lo que no es necesario que el conjunto de datos completo quepa en la memoria; es algo parecido a lo que hacíamos con el `ImageDataGenerator` de _Keras_.\n"
      ],
      "id": "QY1b77HtDMJ4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9d4cd471"
      },
      "outputs": [],
      "source": [
        "train_dataset = (tf.data.Dataset.from_tensor_slices(train_images)\n",
        "                 .shuffle(buffer_size=train_size)\n",
        "                 .batch(batch_size))\n",
        "test_dataset = (tf.data.Dataset.from_tensor_slices(test_images)\n",
        "                .shuffle(buffer_size=test_size)\n",
        "                .batch(batch_size))"
      ],
      "id": "9d4cd471"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7rupamzD3ld"
      },
      "source": [
        "> **Función `shuffle`**\n",
        "El código anterior usamos la función `from_tensor_slices` que crea _Batches_ de `train_images` o `test_images` de tamaño `batch_size` después de haber barajado los datos con `shuffle()`. La función `shuffle()` es una operación que reorganiza aleatoriamente los elementos de un dataset. Se utiliza para asegurar que los modelos no aprendan patrones indeseables de los datos debido al orden en que se presentan las muestras durante el entrenamiento.\n",
        "Cuando se llama a `shuffle(buffer_size)` en un dataset, _TensorFlow_ crea un buffer temporal de tamaño `buffer_size`. A medida que se solicitan elementos del dataset, el siguiente elemento se selecciona aleatoriamente del buffer y se reemplaza con un elemento nuevo del dataset. Esto crea una mezcla aleatoria de los elementos en el dataset.\n",
        "Un aspecto importante para tener en cuenta es el tamaño del buffer. El tamaño del buffer afecta la aleatoriedad de la mezcla:\n",
        "* `buffer_size` igual al tamaño del dataset: si el tamaño del buffer es igual al tamaño del dataset, la mezcla será perfectamente aleatoria. Todos los elementos del dataset se cargan en el buffer, y cada elemento tiene igual probabilidad de ser seleccionado en cada iteración. Sin embargo, esto puede tener un impacto en la memoria y el rendimiento, especialmente cuando se trabaja con grandes datasets.\n",
        "* `buffer_size` menor que el tamaño del dataset: si el tamaño del buffer es menor que el tamaño del dataset, la mezcla será aproximadamente aleatoria. En este caso, se cargan menos elementos en el buffer, lo que reduce el costo computacional y de memoria. Sin embargo, la aleatoriedad será menos precisa, lo que puede afectar la calidad del entrenamiento del modelo.\n",
        "\n"
      ],
      "id": "Q7rupamzD3ld"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9d674a6d"
      },
      "source": [
        "Ahora definimos un _Autoencoder_. Vamos a hacer algo similar al ejemplo anterior, pero esta vez utilizando convoluciones:"
      ],
      "id": "9d674a6d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfe094f6"
      },
      "outputs": [],
      "source": [
        "class CVAE(tf.keras.Model):\n",
        "  \"\"\"Convolutional variational autoencoder.\"\"\"\n",
        "\n",
        "  def __init__(self, latent_dim):\n",
        "    super(CVAE, self).__init__()\n",
        "    self.latent_dim = latent_dim\n",
        "\n",
        "    # Definimos el encoder\n",
        "    self.encoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(28, 28, 1)),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                                   filters=32,\n",
        "                                   kernel_size=3,\n",
        "                                   strides=(2, 2),\n",
        "                                   activation='relu'),\n",
        "            tf.keras.layers.Conv2D(\n",
        "                                   filters=64,\n",
        "                                   kernel_size=3,\n",
        "                                   strides=(2, 2),\n",
        "                                   activation='relu'),\n",
        "            tf.keras.layers.Flatten(),\n",
        "            # No activation\n",
        "            tf.keras.layers.Dense(latent_dim + latent_dim),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    # Definimos el encoder\n",
        "    self.decoder = tf.keras.Sequential(\n",
        "        [\n",
        "            tf.keras.layers.InputLayer(shape=(latent_dim,)),\n",
        "            tf.keras.layers.Dense(units=7*7*32,\n",
        "                                  activation=tf.nn.relu),\n",
        "            tf.keras.layers.Reshape(target_shape=(7, 7, 32)),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                                            filters=64,\n",
        "                                            kernel_size=3,\n",
        "                                            strides=2,\n",
        "                                            padding='same',\n",
        "                                            activation='relu'),\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                                            filters=32,\n",
        "                                            kernel_size=3,\n",
        "                                            strides=2,\n",
        "                                            padding='same',\n",
        "                                            activation='relu'),\n",
        "            # No activation\n",
        "            tf.keras.layers.Conv2DTranspose(\n",
        "                                            filters=1,\n",
        "                                            kernel_size=3,\n",
        "                                            strides=1,\n",
        "                                            padding='same'),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "  # Función para generar una muestra\n",
        "  @tf.function\n",
        "  def sample(self, eps=None):\n",
        "    \"\"\"\n",
        "     Esta función se encarga de generar muestras nuevas a partir de un espacio\n",
        "     latente. Si no se proporciona un tensor eps, se crea uno con forma\n",
        "     (100, latent_dim) y distribución normal. Luego, pasa eps a la función\n",
        "     decode() y retorna el resultado.\n",
        "     \"\"\"\n",
        "    if eps is None:\n",
        "      eps = tf.random.normal(shape=(100, self.latent_dim))\n",
        "    return self.decode(eps, apply_sigmoid=True)\n",
        "\n",
        "  # Función para codificar con el encoder\n",
        "  def encode(self, x):\n",
        "    \"\"\"\n",
        "     Esta función recibe datos de entrada x y los pasa a través del codificador.\n",
        "     El codificador devuelve dos valores: la media y la varianza logarítmica\n",
        "     (logvar) de la distribución latente. La función tf.split() se utiliza para\n",
        "     dividir el tensor de salida del codificador en dos partes iguales,\n",
        "     representando la media y la varianza logarítmica.\n",
        "    \"\"\"\n",
        "    mean, logvar = tf.split(self.encoder(x),\n",
        "                            num_or_size_splits=2,\n",
        "                            axis=1)\n",
        "    return mean, logvar\n",
        "\n",
        "  # Función para hacer un muestreo a partir de una media y varianza\n",
        "  def reparameterize(self, mean, logvar):\n",
        "    \"\"\"\n",
        "     Esta función toma la media y la varianza logarítmica y realiza el truco de\n",
        "     la reparametrización. Esto implica muestrear un tensor eps usando una\n",
        "     distribución normal y luego calcular\n",
        "     eps * tf.exp(logvar * .5) + mean.\n",
        "     El resultado es una representación latente que se puede pasar al\n",
        "     decodificador.\n",
        "    \"\"\"\n",
        "    eps = tf.random.normal(shape=mean.shape)\n",
        "    return eps * tf.exp(logvar * .5) + mean\n",
        "\n",
        "  # Función para decodificar con el decoder\n",
        "  def decode(self, z, apply_sigmoid=False):\n",
        "    \"\"\"\n",
        "    Esta función toma la representación latente z y la pasa a través del\n",
        "    decodificador para generar la reconstrucción de los datos de entrada.\n",
        "    Si apply_sigmoid es True, se aplica la función sigmoide a los logits del\n",
        "    decodificador y se devuelve la probabilidad resultante. Si apply_sigmoid es\n",
        "    False, la función devuelve los logits directamente.\n",
        "    \"\"\"\n",
        "    logits = self.decoder(z)\n",
        "    if apply_sigmoid:\n",
        "      probs = tf.sigmoid(logits)\n",
        "      return probs\n",
        "    return logits"
      ],
      "id": "cfe094f6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOee2t_HvaRQ"
      },
      "source": [
        "**Entendamos el modelo**\n",
        "\n",
        "De nuevo, tenemos dos bloques importantes, el _encoder_ y el _decoder_:\n",
        "\n",
        "* _Encoder_: aplica dos capas convolucionales `Conv2D` de 64 y 32 filtros respectivamente. Luego aplica una capa `Flatten` para aplanar la salida de la última convolución pasar el resultado por una capa densa de `latent_dim + latent_dim` neuronas. La primera mitad de este vector de salida corresponde al vector de media $\\mu_i$ y la segunda parte al vector de desviaciones  $\\sigma_i$.\n",
        "* _Decoder_: este bloque debe convertir una representación vectorial en una imagen. Es decir, debe hacer una escalada de dimensiones. Para esto usa una capa llamada `Conv2DTranspose`, que cumple la función inversa de `Conv2D`.\n",
        "\n",
        "### **1.2.1. Convolución transpuesta**\n",
        "---\n",
        "\n",
        "También conocida como deconvolución, inversa o transpuesta de una convolución, es una operación matemática que se utiliza para invertir una convolución. La capa `Conv2DTranspose` aplica esta operación a una entrada 2D (generalmente una representación latente) y produce una salida 2D (una imagen generada).\n",
        "\n",
        "La capa `Conv2DTranspose` es similar a la capa `Conv2D` en su estructura. Tiene varios parámetros que se pueden ajustar para controlar su comportamiento y su rendimiento. A continuación, se describen los parámetros más importantes:\n",
        "\n",
        "*    `filters`: número entero que representa la cantidad de filtros (también conocidos como kernels) que se utilizan en la capa. Cada filtro es una matriz 2D que se aplica a la entrada para extraer características específicas.\n",
        "\n",
        "*    `kernel_size`: tamaño de la ventana de convolución en la capa. Es un número entero o una tupla de dos enteros que especifica la altura y el ancho de la ventana de convolución.\n",
        "\n",
        "*    `strides`: número entero o tupla de dos enteros que especifica el desplazamiento de la ventana de convolución en la dirección horizontal y vertical.\n",
        "\n",
        "*    `padding`: cadena que puede ser `valid` o `same`. `valid` significa que no se agrega relleno a la entrada y `same` significa que se agrega suficiente relleno para que la salida tenga la misma forma que la entrada.\n",
        "\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=10P8Oci9m3h04NuiyHhLHJrvYalNBew0i\" alt = \"Gráfico ilustrativo de un ejemplo de convolución transpuesta \"  width=\"100%\" /></center>\n"
      ],
      "id": "pOee2t_HvaRQ"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c6aa912a"
      },
      "source": [
        "### **1.2.2. Función de pérdida**\n",
        "---\n",
        "\n",
        "Como siempre, definimos la función de pérdida y el optimizador, teniendo en cuenta lo que explicamos anteriormente sobre aproximar los parámetros de una distribución normal:"
      ],
      "id": "c6aa912a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OFvIpc-vkAOI"
      },
      "outputs": [],
      "source": [
        "# El optimizador\n",
        "optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "\n",
        "def log_normal_pdf(sample, mean, logvar, raxis=1):\n",
        "  \"\"\"\n",
        "  Esta función calcula la probabilidad logarítmica de una distribución normal\n",
        "  para un conjunto de muestras sample con una media mean y varianza logarítmica\n",
        "  logvar. raxis es el eje a lo largo del cual se realiza la suma. La función\n",
        "  devuelve la suma de las probabilidades logarítmicas de las muestras.\n",
        "  \"\"\"\n",
        "  log2pi = tf.math.log(2. * np.pi)\n",
        "  return tf.reduce_sum(\n",
        "      -.5 * ((sample - mean) ** 2. * tf.exp(-logvar) + logvar + log2pi),\n",
        "      axis=raxis)"
      ],
      "id": "OFvIpc-vkAOI"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CntPUuuxkCX0"
      },
      "source": [
        "La siguiente función, `compute_loss(model, x)`, calcula la función de pérdida, dada una instancia del modelo y un _batch_ de datos de entrada `x`. La función de pérdida en un VAE consta de dos partes: la divergencia de Kullback-Leibler (KL) entre las distribuciones latentes y la entropía cruzada entre los datos de entrada y su reconstrucción.\n",
        "\n",
        "   - Primero, se calculan la media y la varianza logarítmica de la distribución latente utilizando la función `encode()`.\n",
        "   - Luego, se muestrea una representación latente `z` utilizando la función `reparameterize()`.\n",
        "   - A continuación, se decodifica `z` utilizando la función `decode()` para obtener la reconstrucción `x_logit`.\n",
        "   - Se calcula la entropía cruzada entre los logits `x_logit` y los datos de entrada `x` usando `tf.nn.sigmoid_cross_entropy_with_logits()`. La suma de esta entropía cruzada a lo largo de los ejes [1, 2, 3] se almacena en `logpx_z`.\n",
        "   - Se calcula la divergencia KL utilizando la función `log_normal_pdf()` para `logpz` (probabilidad logarítmica de la distribución latente) y `logqz_x` (probabilidad logarítmica de la distribución latente condicional a `x`).\n",
        "   - Finalmente, se devuelve el negativo de la media de la suma de `logpx_z`, `logpz`, y `-logqz_x`, que es la función de pérdida del VAE."
      ],
      "id": "CntPUuuxkCX0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "605e0e39"
      },
      "outputs": [],
      "source": [
        "def compute_loss(model, x):\n",
        "  mean, logvar = model.encode(x)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  x_logit = model.decode(z)\n",
        "  cross_ent = tf.nn.sigmoid_cross_entropy_with_logits(logits=x_logit,\n",
        "                                                      labels=x)\n",
        "  logpx_z = -tf.reduce_sum(cross_ent,\n",
        "                           axis=[1, 2, 3])\n",
        "  logpz = log_normal_pdf(z, 0., 0.)\n",
        "  logqz_x = log_normal_pdf(z,\n",
        "                           mean,\n",
        "                           logvar)\n",
        "  return -tf.reduce_mean(logpx_z + logpz - logqz_x)\n",
        "\n",
        "# Definimos esta función para el entrenamento como lo hacíamos al principio\n",
        "# del curso usando tf.GradientTape() para tener regístro de todos las etapas\n",
        "# del entrenamiento\n",
        "\n",
        "@tf.function\n",
        "def train_step(model, x, optimizer):\n",
        "  \"\"\"\n",
        "  Esta función calcula la pérdida y los gradientes, y los utiliza para\n",
        "  actualizar los parámetros del modelo.\n",
        "  \"\"\"\n",
        "  with tf.GradientTape() as tape:\n",
        "    loss = compute_loss(model, x)\n",
        "  gradients = tape.gradient(loss, model.trainable_variables)\n",
        "  optimizer.apply_gradients(zip(gradients, model.trainable_variables))"
      ],
      "id": "605e0e39"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f987e9db"
      },
      "source": [
        "Y a continuación definimos algunos parámetros para el entrenamiento y definimos el modelo. Entrenaremos durante 10 _epochs_ y vamos a fijar la dimensión latente en 2, es decir, en el espacio latente, cada imagen estará representada por dos dimensiones."
      ],
      "id": "f987e9db"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhWGGpGklzJ8"
      },
      "outputs": [],
      "source": [
        "epochs = 10\n",
        "# Definimos el tamaño del espacio latente de manera que pueda ser visualizado\n",
        "# posteriormente.\n",
        "latent_dim = 2\n",
        "# Definimos el modelo\n",
        "model = CVAE(latent_dim)"
      ],
      "id": "hhWGGpGklzJ8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5f868f72"
      },
      "source": [
        "### **1.2.3. *Generación* de muestras**\n",
        "---\n",
        "\n",
        "La generación de imágenes con un VAE es muy sencilla, especialmente, porque la pérdida busca que el espacio latente tenga **una distribución normal estándar**. Para obtener vectores válidos en dicho espacio basta con generar un vector de esta distribución.\n",
        "\n",
        "> Una característica interesante de las representaciones latentes aprendidas con _Variational Autoencoders_ es que las características latentes aprendidas generalmente están decorrelacionadas. Esto es consecuencia de que la función de pérdida busca que la covarianza de las características latentes sea la identidad. Como resultado, las características latentes aprendidas generalmente **codifican un aspecto en particular** de los datos.\n",
        "\n",
        "\n",
        "Veamos un ejemplo. Definamos primero una función para generar imágenes con el modelo:"
      ],
      "id": "5f868f72"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3f1b43e"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_sample):\n",
        "  mean, logvar = model.encode(test_sample)\n",
        "  z = model.reparameterize(mean, logvar)\n",
        "  predictions = model.sample(z)\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "    plt.subplot(4, 4, i + 1)\n",
        "    plt.imshow(predictions[i, :, :, 0],\n",
        "               cmap='gray')\n",
        "    plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "id": "a3f1b43e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5UcgZtLft7DJ"
      },
      "source": [
        "Elegimos una muestra del conjunto de prueba para generar imágenes de salida:"
      ],
      "id": "5UcgZtLft7DJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dd228c9"
      },
      "outputs": [],
      "source": [
        "num_examples_to_generate = 16\n",
        "assert batch_size >= num_examples_to_generate\n",
        "for test_batch in test_dataset.take(1):\n",
        "  test_sample = test_batch[0:num_examples_to_generate, :, :, :]"
      ],
      "id": "9dd228c9"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tI3YnGBXuEpW"
      },
      "source": [
        "Y generamos las imágenes a medida que entrenamos en modelo:"
      ],
      "id": "tI3YnGBXuEpW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f9dbf72d"
      },
      "outputs": [],
      "source": [
        "generate_and_save_images(model, 0, test_sample)\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "  start_time = time.time()\n",
        "  for train_x in train_dataset:\n",
        "    train_step(model,\n",
        "               train_x,\n",
        "               optimizer)\n",
        "  end_time = time.time()\n",
        "\n",
        "  loss = tf.keras.metrics.Mean()\n",
        "  for test_x in test_dataset:\n",
        "    loss(compute_loss(model,\n",
        "                      test_x))\n",
        "  elbo = -loss.result()\n",
        "  display.clear_output(wait=False)\n",
        "  print('Epoch: {}, Test set ELBO: {}, time elapse for current epoch: {}'\n",
        "        .format(epoch, elbo, end_time - start_time))\n",
        "  generate_and_save_images(model,\n",
        "                           epoch,\n",
        "                           test_sample)"
      ],
      "id": "f9dbf72d"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c619777"
      },
      "source": [
        "Mostramos una imagen generada de la última época de entrenamiento."
      ],
      "id": "0c619777"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "de1c3163"
      },
      "outputs": [],
      "source": [
        "def display_image(epoch_no):\n",
        "  return PIL.Image.open('image_at_epoch_{:04d}.png'.format(epoch_no))"
      ],
      "id": "de1c3163"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7206cf0a"
      },
      "outputs": [],
      "source": [
        "plt.imshow(display_image(epoch))\n",
        "plt.axis('off')"
      ],
      "id": "7206cf0a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "71b19352"
      },
      "source": [
        "\n",
        "Y mostramos un GIF animado de todas las imágenes guardadas."
      ],
      "id": "71b19352"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e718395c"
      },
      "outputs": [],
      "source": [
        "anim_file = 'cvae.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "id": "e718395c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1a1d7b95"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "id": "1a1d7b95"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a4a4b22"
      },
      "source": [
        "### **1.2.4. Representación de reducción de dimensionalidad en 2D a partir del espacio latente**\n",
        "---\n",
        "\n",
        "Como la dimensión latente del modelo es 2, podemos muestrear datos desde un rectángulo en un plano cartesiano. Lo que se espera es que la variación continua de los parámetros de la distribución debe generar variaciones continuas en las imágenes generadas.\n",
        "\n",
        "La siguiente función, `plot_latent_images(model, n, digit_size=28)`, sirve para visualizar imágenes generadas por el modelo VAE a partir del espacio latente en una cuadrícula de `n` x `n`. Mas en detalle, la función hace lo siguiente:\n",
        "\n",
        "1. Crea una distribución normal usando `tfp.distributions.Normal(0, 1)`. Esta distribución se utilizará para generar los puntos en el espacio latente a partir de los cuales se decodificarán las imágenes.\n",
        "\n",
        "2. Se generan dos secuencias linealmente espaciadas de tamaño `n` entre los cuantiles 0.05 y 0.95 de la distribución normal, llamadas `grid_x` y `grid_y`.\n",
        "\n",
        "3. Se inicializa una imagen en blanco de tamaño `image_height` x `image_width`, donde ambos son iguales a `digit_size * n`.\n",
        "\n",
        "4. Se itera sobre los elementos de `grid_x` y `grid_y` y se crea un tensor `z` de dimensión (1, 2) con las coordenadas `(xi, yi)`. Este tensor se pasa a la función `model.sample(z)` para decodificar una imagen a partir del espacio latente.\n",
        "\n",
        "5. La imagen decodificada se redimensiona a las dimensiones `(digit_size, digit_size)` y se coloca en la posición correspondiente dentro de la imagen en blanco.\n",
        "\n",
        "6. Una vez que se han generado todas las imágenes y se han colocado en la imagen en blanco, se utiliza `matplotlib` para visualizar la imagen final. Se configura el tamaño de la figura, se muestra la imagen en escala de grises y se ocultan los ejes.\n",
        "\n",
        "El resultado es una cuadrícula `n` x `n` que muestra las imágenes generadas por el VAE a partir de diferentes puntos en el espacio latente.\n"
      ],
      "id": "6a4a4b22"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bade22bd"
      },
      "outputs": [],
      "source": [
        "def plot_latent_images(model, n, digit_size=28):\n",
        "  \"\"\"Muestra imágenes de n x n de dígitos decodificados del espacio latente.\"\"\"\n",
        "\n",
        "  norm = tfp.distributions.Normal(0, 1)\n",
        "  grid_x = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "  grid_y = norm.quantile(np.linspace(0.05, 0.95, n))\n",
        "  image_width = digit_size*n\n",
        "  image_height = image_width\n",
        "  image = np.zeros((image_height, image_width))\n",
        "\n",
        "  for i, yi in enumerate(grid_x):\n",
        "    for j, xi in enumerate(grid_y):\n",
        "      z = np.array([[xi, yi]])\n",
        "      x_decoded = model.sample(z)\n",
        "      digit = tf.reshape(x_decoded[0], (digit_size, digit_size))\n",
        "      image[i * digit_size: (i + 1) * digit_size,\n",
        "            j * digit_size: (j + 1) * digit_size] = digit.numpy()\n",
        "\n",
        "  plt.figure(figsize=(10, 10))\n",
        "  plt.imshow(image, cmap='Greys_r')\n",
        "  plt.axis('Off')\n",
        "  plt.show()"
      ],
      "id": "bade22bd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "982d01a6"
      },
      "outputs": [],
      "source": [
        "plot_latent_images(model, 20)"
      ],
      "id": "982d01a6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da5415dc"
      },
      "source": [
        "# **2. Redes Generativas Adversarias**\n",
        "---\n",
        "Las **redes generativas adversarias** (_Generative Adversarial Networks_ - GAN) son otro tipo de modelos generativos que utilizan redes neuronales. Una GAN rompe el esquema tradicional de _Encoder-Decoder_ y propone dos componentes :\n",
        "\n",
        "* **Generador**: Se trata de una arquitectura parecida a un _Decoder_, cuya función es transformar vectores aleatorios al espacio de las imágenes.\n",
        "* **Discriminador**: Se trata de una arquitectura que buscará identificar si una imagen generada es real o falsa.\n",
        "\n",
        "En otros términos, una GAN se compone de dos modelos que están compitiendo constantemente.\n",
        "\n",
        "- Por un lado, el generador busca mejorar sus predicciones para engañar al discriminador.\n",
        "- Por otro, el discriminador busca ser más preciso para detectar qué imágenes son producidas artificialmente en el generador y qué imágenes son reales (del dataset).\n",
        "\n",
        "<center><img src=\"https://drive.google.com/uc?export=view&id=12vk0rW6I88WVoPaBq55SOrKRwY3FUJ1u\" alt = \"Gráfico ilustrativo de una red generativa adversaria \" width=\"100%\" /></center>\n",
        "\n",
        "Veamos el funcionamiento de una GAN. Utilizaremos el dataset MNIST para entrenar el generador y el discriminador. Buscamos que el generador provea dígitos escritos a mano que se parezcan a los pertenecientes a la base de datos MNIST.\n"
      ],
      "id": "da5415dc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9559cc64"
      },
      "outputs": [],
      "source": [
        "# Cargamos y preparamos los datos\n",
        "(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()"
      ],
      "id": "9559cc64"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "efe91384"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]"
      ],
      "id": "efe91384"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "780cf851"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 60000\n",
        "BATCH_SIZE = 256\n",
        "\n",
        "# Generamos un dataset por Batch y combinamos los datos\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)"
      ],
      "id": "780cf851"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1f9d2c34"
      },
      "source": [
        "## **2.1 Definición del modelo**\n",
        "---\n",
        "**Generador**\n",
        "\n",
        "El generador utiliza capas `tf.keras.layers.Conv2DTranspose` (upsampling) para producir una imagen a partir de una semilla (ruido aleatorio, en este caso de tamaño 100). El generador inicia con una capa densa que tome esta semilla como entrada, luego muestrea varias veces hasta que se alcanza la imagen deseada, en este caso de tamaño 28x28x1.\n",
        "\n",
        "- Es importante anotar que cada capa utiliza una activación de tipo `tf.keras.layers.LeakyReLU`, excepto por la capa de salida que usa una activación de tipo `tanh`.\n",
        "\n",
        "La capa `LeakyReLU` es una variante de la función de activación `ReLU` que se caracteriza por permitir valores negativos, lo que puede ayudar a evitar el problema de \"neuronas muertas\" en las capas convolucionales de la red. Además, `LeakyReLU` introduce un pequeño gradiente negativo en los valores de entrada negativos, lo que puede ayudar a evitar problemas de saturación y mejorar la capacidad del modelo para aprender características sutiles de las muestras.\n",
        "\n",
        "En modelos tipo GAN, el uso de la capa `LeakyReLU` puede ayudar a mejorar el rendimiento del discriminador al permitir que este aprenda características más sutiles de las muestras reales y generadas, lo que puede hacer que el modelo sea más efectivo para distinguir entre muestras auténticas y generadas.\n",
        "\n",
        "- Además, `LeakyReLU` puede ayudar a estabilizar el proceso de entrenamiento de la GAN, reduciendo la probabilidad de que el modelo se atasque en un óptimo local o diverja.\n"
      ],
      "id": "1f9d2c34"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b103d284"
      },
      "outputs": [],
      "source": [
        "def make_generator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(shape=(100,)))\n",
        "    model.add(layers.Dense(7*7*256, use_bias=False))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((7, 7, 256)))\n",
        "    assert model.output_shape == (None, 7, 7, 256)  # Note: None is the batch size\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(128, (5, 5), strides=(1, 1), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 7, 7, 128)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(64, (5, 5), strides=(2, 2), padding='same', use_bias=False))\n",
        "    assert model.output_shape == (None, 14, 14, 64)\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=False, activation='tanh'))\n",
        "    assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "    return model"
      ],
      "id": "b103d284"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKfQE5fRKeYZ"
      },
      "source": [
        "Podemos observar una imagen obtenida con el generador que aún no ha sido entrenada. Creamos el modelo `generator` y le damos como entrada un vector aleatorio muestreado de una distribución normal:"
      ],
      "id": "wKfQE5fRKeYZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcc4adad"
      },
      "outputs": [],
      "source": [
        "generator = make_generator_model()\n",
        "\n",
        "noise = tf.random.normal([1, 100])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ],
      "id": "fcc4adad"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YY9gTqtSO9Zr"
      },
      "source": [
        "Como era de esperar, la imagen no tiene ningún sentido."
      ],
      "id": "YY9gTqtSO9Zr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7a69f61b"
      },
      "source": [
        "**Discriminador**\n",
        "\n",
        "\n",
        "Implementamos el discriminador como un clasificador de imágenes basado en CNN. Este modelo debe resolver una tarea binaria: decidir si la imagen que recibe es real o falsa:"
      ],
      "id": "7a69f61b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "65fd55b2"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model():\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.InputLayer(shape=(28, 28, 1)))\n",
        "    model.add(layers.Conv2D(64, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Conv2D(128, (5, 5), strides=(2, 2), padding='same'))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(layers.Dense(1))\n",
        "\n",
        "    return model"
      ],
      "id": "65fd55b2"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "157e3303"
      },
      "source": [
        "Como ejemplo, podemos utilizar el discriminador que aún no ha sido entrenado para clasificar la imagen generada anteriormente. Creamos entonces el modelo `discriminator`. El modelo genera valores positivos para imágenes reales y valores negativos para imágenes falsas."
      ],
      "id": "157e3303"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfa24773"
      },
      "outputs": [],
      "source": [
        "discriminator = make_discriminator_model()\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ],
      "id": "bfa24773"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "863191f3"
      },
      "source": [
        "## **2.2 Función de pérdida y optimizador**\n",
        "---\n"
      ],
      "id": "863191f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9784fb67"
      },
      "source": [
        "**Pérdida del discriminador**\n",
        "\n",
        "Debemos cuantificar qué tan bien el discriminador es capaz de distinguir imágenes reales de falsificaciones. Usamos un `BinaryCrossentropy` que debe comparar las predicciones del discriminador en imágenes reales con una matriz de unos y las predicciones del discriminador en imágenes falsas (generadas) con una matriz de ceros:"
      ],
      "id": "9784fb67"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d31cb50f"
      },
      "outputs": [],
      "source": [
        "# Cálculo de la pérdida Binary Cross Entropy\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "    total_loss = real_loss + fake_loss\n",
        "    return total_loss"
      ],
      "id": "d31cb50f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "244fff80"
      },
      "source": [
        "**Pérdida del generador**\n",
        "\n",
        "\n",
        "La pérdida del generador **cuantifica qué tan bien se pudo engañar al discriminador**. Intuitivamente, si el generador está funcionando bien, el discriminador clasificará las imágenes falsas como reales (1). Para ello se compara la decisión del discriminador sobre las imágenes generadas con una matriz de 1.\n"
      ],
      "id": "244fff80"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5493593f"
      },
      "outputs": [],
      "source": [
        "def generator_loss(fake_output):\n",
        "    return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ],
      "id": "5493593f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "952281e5"
      },
      "source": [
        "\n",
        "El discriminador y los optimizadores del generador son diferentes ya que se entrenarán dos redes por separado. En este caso usamos `Adam` con una tasa de aprendizaje de `1e-4`:"
      ],
      "id": "952281e5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8dcce9ca"
      },
      "outputs": [],
      "source": [
        "generator_optimizer = tf.keras.optimizers.Adam(1e-4)\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(1e-4)"
      ],
      "id": "8dcce9ca"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d8cc1d91"
      },
      "source": [
        "**Definición de `Checkpoints`**\n",
        "\n",
        "Vamos a usar `tf.train.Checkpoint`, que permite guardar los valores de los pesos y otros parámetros del modelo en un momento dado durante el entrenamiento. Esto es útil porque el entrenamiento puede llevar mucho tiempo y se pueden necesitar varias iteraciones para llegar a un modelo entrenado que funcione bien.\n",
        "\n",
        "- Si el entrenamiento se interrumpe, ya sea de manera intencional o no, se pueden perder los valores de los pesos y otros parámetros que se han ajustado hasta ese momento.\n",
        "\n",
        "Con `tf.train.Checkpoint`, se pueden guardar los valores de los pesos y otros parámetros de un modelo de manera regular durante el entrenamiento, y restaurarlos más tarde si es necesario. Esto permite reanudar el entrenamiento desde el punto en que se interrumpió o utilizar el modelo entrenado para hacer predicciones en datos nuevos."
      ],
      "id": "d8cc1d91"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2e8f2df6"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)"
      ],
      "id": "2e8f2df6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8777ad4c"
      },
      "source": [
        "**Definición de ciclo de entrenamiento**\n",
        "\n",
        "Definimos algunos parámetros necesarios, como el número de `EPOCHS`, la dimensión del vector de ruido que ingresa al generador, y el número de imágenes  generar en cada iteración:"
      ],
      "id": "8777ad4c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "18973256"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 50\n",
        "noise_dim = 100\n",
        "num_examples_to_generate = 16\n",
        "# Semilla que será utilizada para poder observar el progreso\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ],
      "id": "18973256"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddcc26f6"
      },
      "source": [
        "El ciclo de entrenamiento comienza con el generador que recibe una **semilla aleatoria como entrada**. Esa semilla se usa para producir una imagen. Luego, el discriminador se usa para clasificar imágenes reales (extraídas del conjunto de entrenamiento) e imágenes falsas (producidas por el generador).\n",
        "> La pérdida se calcula para cada uno de estos modelos y los gradientes se utilizan para actualizar el generador y el discriminador.\n",
        "\n",
        "Definimos entonces la iteración del entrenamiento como una función `train_step`:"
      ],
      "id": "ddcc26f6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d8990ce"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))"
      ],
      "id": "0d8990ce"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQ5YiZvFQvEq"
      },
      "source": [
        "Y la función `trian` ejecutará el entrenamiento definido en `train_step` según el número de `epochs`. Durante el entrenamiento, además, vamos a guardar los resultados intermedios para visualizar posteriormente el comportamiento del modelo."
      ],
      "id": "XQ5YiZvFQvEq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89c4a80f"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs):\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      train_step(image_batch)\n",
        "\n",
        "    # Produce imágenes para luego construir un GIF\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Guarda el modelo cada 15 epocas\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Genera y guarda imágenes despúes de la última época\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator,\n",
        "                           epochs,\n",
        "                           seed)"
      ],
      "id": "89c4a80f"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4u6mLvMjREd6"
      },
      "source": [
        "La siguiente función genera y guarda las imágenes generadas por el modelo:"
      ],
      "id": "4u6mLvMjREd6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2f0e95f3"
      },
      "outputs": [],
      "source": [
        "# Generar y guardar imágenes\n",
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # `training` es fijado como False para hacer inferencia.\n",
        "\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap='gray')\n",
        "      plt.axis('off')\n",
        "\n",
        "  plt.savefig('image_at_epoch_{:04d}.png'.format(epoch))\n",
        "  plt.show()"
      ],
      "id": "2f0e95f3"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsWV-YWYRPPw"
      },
      "source": [
        "Y entonces podemos entrenar todo."
      ],
      "id": "RsWV-YWYRPPw"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9af4ad2a"
      },
      "source": [
        "## **2.3 Entrenamiento del modelo**\n",
        "---\n",
        "\n",
        "Con el método `train ()` definido anteriormente es posible entrenar el generador y el discriminador simultáneamente.\n",
        "\n",
        "> Se debe tener en cuenta que entrenar GAN puede ser complicado. Es importante que el generador y el discriminador **no se dominen entre sí** (por ejemplo, que entrenan a un ritmo similar).\n",
        "\n",
        "Al comienzo del entrenamiento, las imágenes generadas parecen ruido aleatorio. A medida que avanza el entrenamiento, los dígitos generados se verán cada vez más reales. Después de aproximadamente 50 épocas, se parecerán a los dígitos MNIST.\n",
        "\n",
        "Entrenemos:"
      ],
      "id": "9af4ad2a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15203270"
      },
      "outputs": [],
      "source": [
        "train(train_dataset, EPOCHS)"
      ],
      "id": "15203270"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYgUm6K7Qqoe"
      },
      "source": [
        "Y usemos el `checkpoint` para restaurar los pesos del modelo:"
      ],
      "id": "OYgUm6K7Qqoe"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e1a7cde6"
      },
      "outputs": [],
      "source": [
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "id": "e1a7cde6"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be87ce8d"
      },
      "source": [
        "**Creamos un GIF**\n",
        "\n",
        "Como hemos guardado las imágenes que puede generar el modelo después de cada _epoch_, podemos visualizar la evolución de la calidad del proceso generativo.\n",
        "\n",
        "Primero, veamos lo que puede generar el modelo después de 50 epochs de entrenamiento:"
      ],
      "id": "be87ce8d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "451a5960"
      },
      "outputs": [],
      "source": [
        "PIL.Image.open('image_at_epoch_0050.png')"
      ],
      "id": "451a5960"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d5b99525"
      },
      "source": [
        "\n",
        "Ahora, utilizamos _imageio_ para crear un gif animado con las imágenes guardadas durante el entrenamiento."
      ],
      "id": "d5b99525"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3b3bca56"
      },
      "outputs": [],
      "source": [
        "anim_file = 'dcgan.gif'\n",
        "\n",
        "with imageio.get_writer(anim_file, mode='I') as writer:\n",
        "  filenames = glob.glob('image*.png')\n",
        "  filenames = sorted(filenames)\n",
        "  for filename in filenames:\n",
        "    image = imageio.imread(filename)\n",
        "    writer.append_data(image)\n",
        "  image = imageio.imread(filename)\n",
        "  writer.append_data(image)"
      ],
      "id": "3b3bca56"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hj76TAg1SKN5"
      },
      "source": [
        "Veamos:"
      ],
      "id": "hj76TAg1SKN5"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5f0476c"
      },
      "outputs": [],
      "source": [
        "import tensorflow_docs.vis.embed as embed\n",
        "embed.embed_file(anim_file)"
      ],
      "id": "c5f0476c"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "508d7366"
      },
      "source": [
        "A nivel generativo las GAN pueden obtener mejores resultados en comparación con los _Autoencoders_. **No obstante**, aunque las GANs son una herramienta poderosa en el aprendizaje profundo, su implementación y entrenamiento pueden ser un desafío debido a algunas dificultades inherentes a la arquitectura de la red y al proceso de entrenamiento.\n",
        "\n",
        "- **En primer lugar**, las GANs tienen una arquitectura compleja que incluye dos redes neuronales, una generadora y otra discriminadora, que se entrenan de forma adversaria. El entrenamiento de dos redes neuronales al mismo tiempo es más complicado que entrenar una sola red, y esto puede requerir un ajuste más cuidadoso de los hiperparámetros, como la tasa de aprendizaje, el tamaño del lote y el número de capas.\n",
        "\n",
        "- **En segundo lugar**, las GANs pueden ser difíciles de entrenar debido a la inestabilidad del proceso de entrenamiento. A veces, la red generadora puede aprender a producir muestras que engañan al discriminador, pero estas muestras pueden ser muy diferentes de las muestras reales.\n",
        "\n",
        "    - Además, el proceso de entrenamiento de las GANs puede ser muy intensivo en términos de recursos computacionales, ya que se requiere entrenar dos redes neuronales al mismo tiempo. Por lo tanto, es posible que se necesite utilizar hardware especializado, como unidades de procesamiento gráfico (GPU) o unidades de procesamiento tensorial (TPU), para acelerar el proceso de entrenamiento.\n",
        "\n",
        "- **Finalmente**, puede ser difícil evaluar la calidad y la diversidad de las muestras generadas por una GAN, ya que no existe una medida objetiva para la calidad de la generación. Por lo tanto, es posible que se necesite utilizar técnicas adicionales, como el análisis visual o la evaluación humana, para evaluar la calidad y la diversidad de las muestras generadas.\n",
        "\n",
        "En resumen, la implementación de una GAN puede ser un desafío debido a la complejidad de su arquitectura, la inestabilidad del proceso de entrenamiento, la intensidad computacional del proceso de entrenamiento y la dificultad para evaluar la calidad de las muestras generadas. Sin embargo, a pesar de estas dificultades, las GANs siguen siendo una herramienta poderosa en el aprendizaje profundo y se utilizan en una amplia variedad de aplicaciones, desde la generación de imágenes hasta la creación de música y el diseño de moda."
      ],
      "id": "508d7366"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8631af6a"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "----\n",
        "\n",
        "- [*Introducción a los codificadores automáticos*](https://www.tensorflow.org/tutorials/generative/autoencoder?hl=es-419)\n",
        "\n",
        "- [*Codificador automático variacional convolucional*](https://www.tensorflow.org/tutorials/generative/cvae?hl=es-419)"
      ],
      "id": "8631af6a"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWZthFGu9qCO"
      },
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "  * [Juan Sebastián Lara](https://http://juselara.com/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](https://www.linkedin.com/in/mario-andres-rodriguez-triana-394806145/).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "id": "RWZthFGu9qCO"
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "jupytext": {
      "cell_metadata_filter": "colab,-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}