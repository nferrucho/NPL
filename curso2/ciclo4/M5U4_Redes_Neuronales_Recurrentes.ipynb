{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nferrucho/NPL/blob/main/curso2/ciclo4/M5U4_Redes_Neuronales_Recurrentes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src = \"https://drive.google.com/uc?export=view&id=1VV2e_u46fNm_ewns8QW2HGRZAPHh-e2t\" alt = \"Encabezado MLDS\" width = \"100%\">  </img>"
      ],
      "metadata": {
        "id": "FcHhwdTuiQCq"
      },
      "id": "FcHhwdTuiQCq"
    },
    {
      "cell_type": "markdown",
      "id": "8feb1551",
      "metadata": {
        "id": "8feb1551"
      },
      "source": [
        "# **Modelos de Lenguaje**\n",
        "----\n",
        "Los modelos de lenguaje son herramientas fundamentales en el campo del procesamiento del lenguaje natural (NLP, por sus siglas en inglés). Estos modelos permiten a las máquinas comprender y generar texto de manera más efectiva, lo que ha llevado a importantes avances en aplicaciones como traducción automática, resumen de texto, análisis de sentimientos y chatbots, entre otros. En este notebook, exploraremos los conceptos básicos de los modelos de lenguaje, los diferentes niveles en los que pueden operar y cómo se pueden implementar utilizando redes neuronales.\n",
        "\n",
        "## Definición de modelo de lenguaje\n",
        "\n",
        "Un modelo de lenguaje es un sistema matemático que permite estimar la probabilidad de una secuencia de palabras o caracteres en un texto. El objetivo principal es capturar las estructuras y patrones presentes en el lenguaje natural para generar texto coherente y gramaticalmente correcto.\n",
        "\n",
        "## Niveles de modelos de lenguaje\n",
        "\n",
        "* Modelos a nivel de carácter: Estos modelos tratan el texto como una secuencia de caracteres individuales y estiman la probabilidad de cada carácter dado el contexto de los caracteres anteriores. Aunque son más simples, tienden a ser menos eficientes en términos de rendimiento y tiempo de entrenamiento.\n",
        "\n",
        "* Modelos a nivel de palabra: A diferencia de los modelos a nivel de carácter, estos modelos consideran el texto como una secuencia de palabras y estiman la probabilidad de cada palabra dado el contexto de las palabras anteriores. Estos modelos son más eficientes y producen resultados de mayor calidad.\n",
        "\n",
        "## N-gramas\n",
        "\n",
        "Los n-gramas son una técnica común en el modelado de lenguaje que consiste en dividir el texto en fragmentos de n palabras o caracteres consecutivos. Por ejemplo, un bigrama (n=2) es una secuencia de dos palabras o caracteres, mientras que un trigrama (n=3) es una secuencia de tres palabras o caracteres. Los modelos de n-gramas estiman la probabilidad de una palabra o carácter en función de las n-1 palabras o caracteres anteriores. Aunque los n-gramas pueden capturar cierta información contextual, su capacidad para modelar dependencias a largo plazo es limitada.\n",
        "\n",
        "##  Redes neuronales en el modelado de lenguaje\n",
        "\n",
        "Las redes neuronales han demostrado ser una herramienta poderosa para el modelado de lenguaje, ya que pueden capturar relaciones más complejas y dependencias a largo plazo en el texto. Algunos de los enfoques más populares incluyen:\n",
        "\n",
        "*    Redes neuronales recurrentes (RNN): Las RNN son una clase de redes neuronales que pueden procesar secuencias de datos de longitud variable. Son especialmente adecuadas para el modelado de lenguaje, ya que pueden mantener información sobre el contexto previo en su estado interno. Las variantes más avanzadas, como las LSTM (Long Short-Term Memory) y las GRU (Gated Recurrent Unit), han demostrado un rendimiento aún mejor en tareas de NLP.\n",
        "\n",
        "*    Modelos de atención y _Transformers_: Los modelos basados en mecanismos de atención, como el modelo _Transformer_, han revolucionado el campo del NLP. Estos modelos son capaces de capturar dependencias a largo plazo y contextualizar cada palabra o carácter en función de su posición y contexto en la secuencia en la secuencia de entrada. Los _Transformers_ han demostrado un rendimiento superior en una amplia gama de tareas de NLP y han sido la base para modelos de lenguaje de última generación, como BERT, GPT y T5.\n",
        "\n",
        "*    Redes neuronales convolucionales (CNN): Aunque las CNN son más conocidas por su uso en la clasificación y detección de imágenes, también se han aplicado con éxito al modelado de lenguaje. En este contexto, las CNN pueden capturar patrones locales y características relevantes en secuencias de texto. Sin embargo, a menudo se combinan con otros enfoques, como las RNN o los _Transformers_, para lograr un mejor rendimiento.\n",
        "\n",
        "\n",
        "En este notebook introduciremos las **Redes Neuronales Recurrentes**. Veremos:\n",
        "\n",
        "- Modelos probabilísticos a nivel de carácter.\n",
        "- Redes Neuronales Recurrentes Simples\n",
        "- LSTM (_Long Short Term Memory_)\n",
        "- GRU (_Gated Recurrent Unit_)\n",
        "- Generación automática de texto\n",
        "\n",
        "\n",
        "Comencemos entonces con un ejemplo de generación automática de texto. Para ello comenzamos importando las librerías necesarias:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "687bdf4a",
      "metadata": {
        "id": "687bdf4a"
      },
      "outputs": [],
      "source": [
        "# Seleccionamos la versión más reciente de Tensorflow 2.\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, random\n",
        "from pprint import pprint\n",
        "%matplotlib inline\n",
        "plt.style.use(\"ggplot\")\n",
        "# Seleccionamos una semilla para los RNG (Random Number Generator)\n",
        "tf.random.set_seed(123)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b5d2373",
      "metadata": {
        "id": "9b5d2373"
      },
      "source": [
        "# **1. Modelos probabilísticos del lenguaje a nivel de carácter**\n",
        "----\n",
        "Una de las principales ventajas de las redes neuronales es que **permiten aproximar modelos o distribuciones comúnmente usados como modelos probabilísticos del lenguaje**. Esto se consigue fundamentalmente al explotar las propiedades de algunos tipos de activaciones como _Sigmoide_ y _Softmax_.\n",
        "- Por ejemplo, una familia de modelos probabilísticos típicos son **los modelos de n-gramas**, donde se modela una distribución $P(c_{k+1}|c_1,\\dots,c_{k})$ que representa la probabilidad de que un carácter $c_k$ tenga lugar luego de un k-gram o secuencia de caracteres $c_1,\\dots,c_{k}$.\n",
        "\n",
        "El objetivo de una red neuronal en esta tarea es **aproximar esta distribución**, esto se hace típicamente de forma paramétrica (pesos de la red) y la salida está asociada comúnmente con el valor de la probabilidad. Veamos un ejemplo simple de esto con un modelo 3-gramas en un corpus muy pequeño.\n",
        "\n",
        "- Comenzamos definiendo el corpus a partir de un texto de ejemplo tomado de la biografía de Alan Turing de Wikipedia :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "991353e4",
      "metadata": {
        "id": "991353e4"
      },
      "outputs": [],
      "source": [
        "corpus = \"\"\"\n",
        "alan mathison turing es considerado uno de los padres de la ciencia de la\n",
        "computacion y precursor de la informatica moderna. proporciono una influyente\n",
        "formalizacion de los conceptos de algoritmo y computacion: la maquina de turing.\n",
        "en el campo de la inteligencia artificial, es conocido sobre todo por la\n",
        "concepcion del test de turing, un criterio segun el cual puede juzgarse\n",
        "la inteligencia de una maquina si sus respuestas en la prueba son indistinguibles\n",
        "de las de un ser humano.\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cfe3dd20",
      "metadata": {
        "id": "cfe3dd20"
      },
      "source": [
        "Ahora vamos a codificar cada uno de los caracteres como enteros. Más adelante veremos en detalle este proceso (llamado **vectorización**).\n",
        "\n",
        "- Primero extraemos el vocabulario del ejemplo (a nivel de carácter) y calculamos su tamaño :\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_vocab = sorted(set(corpus))\n",
        "example_vocab_size = len(example_vocab)"
      ],
      "metadata": {
        "id": "dbEXtoG_jz7_"
      },
      "id": "dbEXtoG_jz7_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Luego asignamos un número entero a cada carácter del vocabulario :"
      ],
      "metadata": {
        "id": "KJBfYUYGkG9K"
      },
      "id": "KJBfYUYGkG9K"
    },
    {
      "cell_type": "code",
      "source": [
        "char2int = {u:i for i, u in enumerate(example_vocab)}\n",
        "int2char = np.array(example_vocab)"
      ],
      "metadata": {
        "id": "HHgJRWKckTWK"
      },
      "id": "HHgJRWKckTWK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos:"
      ],
      "metadata": {
        "id": "zgMScYFwkWob"
      },
      "id": "zgMScYFwkWob"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b2e9a5d",
      "metadata": {
        "id": "9b2e9a5d"
      },
      "outputs": [],
      "source": [
        "# Mostramos la codificación de los primeros 20 caracteres\n",
        "print('{')\n",
        "for char,_ in zip(char2int, range(20)):\n",
        "    print(f'  {repr(char):4s}: {char2int[char]:3d}')\n",
        "print('  ...\\n}')\n",
        "print(\"Tamaño vocabulario:\", len(example_vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3661f3e0",
      "metadata": {
        "id": "3661f3e0"
      },
      "source": [
        "Convertimos el dataset a 3-gramas (secuencias de tres caracteres y el carácter sucesor) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8047b2b",
      "metadata": {
        "id": "c8047b2b"
      },
      "outputs": [],
      "source": [
        "# Definimos las variables donde se guardará el dataset\n",
        "X = []; y = []\n",
        "# Iteramos para todos los posibles 3-gramas en el corpus\n",
        "for i in range(len(corpus)-4):\n",
        "    X.append(corpus[i:i+3])\n",
        "    y.append(corpus[i+3])\n",
        "print(X[:5])\n",
        "print(y[:5])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f2a0a460",
      "metadata": {
        "id": "f2a0a460"
      },
      "source": [
        "Veamos un ejemplo de secuencia de caracteres y el sucesor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e6651c9",
      "metadata": {
        "id": "0e6651c9"
      },
      "outputs": [],
      "source": [
        "print(f\"3-grama ---> sucesor\")\n",
        "print(f\"{X[1]}     ---> {y[1]}\")\n",
        "print(f\"{X[15]}     ---> {y[15]}\")\n",
        "print(f\"{X[17]}     ---> {y[17]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d65a25cd",
      "metadata": {
        "id": "d65a25cd"
      },
      "source": [
        "Ahora, convertimos los datos a valores enteros:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50976616",
      "metadata": {
        "id": "50976616"
      },
      "outputs": [],
      "source": [
        "X_int = np.array(list(map(lambda i: [char2int[i[0]], char2int[i[1]], char2int[i[2]]], X)))\n",
        "y_int = np.array(list(map(lambda i: char2int[i], y)))\n",
        "print(X_int.shape)\n",
        "print(y_int.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos:"
      ],
      "metadata": {
        "id": "Ub3mzqzzktV6"
      },
      "id": "Ub3mzqzzktV6"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27625bf6",
      "metadata": {
        "id": "27625bf6"
      },
      "outputs": [],
      "source": [
        "print(f\"3-grama ---> codificación\")\n",
        "print(f\"{X[1]}     ---> {X_int[1]}\")\n",
        "print(f\"{X[15]}     ---> {X_int[15]}\")\n",
        "print(f\"{X[17]}     ---> {X_int[17]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3efb65f0",
      "metadata": {
        "id": "3efb65f0"
      },
      "source": [
        "Nuestra red neuronal tomará como entrada **la codificación en enteros** de los caracteres y la salida será la **probabilidad de cada uno de los posibles caracteres** de ser el sucesor.\n",
        "\n",
        "- Definamos este modelo en _Tensorflow_: Será un modelo con una capa de entrada de 3 neuronas, seguido por una capa densa con activación _Sigmoide_, y una capa de salida de tamaño igual al tamaño del vocabulario"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = tf.keras.layers.Input(shape=(3,))\n",
        "densa = tf.keras.layers.Dense(64, activation='sigmoid')(input)\n",
        "logits = tf.keras.layers.Dense(example_vocab_size, activation='linear')(densa)\n",
        "probits = tf.keras.layers.Activation('softmax')(logits)\n",
        "\n",
        "model = tf.keras.models.Model(inputs=input, outputs=probits)"
      ],
      "metadata": {
        "id": "iCucPgWzliLL"
      },
      "id": "iCucPgWzliLL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "agFPH2IRlmoo"
      },
      "id": "agFPH2IRlmoo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "9799c5f6",
      "metadata": {
        "id": "9799c5f6"
      },
      "source": [
        "En este caso definimos dos capas llamadas **logits** y **probits**, los cuales son conceptos comunes en modelos probabilísticos.\n",
        "\n",
        "- Por un lado, los **logits** se encuentran en un dominio real y representan una especie de log-verosimilitud (logaritmo de una probabilidad) que a su vez está relacionada con una distribución de probabilidad sobre caracteres.\n",
        "- Por otro lado, los **probits** se encuentran en un rango de 0 a 1 y representan la probabilidad de cada carácter. **probits** se calcula a partir de **logits** usando la función de activación _softmax_."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compilamos usando `categorical_crossentropy` como función de pérdida y `Adam` como optimizador con una tasa de aprendizaje de $0.01$:"
      ],
      "metadata": {
        "id": "41aaj4KPnDmn"
      },
      "id": "41aaj4KPnDmn"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0e87851b",
      "metadata": {
        "id": "0e87851b"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=tf.optimizers.Adam(learning_rate=1e-2))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Para entrenar el modelo necesitamos convertir las etiquetas o caracteres sucesores en *one-hot* :"
      ],
      "metadata": {
        "id": "i0w6jgdPndk4"
      },
      "id": "i0w6jgdPndk4"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38a2e60c",
      "metadata": {
        "id": "38a2e60c"
      },
      "outputs": [],
      "source": [
        "Y = tf.keras.utils.to_categorical(y_int, num_classes=example_vocab_size)\n",
        "print(Y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ce18863",
      "metadata": {
        "id": "3ce18863"
      },
      "source": [
        "Finalmente, entrenamos el modelo durante 1000 epochs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8137d12a",
      "metadata": {
        "id": "8137d12a"
      },
      "outputs": [],
      "source": [
        "hist = model.fit(X_int, Y, epochs=1000)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe2667c3",
      "metadata": {
        "id": "fe2667c3"
      },
      "source": [
        "Veamos un ejemplo de las probabilidades predichas para algunos 3-gramas. Revise cuál carácter es el más probable de ser el siguiente, según el modelo:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`ala`**"
      ],
      "metadata": {
        "id": "xK4HPRe6Mdkb"
      },
      "id": "xK4HPRe6Mdkb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1055a477",
      "metadata": {
        "id": "1055a477"
      },
      "outputs": [],
      "source": [
        "# 3-gram ejemplo\n",
        "seq = 'ala'\n",
        "# Codificación en enteros\n",
        "seq = np.array([char2int[i] for i in seq]).reshape(1, -1)\n",
        "print(seq)\n",
        "# Predicciones\n",
        "probs = model.predict(seq)\n",
        "# Veamos un diagrama de barras de las probabilidades\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.bar(int2char, probs[0]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`tur`**"
      ],
      "metadata": {
        "id": "o7W9qbNUoxj0"
      },
      "id": "o7W9qbNUoxj0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c58a7e2",
      "metadata": {
        "id": "5c58a7e2"
      },
      "outputs": [],
      "source": [
        "# 3-gram ejemplo\n",
        "seq = 'tur'\n",
        "# Codificación en enteros\n",
        "seq = np.array([char2int[i] for i in seq]).reshape(1, -1)\n",
        "# Predicciones\n",
        "probs = model.predict(seq)\n",
        "# Veamos un diagrama de barras de las probabilidades\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.bar(int2char, probs[0]);"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* **`aaa`**"
      ],
      "metadata": {
        "id": "TQa-AiiZou8q"
      },
      "id": "TQa-AiiZou8q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f8e5c49",
      "metadata": {
        "id": "5f8e5c49"
      },
      "outputs": [],
      "source": [
        "# 3-gram ejemplo\n",
        "seq = 'aaa'\n",
        "# Codificación en enteros\n",
        "seq = np.array([char2int[i] for i in seq]).reshape(1, -1)\n",
        "# Predicciones\n",
        "probs = model.predict(seq)\n",
        "# Veamos un diagrama de barras de las probabilidades\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.bar(int2char, probs[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc1e85eb",
      "metadata": {
        "id": "cc1e85eb"
      },
      "source": [
        " # **2. Redes recurrentes**\n",
        "----\n",
        "Las redes neuronales recurrentes (RNN) son un tipo de arquitectura de redes neuronales especialmente diseñada para procesar secuencias de datos. Más específicamente, pueden recordar, y sus decisiones están influenciadas por el pasado. Esto las hace ideales para tareas que involucran datos secuenciales o temporales. Se utilizan en una amplia gama de aplicaciones y dominios, algunos de los cuales incluyen:\n",
        "\n",
        "*    Predicción de series temporales: Las RNN pueden utilizarse para predecir valores futuros en series temporales, como el clima o las ventas de productos. Las RNN pueden modelar relaciones temporales y capturar patrones a lo largo del tiempo.\n",
        "\n",
        "*    Reconocimiento de voz: Las RNN son empleadas en sistemas de reconocimiento automático de voz para convertir señales de audio en texto. Estos sistemas deben tener en cuenta la secuencia temporal de las señales de audio para generar transcripciones precisas.\n",
        "\n",
        "*    Generación de música: Las RNN también pueden ser utilizadas para generar composiciones musicales, ya que la música es inherentemente secuencial y temporal. Las RNN pueden aprender patrones y estructuras en la música para crear nuevas piezas.\n",
        "\n",
        "*    Control de robots y sistemas autónomos: En el control de robots y vehículos autónomos, las RNN pueden utilizarse para procesar secuencias de datos de sensores y tomar decisiones en función de la información contextual.\n",
        "\n",
        "*    Análisis de vídeo: Las RNN pueden utilizarse en el análisis de vídeos para detectar y reconocer objetos, acciones y eventos a lo largo del tiempo, ya que los vídeos son secuencias de imágenes.\n",
        "\n",
        "*    Y por supuesto, procesamiento del lenguaje natural: las RNN son ampliamente utilizadas en tareas de NLP, como traducción automática, análisis de sentimientos, generación de texto, resumen automático, reconocimiento de entidades nombradas y más. Estas tareas a menudo requieren modelar secuencias de palabras o caracteres y capturar dependencias de contexto.\n",
        "\n",
        "\n",
        "\n",
        "A pesar de que las redes neuronales multicapa también recuerdan cosas, **las redes recurrentes utilizan información de lo que recuerdan de entradas anteriores para generar una salida**. Este comportamiento se consigue al añadir un lazo de realimentación sobre una determinada red neuronal $A$ como se muestra en la siguiente figura :\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1MX2WYF6JGBZ5OsphTU-C2iAjJK7sQili' alt = \"Gráfico ilustrativo del lazo de realimentación sobre una red neuronal \" width =\"25%\"></img></center>\n",
        "\n",
        "Las redes neuronales aprenden por medio de la propagación de un error, no obstante, debido a la realimentación no es posible entrenar las redes recurrentes directamente, por ello, se utiliza una **aproximación** al considerar solamente un número finito de estados previos, como se muestra a continuación :\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=10SLpNXUgeCx10RX9EQu7IGFlUeKhYiVs' alt =\"Gráfico ilustrativo del número finito de estados previos en una red\" width =\"100%\"></img></center>\n",
        "\n",
        "Este tipo de arquitectura en forma de cadena es precisamente lo que hace que este tipo de redes sean muy útiles para datos con estructura secuencial.\n",
        "\n",
        "En este caso, veremos la implementación de tres tipos de redes recurrentes comúnmente usadas :  \n",
        "- ```tf.keras.layers.SimpleRNN```\n",
        "-  ```tf.keras.layers.GRU ```\n",
        "-  ```tf.keras.layers.LSTM ```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23f25991",
      "metadata": {
        "id": "23f25991"
      },
      "source": [
        "## **Modelos secuenciales**\n",
        "----\n",
        "\n",
        "Cuando trabajamos con secuencias hay varias formas de representar los datos, no obstante, lo más típico es utilizar arquitecturas de tipo **_Many to One_** _(Muchos a uno)_ o **_Many to Many_** _(Muchos a Muchos)_ :\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1ABK3maujqA-63sGn9F5-IiNpE8jTAjxh' alt =\"Gráfico ilustrativo de las distintas arquitecturas de tipo Many to One y Many to Many\"width =\"100%\"></img></center>\n",
        "\n",
        "En nuestra aplicación de modelos probabilísticos, una arquitectura *Many-to-One* puede ser usada para predecir el siguiente carácter dada determinada oración o secuencia de caracteres : <center>$P(c_{k+1}|c_1,\\dots,c_{k})$</center>\n",
        "\n",
        "Una arquitectura *Many-to-Many* podría ser usada para predecir la siguiente oración o secuencia de caracteres:\n",
        "<center>$P(c_{k+1}, \\dots, c_{k+m}|c_1,\\dots,c_{k})$. </center>\n",
        "\n",
        "En los siguientes ejemplos trabajaremos a **nivel de carácter**. Utilizaremos una arquitectura de tipo *Many-to-Many*, es decir nuestro modelo recibirá una secuencia y predecirá una secuencia.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.1. Procesamiento de los datos**\n",
        "----\n",
        "\n",
        "### **`Dataset` de _Tensorflow_**\n",
        "\n",
        "En _TensorFlow_ existe un objeto de tipo `Dataset`. Un `Dataset` de TensorFlow es una abstracción de alto nivel que representa una secuencia de elementos, donde cada elemento consta de uno o más tensores. Los objetos `Dataset` facilitan la carga, el preprocesamiento y la iteración de datos en el proceso de entrenamiento y evaluación de modelos de aprendizaje automático.\n",
        "\n",
        "Además, los `Dataset` están diseñados para ser eficientes y escalables, lo que permite cargar y procesar grandes cantidades de datos de manera eficiente, incluso en hardware limitado. Algunas de las características clave de los `Dataset` de _TensorFlow_ incluyen la capacidad de:\n",
        "\n",
        "*    Realizar transformaciones de datos, como mapeo, filtrado, reducción y mezcla.\n",
        "*    Cargar datos de diversas fuentes, como archivos locales, sistemas de archivos distribuidos y servicios en la nube.\n",
        "*    Realizar la carga y el preprocesamiento de datos de manera paralela y asíncrona para aprovechar al máximo el hardware disponible.\n",
        "\n",
        "### **Método `from_tensor_slices`**\n",
        "\n",
        "El método `from_tensor_slices` es una función de la clase `Dataset` que permite crear un nuevo dataset a partir de tensores existentes. Esta función toma como entrada uno o más tensores y devuelve un dataset donde cada elemento corresponde a una \"rebanada\" de los tensores de entrada a lo largo de su primera dimensión.\n",
        "\n",
        "La función `from_tensor_slices` es especialmente útil cuando se trabaja con datos estructurados, como imágenes y etiquetas, que se almacenan como tensores. Al utilizar este método, se pueden crear fácilmente datasets de entrenamiento y validación que contengan pares de datos y etiquetas correspondientes.\n",
        "\n",
        "### **Ejemplo de uso del método `from_tensor_slices`**\n",
        "\n",
        "Supongamos que tenemos dos tensores, `data` y `labels`, que representan un conjunto de imágenes y sus etiquetas correspondientes:\n",
        "\n"
      ],
      "metadata": {
        "id": "vA1Yx_1d6DdB"
      },
      "id": "vA1Yx_1d6DdB"
    },
    {
      "cell_type": "code",
      "source": [
        "data = tf.random.normal([100, 28, 28])  # 100 imágenes de 28x28\n",
        "labels = tf.random.uniform([100], minval=0, maxval=10, dtype=tf.int32)  # 100 etiquetas"
      ],
      "metadata": {
        "id": "XXeLdRpm8uIH"
      },
      "id": "XXeLdRpm8uIH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos utilizar el método `from_tensor_slices` para crear un dataset a partir de estos tensores:"
      ],
      "metadata": {
        "id": "it0pnxTqBUOd"
      },
      "id": "it0pnxTqBUOd"
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((data, labels))"
      ],
      "metadata": {
        "id": "BxFd4xeIBW2-"
      },
      "id": "BxFd4xeIBW2-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, dataset es un objeto Dataset que contiene 100 elementos, donde cada elemento es un par de tensores (imagen, etiqueta).\n",
        "\n",
        "### **Método `batch`**\n",
        "\n",
        "El método `batch` es otra función importante de la clase `Dataset` en _TensorFlow_, que permite agrupar elementos consecutivos de un dataset en lotes (batches) de un tamaño específico.\n",
        "\n",
        "- Ahora podemos aplicar el método batch para agrupar los elementos del dataset en lotes de un tamaño específico, por ejemplo, 32:"
      ],
      "metadata": {
        "id": "lK0evy48DyOo"
      },
      "id": "lK0evy48DyOo"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32\n",
        "dataset = dataset.batch(batch_size)"
      ],
      "metadata": {
        "id": "VxudD_8_D3FO"
      },
      "id": "VxudD_8_D3FO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Después de aplicar el método `batch`, el `dataset` contendrá lotes de 32 elementos cada uno (excepto posiblemente el último _batch_, que podría ser más pequeño si el número total de elementos no es divisible por el tamaño del _batch_). Al iterar sobre este `dataset`, obtendremos tensores de forma `(batch_size, data_shape)` para los datos y las etiquetas."
      ],
      "metadata": {
        "id": "kVncCqOaD4os"
      },
      "id": "kVncCqOaD4os"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**La Biblia**\n",
        "\n",
        "Como ejemplo en este notebook utilizaremos un libro de acceso libre, cuyo texto tiene una estructura particular: la **Biblia**. Veamos cómo cargarlo y preprocesarlo.\n",
        "\n",
        "Primero comenzamos cargando el texto y codificándolo en un formato típico de _Python_ desde un archivo de texto plano:"
      ],
      "metadata": {
        "id": "os153bgWBgr5"
      },
      "id": "os153bgWBgr5"
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -O biblia.txt https://raw.githubusercontent.com/medelr/MDDS5/gh-pages/biblia.txt?raw=true\n",
        "with open(\"biblia.txt\", \"rb\") as f:\n",
        "    text = f.read().decode(encoding=\"latin-1\")\n",
        "print(f\"Longitud del texto en caracteres: {len(text)}\")"
      ],
      "metadata": {
        "id": "EBPuxqXX6bXl"
      },
      "id": "EBPuxqXX6bXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos los primeros 1000 caracteres del texto :"
      ],
      "metadata": {
        "id": "ApLnfJ9R6ayr"
      },
      "id": "ApLnfJ9R6ayr"
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:1000])"
      ],
      "metadata": {
        "id": "dZcSwJnC6ef1"
      },
      "id": "dZcSwJnC6ef1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Podemos hacer una **exploración descriptiva del texto**. Por ejemplo, empecemos contando el número total de caracteres diferentes. Al convertir el texto en un objeto tipo `set` se eliminan los duplicados y podemos contar los caracteres :"
      ],
      "metadata": {
        "id": "K8lojL9a6j7F"
      },
      "id": "K8lojL9a6j7F"
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtenemos los caracteres únicos en el texto\n",
        "vocab = sorted(set(text))\n",
        "vocab_size = len(vocab)\n",
        "print(f'Se encontraron {vocab_size} caracteres únicos')"
      ],
      "metadata": {
        "id": "5ITBF15P6m6O"
      },
      "id": "5ITBF15P6m6O",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Veamos:"
      ],
      "metadata": {
        "id": "y_Nk-K6I6qNA"
      },
      "id": "y_Nk-K6I6qNA"
    },
    {
      "cell_type": "code",
      "source": [
        "vocab"
      ],
      "metadata": {
        "id": "OnRVMuZw6ppn"
      },
      "id": "OnRVMuZw6ppn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora veamos cómo preparar el dataset utilizando ```tf.data```:"
      ],
      "metadata": {
        "id": "aa1JRyDD6zRV"
      },
      "id": "aa1JRyDD6zRV"
    },
    {
      "cell_type": "markdown",
      "id": "881cbbd4",
      "metadata": {
        "id": "881cbbd4"
      },
      "source": [
        "### **2.1.1. Vectorización**\n",
        "----\n",
        "\n",
        "Para entrenar una red recurrente es necesario que los caracteres se conviertan a una **representación numérica**, en este caso, crearemos dos tablas de consulta, una para mapear los caracteres a enteros y otra para mapear de enteros a caracteres:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b41bbc48",
      "metadata": {
        "id": "b41bbc48"
      },
      "outputs": [],
      "source": [
        "# Convertir de carácter a número entero\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "\n",
        "# Mostramos la codificación de los primeros 20 caracteres\n",
        "print('{')\n",
        "for char,_ in zip(char2idx, range(20)):\n",
        "    print(f'  {repr(char):4s}: {char2idx[char]:3d}')\n",
        "print('  ...\\n}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed28153f",
      "metadata": {
        "id": "ed28153f"
      },
      "outputs": [],
      "source": [
        "# Convertir de entero a String\n",
        "idx2char = np.array(vocab)\n",
        "idx2char"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8311186",
      "metadata": {
        "id": "a8311186"
      },
      "outputs": [],
      "source": [
        "# Convertimos todo el texto a enteros :\n",
        "text_as_int = np.array([char2idx[c] for c in text])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e20aa90a",
      "metadata": {
        "id": "e20aa90a"
      },
      "source": [
        "Veamos un ejemplo de cómo es la representación de los primeros 20 caracteres del texto :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65961bf8",
      "metadata": {
        "id": "65961bf8"
      },
      "outputs": [],
      "source": [
        "print(f\"Caracteres originales:\\n\\t{repr(text[10:30])}\")\n",
        "print(f\"Codificación:\\n\\t{text_as_int[10:30]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1faeef88",
      "metadata": {
        "id": "1faeef88"
      },
      "source": [
        "### **2.1.2. Ejemplos y etiquetas**\n",
        "----\n",
        "\n",
        "Como dijimos anteriormente, utilizaremos un enfoque *Many-to-Many*, es decir, la entrada y la salida de la red neuronal serán secuencias de tamaño ```seq_length```. Ahora, veamos cómo crear un `Dataset` dentro de _TensorFlow_ con esta estructura :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ca04cf01",
      "metadata": {
        "id": "ca04cf01"
      },
      "outputs": [],
      "source": [
        "# Definimos la longitud de cada secuencia\n",
        "seq_length = 100\n",
        "# Definimos el número de ejemplos que verá la red en cada época\n",
        "examples_per_epoch = len(text) // (seq_length + 1)\n",
        "# Creamos un dataset con la representación vectorizada\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "# Mostramos un ejemplo con un batch de tamaño 10\n",
        "for i in char_dataset.take(10):\n",
        "    print(idx2char[i])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ca0e05a3",
      "metadata": {
        "id": "ca0e05a3"
      },
      "source": [
        "Utilizamos el método `batch` para convertir las secuencias al tamaño que deseamos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5f8d7ee",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "c5f8d7ee"
      },
      "outputs": [],
      "source": [
        "# Obtenemos las secuencias de tamaño 100 + 1\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "# Mostramos las primeras 5 secuencias\n",
        "for item in sequences.take(5):\n",
        "    print(repr(''.join(idx2char[item.numpy()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "adfdb3ee",
      "metadata": {
        "id": "adfdb3ee"
      },
      "source": [
        "Creamos un dataset donde las observaciones son secuencias de tamaño 100 y las etiquetas son la misma secuencia desplazada un carácter, es decir excluye el primer carácter e incluye el carácter siguiente :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "857e0462",
      "metadata": {
        "id": "857e0462"
      },
      "outputs": [],
      "source": [
        "# Creamos una función para separar el último carácter\n",
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# Aplicamos la función a todo el dataset original\n",
        "dataset = sequences.map(split_input_target)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "id": "1xxfwljRpP2g"
      },
      "id": "1xxfwljRpP2g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "993f2088",
      "metadata": {
        "id": "993f2088"
      },
      "source": [
        "- Veamos un ejemplo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a5c7a280",
      "metadata": {
        "id": "a5c7a280"
      },
      "outputs": [],
      "source": [
        "for entrada, salida in dataset.take(2):\n",
        "    print(\"Entrada:\")\n",
        "    print(repr(''.join(idx2char[entrada.numpy()])))\n",
        "    print(\"Salida:\")\n",
        "    print(repr(''.join(idx2char[salida.numpy()])))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85e95730",
      "metadata": {
        "id": "85e95730"
      },
      "source": [
        "Ahora, especificamos el número de _Batches_ que usaremos para entrenar la red y un tamaño de _Buffer_, el cual es necesario para aleatorizar localmente las observaciones (```tf.data``` carga este número de muestras en memoria para poder aleatorizar el entrenamiento)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "44b9b0cc",
      "metadata": {
        "id": "44b9b0cc"
      },
      "outputs": [],
      "source": [
        "batch_size = 64\n",
        "buffer_size = 10000\n",
        "\n",
        "# Creamos el dataset con el Batch size y el Buffer\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ed60cbea",
      "metadata": {
        "id": "ed60cbea"
      },
      "source": [
        "Como podemos ver, cada _batch_ tiene 64 secuencias de tamaño 100 en la entrada y en la salida. La cadena de la salida corresponde un desplazamiento de un carácter hacia adelante de la cadena de entrada."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3a0f94fa",
      "metadata": {
        "id": "3a0f94fa"
      },
      "source": [
        "## **2.2. SimpleRNN**\n",
        "----\n",
        "\n",
        "Una capa **SimpleRNN** contiene unidades básicas de neuronas recurrentes, es decir, se trata de **neuronas totalmente conectadas** donde la salida es realimentada a la entrada.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1MX2WYF6JGBZ5OsphTU-C2iAjJK7sQili' alt = \"Gráfico ilustrativo del lazo de realimentación sobre una red neuronal \" width =\"25%\"></img></center>\n",
        "\n",
        "Veamos cómo construir un modelo con SimpleRNN en _TensorFlow_:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e2946c9d",
      "metadata": {
        "id": "e2946c9d"
      },
      "source": [
        "### **2.2.1. Definición del modelo**\n",
        "----\n",
        "\n",
        "Utilizaremos ```tf.keras.models.Sequential``` para definir el modelo, en este caso utilizaremos tres capas :\n",
        "\n",
        "* ```tf.keras.layers.Embedding```: Se trata de la entrada del modelo y busca convertir los valores enteros en un *embedding*.\n",
        "* ```tf.keras.layers.SimpleRNN```: Se trata de la capa con unidades recurrentes.\n",
        "* ```tf.keras.layers.Dense```: Agregamos una capa totalmente conectada para la predicción del carácter más probable.\n",
        "\n",
        "La capa `SimpleRNN` se utiliza para modelar secuencias temporales y puede procesar datos secuenciales. Los argumentos más importantes de la capa `SimpleRNN` son:\n",
        "\n",
        "*    `units`: Este argumento tipo `int` es obligatorio y define el número de unidades recurrentes (neuronas) en la capa SimpleRNN. Aumentar el número de unidades puede aumentar la capacidad del modelo para capturar dependencias temporales más complejas, pero también puede aumentar el tiempo de entrenamiento y el riesgo de sobreajuste.\n",
        "\n",
        "*    `activation`: Especifica la función de activación que se utilizará en las unidades recurrentes. Por defecto, se usa la función de activación `tanh`.\n",
        "\n",
        "*    `dropout`, `recurrent_dropout`: Estos argumentos, que varían entre 0 y 1, especifican la tasa de _dropout_ para la entrada y la conexión recurrente, respectivamente.\n",
        "\n",
        "*    `recurrent_initializer`: Define el método de inicialización de los pesos para la matriz de conexión recurrente en la capa RNN. La matriz de conexión recurrente representa las conexiones entre las unidades recurrentes en pasos de tiempo consecutivos.\n",
        "\n",
        "*    `return_sequences`: Un valor booleano que indica si la capa debe devolver la secuencia completa de salidas para cada paso de tiempo en lugar de solo la última salida. Por defecto, es `False`. Esto es útil cuando se desea apilar varias capas RNN o cuando se necesita la secuencia completa de salidas para una tarea específica.\n",
        "\n",
        "*    `return_state`: Un valor booleano que indica si la capa debe devolver el último estado oculto además de la salida. Por defecto, es `False`. Esto puede ser útil si desea utilizar el último estado oculto en otras partes del modelo o si está trabajando con un modelo de secuencia a secuencia.\n",
        "\n",
        "*    `stateful`: Un valor booleano que indica si la capa debe mantener su estado oculto entre llamadas consecutivas para su uso en secuencias más largas o en lotes con dependencias temporales. Por defecto, es `False`.\n",
        "\n",
        "Aquí usaremos una capa recurrente con 1024 unidades, y un inicializador `glorot_uniform`. Esta inicialización de pesos tiene como objetivo mantener una varianza adecuada en las salidas y gradientes a lo largo de las capas durante el entrenamiento de una red neuronal, lo que puede ayudar a mejorar la convergencia y la estabilidad del entrenamiento."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ec2aab7c",
      "metadata": {
        "id": "ec2aab7c"
      },
      "outputs": [],
      "source": [
        "# Dimensión del embedding\n",
        "embedding_dim = 256\n",
        "# Número de unidades en la capa recurrente\n",
        "rnn_units = 1024\n",
        "\n",
        "model_srnn = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[batch_size, None]),\n",
        "                                  tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                  tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                                            return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                            stateful=True,\n",
        "                                                            recurrent_initializer='glorot_uniform'),\n",
        "                                  tf.keras.layers.Dense(vocab_size)])\n",
        "model_srnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Vemos el modelo de forma gráfica :"
      ],
      "metadata": {
        "id": "kkWOe-fZsWiG"
      },
      "id": "kkWOe-fZsWiG"
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model_srnn)"
      ],
      "metadata": {
        "id": "Fsw_AnpIsbKf"
      },
      "id": "Fsw_AnpIsbKf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "10895f8e",
      "metadata": {
        "id": "10895f8e"
      },
      "source": [
        "Es importante resaltar el **tamaño de salida del modelo**, el cual se compone de: ```(batch_size, sequence_length, vocab_size)```, como se puede observar, el modelo no requiere secuencias de un tamaño fijo, por lo cual, puede ser entrenado con una secuencia de un tamaño dado y posteriormente usarse con secuencias de distinta longitud.\n",
        "\n",
        "- La idea de este modelo es predecir desde el segundo hasta el último carácter de la secuencia de entrada junto con el carácter sucesor. En este caso, nos enfocaremos primordialmente en la distribución de este último.\n",
        "\n",
        "Veamos las distribuciones predichas por el modelo (pesos aleatorios) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0cafefe",
      "metadata": {
        "lines_to_next_cell": 2,
        "id": "b0cafefe"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_srnn(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "print(f\"Tamaño de la secuencia de entrada {input_example_batch.shape}\")\n",
        "print(f\"Tamaño de la secuencia de salida target {target_example_batch.shape}\")\n",
        "print(f\"Tamaño de la secuencia de salida predicha {example_batch_predictions.shape}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c2f52c6",
      "metadata": {
        "id": "1c2f52c6"
      },
      "source": [
        "Podemos ver que la predicción para una secuencia de tamaño 100 (codificación en enteros) es una secuencia de tamaño 100 (one-hot).\n",
        "- Veamos la distribución del carácter sucesor :\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e4c688b",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "5e4c688b"
      },
      "outputs": [],
      "source": [
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_batch_predictions[0][-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_batch_predictions[0][-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "de43d769",
      "metadata": {
        "id": "de43d769"
      },
      "source": [
        "### **2.2.2. Entrenamiento**\n",
        "----\n",
        "\n",
        "Como nuestro dataset se compone de valores enteros (no estamos usando one-hot encoding para los valores verdaderos, aunque si para las predicciones), utilizaremos la pérdida `tf.keras.losses.sparse_categorical_crossentropy`. Así mismo, como nuestra red no está prediciendo el valor entero directamente (está prediciendo los logits de cada número) especificamos el argumento `from_logits=True`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bfa1a644",
      "metadata": {
        "lines_to_next_cell": 1,
        "id": "bfa1a644"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7f0b5b56",
      "metadata": {
        "id": "7f0b5b56"
      },
      "source": [
        "- Ahora, compilamos el modelo. Usamos `Adam` como optimizador."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "546abf78",
      "metadata": {
        "id": "546abf78"
      },
      "outputs": [],
      "source": [
        "model_srnn.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "                   loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4a28af20",
      "metadata": {
        "id": "4a28af20"
      },
      "source": [
        "- Finalmente, se debe entrenar el modelo.\n",
        "\n",
        "***Nota: En este caso no entrenaremos el modelo, ya que, el entrenamiento puede llegar a tomar mucho tiempo (para entrenarlo utilice el código comentado). En lugar, cargaremos los pesos de un modelo previamente entrenado.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f0d009af",
      "metadata": {
        "id": "f0d009af"
      },
      "outputs": [],
      "source": [
        "# Utilice el siguiente código si desea entrenar el modelo (puede tomar tiempo)\n",
        "\"\"\"\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"srnn.h5\",\n",
        "    verbose = 1,\n",
        "    save_weights_only=True)\n",
        "model_srnn.fit(dataset.repeat(), epochs=20, callbacks=[checkpoint_callback],\n",
        "               steps_per_epoch=examples_per_epoch//batch_size, verbose=True)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f79f0d",
      "metadata": {
        "id": "16f79f0d"
      },
      "outputs": [],
      "source": [
        "# Descargamos los pesos de un modelo preentrenado\n",
        "!wget -O srnn.h5 https://github.com/mindlab-unal/mlds5-dataset-unit4-RedesNeuronalesRecurrentes/blob/main/srnn.h5?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a980bb1",
      "metadata": {
        "id": "3a980bb1"
      },
      "outputs": [],
      "source": [
        "# Cargamos los pesos del modelo\n",
        "model_srnn.load_weights(\"srnn.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3771175e",
      "metadata": {
        "id": "3771175e"
      },
      "source": [
        "- Veamos el mismo ejemplo que mostramos al comienzo para mostrar la distribución del carácter sucesor pero con el modelo entrenado :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a140a92",
      "metadata": {
        "id": "3a140a92"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "example_batch_predictions = model_srnn(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "idx = 6\n",
        "example_pred_seq = example_batch_predictions[idx]\n",
        "\n",
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[idx]])))\n",
        "print(f\"Carácter sucesor más probable: {idx2char[np.argmax(example_pred_seq[-1])]}\")\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_pred_seq[-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_pred_seq[-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df818a38",
      "metadata": {
        "id": "df818a38"
      },
      "source": [
        "## **2.3. Long Short-Term Memory LSTM**\n",
        "----\n",
        "\n",
        "Uno de los principales problemas de las redes recurrentes es que **tienden a tener una memoria de corto plazo**, es decir, las redes tenderán a memorizar los últimos ejemplos que vieron mientras olvidan ejemplos vistos anteriormente. Es decir, tienen memoria de corto plazo.\n",
        "\n",
        "- Una variación de red recurrente son las **long short-term memory (LSTMs)**, se trata de unidades diseñadas para aprender dependencias a largo plazo, al igual que una unidad recurrente también se entrenan en forma de cadena :\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1XH-H8bzLnHyD75mke2OWIl32aA23J8r3' alt =\"Gráfico ilustrativo de una red neuronal recurrente LSTM\" width =\"80%\"></img></center>\n",
        "\n",
        "No obstante, cada unidad se compone de **cuatro neuronas** convencionales, cada una aportando un efecto sobre el estado anterior y el estado actual.\n",
        "\n",
        "- Los círculos rojos con una X que se muestran en la anterior imagen, corresponden a *gate connections*, conexiones que son de tipo multiplicativo. Estas conexiones permiten controlar el **flujo de información** a través del tiempo. Dependiendo del valor que las alimente pueden dejar pasar (un 1) o bloquear (un 0) el paso de información.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1IBIB3n2QBbfCq8hUlCe5dlRxyXBotTgW' alt =\"  Gráfico ilustrativo de como se ejecuta la LSTM para generar logits que predicen la probabilidad logarítmica del siguiente carácter\" width =\"50%\"></img></center>\n",
        "\n",
        "Veamos un ejemplo con una capa LSTM :"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "551daa10",
      "metadata": {
        "id": "551daa10"
      },
      "source": [
        "### **2.3.1. Definición del modelo**\n",
        "----\n",
        "Vamos a definir un modelo equivalente al de la red recurrente simple, sin embargo, ahora agregaremos una **capa LSTM**. Veamos como implementar este modelo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "14b893fa",
      "metadata": {
        "id": "14b893fa"
      },
      "outputs": [],
      "source": [
        "# Dimensión del embedding\n",
        "embedding_dim = 256\n",
        "# Número de unidades en la capa recurrente\n",
        "rnn_units = 1024\n",
        "\n",
        "model_lstm = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[batch_size, None]),\n",
        "                                  tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                  tf.keras.layers.LSTM(rnn_units,\n",
        "                                                       return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                       stateful=True,\n",
        "                                                       recurrent_initializer='glorot_uniform'),\n",
        "                                  tf.keras.layers.Dense(vocab_size)])\n",
        "model_lstm.summary()\n",
        "tf.keras.utils.plot_model(model_lstm)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7a1d95",
      "metadata": {
        "id": "2a7a1d95"
      },
      "source": [
        "Veamos las distribuciones predichas por el modelo (pesos aleatorios) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8c8f5b6e",
      "metadata": {
        "id": "8c8f5b6e"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_lstm(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "example_pred_seq = example_batch_predictions[0]\n",
        "print(f\"Tamaño de la secuencia de entrada {input_example_batch[0].shape}\")\n",
        "print(f\"Tamaño de la secuencia de salida {example_pred_seq.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba7c7c6f",
      "metadata": {
        "id": "ba7c7c6f"
      },
      "source": [
        "- Veamos la distribución del carácter sucesor :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "576f474b",
      "metadata": {
        "id": "576f474b"
      },
      "outputs": [],
      "source": [
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_pred_seq[-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_pred_seq[-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad41ffbb",
      "metadata": {
        "id": "ad41ffbb"
      },
      "source": [
        "### **2.3.2. Entrenamiento**\n",
        "----\n",
        "\n",
        "Seguimos el mismo enfoque que usamos en el modelo anterior, es decir, misma pérdida y mismo optimizador :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a749a185",
      "metadata": {
        "id": "a749a185"
      },
      "outputs": [],
      "source": [
        "model_lstm.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0ffadb3a",
      "metadata": {
        "id": "0ffadb3a"
      },
      "source": [
        "- Finalmente, se debe entrenar el modelo.\n",
        "\n",
        "***Nota: En este caso no entrenaremos el modelo, ya que, el entrenamiento puede llegar a tomar mucho tiempo (para entrenarlo utilice el código comentado). En lugar, cargaremos los pesos de un modelo previamente entrenado.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3baf5fe",
      "metadata": {
        "id": "f3baf5fe"
      },
      "outputs": [],
      "source": [
        "# Utilice el siguiente código si desea entrenar el modelo (puede tomar tiempo)\n",
        "\"\"\"\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"lstm.h5\",\n",
        "    save_weights_only=True)\n",
        "model_lstm.fit(dataset.repeat(), epochs=50, callbacks=[checkpoint_callback],\n",
        "               steps_per_epoch=examples_per_epoch//batch_size)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c73bd56b",
      "metadata": {
        "id": "c73bd56b"
      },
      "outputs": [],
      "source": [
        "# Descargamos los pesos de un modelo preentrenado\n",
        "!wget -O lstm.h5 https://github.com/mindlab-unal/mlds5-dataset-unit4-RedesNeuronalesRecurrentes/blob/main/lstm.h5?raw=true\n",
        "model_lstm.load_weights(\"lstm.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "44ac96ff",
      "metadata": {
        "id": "44ac96ff"
      },
      "source": [
        "Veamos el mismo ejemplo que mostramos al comienzo para mostrar la distribución del carácter sucesor pero con el modelo entrenado :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e62e7249",
      "metadata": {
        "id": "e62e7249"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "example_batch_predictions = model_lstm(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "example_pred_seq = example_batch_predictions[0]\n",
        "\n",
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print(f\"Carácter sucesor más probable: {idx2char[np.argmax(example_pred_seq[-1])]}\")\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_pred_seq[-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_pred_seq[-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "12669972",
      "metadata": {
        "id": "12669972"
      },
      "source": [
        "## **2.4. Gated Recurrent Unit GRU**\n",
        "----\n",
        "\n",
        "**Las gated recurrent units (GRUs)** son versiones mejoradas de las redes neuronales recurrentes estándar y son una variante de las LSTM.\n",
        "\n",
        "- Este tipo de unidades retienen las propiedades de las LSTM siendo más simples y rápidas que las LSTM.\n",
        "\n",
        "Las GRUs poseen dos compuertas: **_update gate_** y **_reset gate_**, las cuales deciden qué información debe llegar a la salida y si la información se mantiene o se reescribe :\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1w6nC0Q5vqR67WQrO8tGQGRBCZsTxlGBm' alt =\"Gráfico ilustrativo de la gated recurrent unit\" width =\"70%\"></img></center>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47d39cbf",
      "metadata": {
        "id": "47d39cbf"
      },
      "source": [
        "### **2.4.1 Definición del modelo**\n",
        "----\n",
        "\n",
        "Definimos un modelo equivalente a los dos modelos anteriores, pero con GRUs:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1e9749c",
      "metadata": {
        "id": "d1e9749c"
      },
      "outputs": [],
      "source": [
        "# Dimensión del embedding\n",
        "embedding_dim = 256\n",
        "# Número de unidades en la capa recurrente\n",
        "rnn_units = 1024\n",
        "\n",
        "model_gru = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[batch_size, None]),\n",
        "                                 tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                 tf.keras.layers.GRU(rnn_units,\n",
        "                                                     return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                     stateful=True,\n",
        "                                                     recurrent_initializer='glorot_uniform'),\n",
        "                                 tf.keras.layers.Dense(vocab_size)])\n",
        "model_gru.summary()\n",
        "tf.keras.utils.plot_model(model_gru)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "76c8085e",
      "metadata": {
        "id": "76c8085e"
      },
      "source": [
        "Podemos ver que este modelo tiene un menor número de parámetros en comparación con el modelo de LSTM. Veamos las distribuciones predichas por el modelo (pesos aleatorios) :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c4b38fc",
      "metadata": {
        "id": "0c4b38fc"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model_gru(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "example_pred_seq = example_batch_predictions[0]\n",
        "print(f\"Tamaño de la secuencia de entrada {input_example_batch[0].shape}\")\n",
        "print(f\"Tamaño de la secuencia de salida {example_pred_seq.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0cc8121",
      "metadata": {
        "id": "c0cc8121"
      },
      "source": [
        "- Veamos la distribución del carácter sucesor :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6755b464",
      "metadata": {
        "id": "6755b464"
      },
      "outputs": [],
      "source": [
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_pred_seq[-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_pred_seq[-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9fab385b",
      "metadata": {
        "id": "9fab385b"
      },
      "source": [
        "### **2.4.2. Entrenamiento**\n",
        "----\n",
        "\n",
        "Seguimos el mismo enfoque que usamos con los dos modelos anteriores :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42e75392",
      "metadata": {
        "id": "42e75392"
      },
      "outputs": [],
      "source": [
        "model_gru.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss=loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "690998ba",
      "metadata": {
        "id": "690998ba"
      },
      "source": [
        "- Finalmente, se debe entrenar el modelo.\n",
        "\n",
        "***Nota: En este caso no entrenaremos el modelo, ya que, el entrenamiento puede llegar a tomar mucho tiempo (para entrenarlo utilice el código comentado). En lugar, cargaremos los pesos de un modelo previamente entrenado.***"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0e33586",
      "metadata": {
        "id": "b0e33586"
      },
      "outputs": [],
      "source": [
        "# Utilice el siguiente código si desea entrenar el modelo (puede tomar tiempo)\n",
        "\"\"\"\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=\"gru.weights.h5\",\n",
        "    save_weights_only=True)\n",
        "model_gru.fit(dataset.repeat(), epochs=50, callbacks=[checkpoint_callback],\n",
        "              steps_per_epoch=examples_per_epoch//batch_size)\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8864fc8",
      "metadata": {
        "id": "d8864fc8"
      },
      "outputs": [],
      "source": [
        "# Descargamos los pesos de un modelo preentrenado\n",
        "!wget -O gru.h5 https://github.com/mindlab-unal/mlds5-dataset-unit4-RedesNeuronalesRecurrentes/blob/main/gru.h5?raw=true"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67f61090",
      "metadata": {
        "id": "67f61090"
      },
      "outputs": [],
      "source": [
        "# Cargamos los pesos del modelo\n",
        "# Si entrenó el modelo por su cuenta, cambiar a gru.weights.h5\n",
        "model_gru.load_weights(\"gru.h5\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "13b53701",
      "metadata": {
        "id": "13b53701"
      },
      "source": [
        "Veamos el mismo ejemplo que mostramos al comienzo para mostrar la distribución del carácter sucesor pero con el modelo entrenado :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1c43d008",
      "metadata": {
        "id": "1c43d008"
      },
      "outputs": [],
      "source": [
        "# Obtenemos la predicción para el primer Batch\n",
        "example_batch_predictions = model_gru(input_example_batch)\n",
        "# Seleccionamos la secuencia deseada\n",
        "example_pred_seq = example_batch_predictions[0]\n",
        "\n",
        "print(\"Entrada: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print(f\"Carácter sucesor más probable: {idx2char[np.argmax(example_pred_seq[-1])]}\")\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(211)\n",
        "plt.bar([repr(i) for i in idx2char], example_pred_seq[-1])\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Logits\")\n",
        "plt.subplot(212)\n",
        "plt.bar([repr(i) for i in idx2char], tf.nn.softmax(example_pred_seq[-1]))\n",
        "plt.xlim([0, 88])\n",
        "plt.title(\"Probits\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "111f392f",
      "metadata": {
        "id": "111f392f"
      },
      "source": [
        "## **2.5. Probabilidades de textos**\n",
        "----\n",
        "El modelo que fue entrenado permite calcular la probabilidad de un carácter sucesor dada una secuencia de caracteres previos :\n",
        "\n",
        "- $P(c_i | c_{1},\\dots, c_{i-1})$. Podemos utilizar esta probabilidad condicional para calcular la probabilidad conjunta de una secuencia como se muestra a continuación :\n",
        "\n",
        "$$\n",
        "P(c_1,\\dots,c_n)=P(c_1)\\prod_{i=2}^n P(c_i|c_1,\\dots,c_{i-1})\n",
        "$$\n",
        "\n",
        "Esta función nos permite saber qué tan probable es una secuencia de caracteres según lo que aprendió el modelo. En la práctica, utilizar esta probabilidad conjunta no es muy recomendable (por la inestabilidad numérica que conlleva). Por ello, vamos a utilizar la función de log-verosimilitud del modelo :\n",
        "\n",
        "$$\n",
        "L(c_1,\\dots,c_n)=\\log{P(c_1)}+\\sum_{i=2}^N \\log{P(c_i|c_1,\\dots,c_{i-1})}\n",
        "$$\n",
        "\n",
        "- Veamos como definir esta función:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fe7855e",
      "metadata": {
        "id": "3fe7855e"
      },
      "outputs": [],
      "source": [
        "# Función que calcula la probabilidad de un Texto de acuerdo con la fórmula dada anteriormente:\n",
        "def log_likelihood(model, text):\n",
        "    res = []\n",
        "    for i,char in enumerate(text[:-1]):\n",
        "        # Creamos un Batch de ceros de secuencias de tamaño 1\n",
        "        text_int = np.zeros((64,1))\n",
        "        # Asignamos la codificación del carácter\n",
        "        text_int[0] = char2idx[char]\n",
        "        # Obtenemos la distribución del carácter sucesor\n",
        "        probs = tf.nn.softmax(model.predict(text_int, batch_size=64, verbose=False)[0,-1])\n",
        "        # Calculamos el log-likelihood del siguiente carácter\n",
        "        res.append(probs[char2idx[text[i+1]]].numpy())\n",
        "    return np.log(res).sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "983aa220",
      "metadata": {
        "id": "983aa220"
      },
      "source": [
        "Veamos un ejemplo de qué oraciones son más probables (mayor log-likelihood) dentro del corpus :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867c1ecf",
      "metadata": {
        "id": "867c1ecf"
      },
      "outputs": [],
      "source": [
        "print(log_likelihood(model_lstm, \"la ecuacion del volumen de la esfera\"))\n",
        "print(log_likelihood(model_lstm, \"Dios ama a sus hijos los judios \"))\n",
        "print(log_likelihood(model_lstm, \"la reina de inglaterra viaja en globo\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9d68d88e",
      "metadata": {
        "id": "9d68d88e"
      },
      "source": [
        "Esto permite detectar cual es la probabilidad de que un texto particular haya sido generado por el mismo proceso que genero el texto de entrenamiento.\n",
        "- Por ejemplo, el siguiente texto es aún menos probable pues no contiene espacios que separen las palabras:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29dcd923",
      "metadata": {
        "id": "29dcd923"
      },
      "outputs": [],
      "source": [
        "print(log_likelihood(model_lstm, \"lareinadeinglaterraviajaenglobo\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "49cbe61d",
      "metadata": {
        "id": "49cbe61d"
      },
      "source": [
        "También podemos identificar cuándo una palabra o secuencia no tiene una estructura morfológica correcta:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3fc24fc6",
      "metadata": {
        "id": "3fc24fc6"
      },
      "outputs": [],
      "source": [
        "from itertools import permutations\n",
        "text = list(u'dios')\n",
        "# Creamos todas las posibles permutaciones de la palabra dios\n",
        "perms = [''.join(perm) for perm in permutations(text)]\n",
        "perms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0966377f",
      "metadata": {
        "id": "0966377f"
      },
      "outputs": [],
      "source": [
        "text = list(u'dios')\n",
        "perms = [''.join(perm) for perm in permutations(text)]\n",
        "\n",
        "# Mostramos las secuencias ordenadas por verosimilitud\n",
        "for p, t in sorted([(log_likelihood(model_lstm, text), text) for text in perms], reverse=True):\n",
        "    print(p, t)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6635abcb",
      "metadata": {
        "id": "6635abcb"
      },
      "source": [
        "## **2.6. Generación automática de texto**\n",
        "\n",
        "Para la generación automática de texto definimos la función `generate_text`. Esta función realiza los siguientes pasos en cada iteración:\n",
        "\n",
        "* Comienza codificando un _String_ inicial a enteros.\n",
        "* Realizar una predicción a partir de la codificación de la entrada.\n",
        "* Utiliza distribución categórica de la predicción para muestrear el índice (en el vocabulario) del siguiente carácter. Este será la **entrada de la siguiente iteración**.\n",
        "* Cada que se realiza una predicción, los estados internos (lazos de _Feedback_) se van ajustando hasta que el modelo entiende mejor el contexto y puede realizar mejores predicciones.\n",
        "\n",
        "<center><img src='https://drive.google.com/uc?export=view&id=1QHKE4ZIP4ELkhjHIHizaOBX61ucfJusR' alt =\"Generación de texto con un RNN\" width =\"80%\"></img></center>\n",
        "\n",
        "Típicamente, en la generación de texto se utiliza un parámetro conocido como _Temperature_. Este es un hiperparámetro que se utiliza para controlar la aleatoriedad y la diversidad en la generación de texto. La temperatura afecta la distribución de probabilidad de las palabras o tokens generados por el modelo durante la decodificación.\n",
        "\n",
        "Como se acabó de mencionar, la decodificación se realiza generalmente utilizando un enfoque de muestreo o búsqueda de haz (beam search) sobre la distribución de probabilidad de las letras o palabras generadas por el modelo. La temperatura se aplica a esta distribución de probabilidad para ajustar el equilibrio entre la diversidad y la calidad del texto generado.\n",
        "\n",
        "Cuando la temperatura es igual a 1, se utiliza la distribución original de probabilidad dada por el modelo. A medida que disminuye la temperatura (valores menores a 1), la distribución de probabilidad se vuelve más \"puntiaguda\", lo que significa que las palabras con mayor probabilidad se vuelven aún más probables, mientras que las palabras con menor probabilidad se vuelven menos probables. Esto puede resultar en una generación de texto más conservadora y centrada en el contexto, pero también puede generar repeticiones y menos diversidad en la salida.\n",
        "\n",
        "Por otro lado, cuando la temperatura es mayor que 1, la distribución de probabilidad se vuelve más \"plana\". Esto significa que las palabras menos probables tienen una mayor probabilidad de ser seleccionadas, lo que puede llevar a una generación de texto más creativa y diversa, pero también puede resultar en una salida menos coherente y gramaticalmente incorrecta.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "172119b4",
      "metadata": {
        "id": "172119b4"
      },
      "outputs": [],
      "source": [
        "def generate_text(model, start_string, text_len=1000, temperature=0.5):\n",
        "    # model: Modelo de tipo Many-to-Many\n",
        "    # start_string: Secuencia de caracteres inicial\n",
        "    # text_len: Longitud del texto a generar\n",
        "    # temperature: Parámetro de control de la distribución categórica\n",
        "    #               - Valores de temperature bajos (< 1) lleva a valores más determinísticos\n",
        "    #               - Valores de temperature altos (> 1) aumenta la aleatoriedad.\n",
        "\n",
        "\n",
        "    # Vectorizamos el String inicial\n",
        "    input_eval = [char2idx[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Lista para guardar los resultados\n",
        "    text_generated = []\n",
        "\n",
        "    # Reiniciamos los estados del modelo\n",
        "    for layer in model.layers:\n",
        "        if hasattr(layer, 'reset_states'):\n",
        "            layer.reset_states()\n",
        "    # Iteramos para obtener el número de carácteres deseado\n",
        "    for i in range(text_len):\n",
        "        # Obtenemos las predicciones\n",
        "        predictions = model(input_eval)\n",
        "        # Removemos el eje de los batch\n",
        "        predictions = tf.squeeze(predictions, 0)\n",
        "        # Utilizamos la distribución categórica para obtener el siguiente carácter\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "        # predicted_id es el carácter predicho (este será la entrada en la siguiente iteración)\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        # Agregamos el String correspondiente al id predicho\n",
        "        text_generated.append(idx2char[predicted_id])\n",
        "    return (start_string + ''.join(text_generated))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e668c209",
      "metadata": {
        "id": "e668c209"
      },
      "source": [
        "Ahora, veamos el texto generado con cada uno de los modelos entrenados:\n",
        "\n",
        "* **SimpleRNN**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ddd7c213",
      "metadata": {
        "id": "ddd7c213"
      },
      "outputs": [],
      "source": [
        "# Creamos un modelo con un tamaño de batche 1 con los pesos del modelo entrenado\n",
        "model_srnn = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[1, None]),\n",
        "                                  tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                  tf.keras.layers.SimpleRNN(rnn_units,\n",
        "                                                            return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                            stateful=True,\n",
        "                                                            recurrent_initializer='glorot_uniform'),\n",
        "                                  tf.keras.layers.Dense(vocab_size)])\n",
        "\n",
        "model_srnn.load_weights(\"srnn.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b4c3bc1f",
      "metadata": {
        "id": "b4c3bc1f"
      },
      "outputs": [],
      "source": [
        "model_srnn.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "53a0f36e",
      "metadata": {
        "id": "53a0f36e"
      },
      "source": [
        "- Comenzamos generando texto con un valor de temperatura bajo :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1ca47a9f",
      "metadata": {
        "id": "1ca47a9f"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model_srnn, start_string=u\"Dios dijo: \", temperature=0.1))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a9018280",
      "metadata": {
        "id": "a9018280"
      },
      "source": [
        "- Ahora, veamos el resultado con un valor de temperatura alto :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7cff938",
      "metadata": {
        "id": "f7cff938"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model_srnn, start_string=u\"Dios dijo: \", temperature=2.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9e5a0221",
      "metadata": {
        "id": "9e5a0221"
      },
      "source": [
        "Lo ideal es encontrar un valor de temperatura apropiado, es decir, donde el texto generado no sea tan repetitivo o determinístico ni tan aleatorio e incoherente, veamos un ejemplo con un valor apropiado :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0daece54",
      "metadata": {
        "id": "0daece54"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model_srnn, start_string=u\"Dios dijo: \", temperature=0.5))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2cac965",
      "metadata": {
        "id": "d2cac965"
      },
      "source": [
        "* **LSTM**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b08b5d3d",
      "metadata": {
        "id": "b08b5d3d"
      },
      "outputs": [],
      "source": [
        "# Creamos un modelo con un tamaño de batche 1 con los pesos del modelo entrenado\n",
        "model_lstm = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[1, None]),\n",
        "                                  tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                  tf.keras.layers.LSTM(rnn_units,\n",
        "                                                       return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                       stateful=True,\n",
        "                                                       recurrent_initializer='glorot_uniform'),\n",
        "                                  tf.keras.layers.Dense(vocab_size)])\n",
        "\n",
        "model_lstm.load_weights(\"lstm.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "65570d2b",
      "metadata": {
        "id": "65570d2b"
      },
      "outputs": [],
      "source": [
        "model_lstm.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55040b70",
      "metadata": {
        "id": "55040b70"
      },
      "outputs": [],
      "source": [
        "print(generate_text(model_lstm, start_string=u\"Dios dijo: \"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3af3ca39",
      "metadata": {
        "id": "3af3ca39"
      },
      "source": [
        "* **GRU**:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95876bd5",
      "metadata": {
        "id": "95876bd5"
      },
      "outputs": [],
      "source": [
        "# Creamos un modelo con un tamaño de batche 1 con los pesos del modelo entrenado\n",
        "model_gru = tf.keras.Sequential([tf.keras.layers.InputLayer(batch_input_shape=[1, None]),\n",
        "                                 tf.keras.layers.Embedding(vocab_size, embedding_dim),\n",
        "                                 tf.keras.layers.GRU(rnn_units,\n",
        "                                                     return_sequences=True, # Este argumento hace que el modelo sea Many-to-Many\n",
        "                                                     stateful=True,\n",
        "                                                     recurrent_initializer='glorot_uniform'),\n",
        "                                 tf.keras.layers.Dense(vocab_size)])\n",
        "\n",
        "model_gru.load_weights(\"gru.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9114b58f",
      "metadata": {
        "id": "9114b58f"
      },
      "outputs": [],
      "source": [
        "model_gru.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "084b715c",
      "metadata": {
        "id": "084b715c"
      },
      "outputs": [],
      "source": [
        "print(generate_text(\n",
        "    model_gru, start_string=u\"El sentido de la vida es \", temperature=0.3,\n",
        "    text_len=10000\n",
        "    ).strip())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusión**\n",
        "\n",
        "A lo largo de este notebook, hemos explorado y analizado el funcionamiento, las aplicaciones y las técnicas relacionadas con las redes neuronales recurrentes (RNN). Las RNN son una clase de modelos de aprendizaje profundo diseñados para procesar secuencias temporales o datos secuenciales, como series de tiempo, texto y señales de audio. Las RNN tienen la capacidad de mantener información de estado a lo largo del tiempo, lo que les permite capturar dependencias temporales y contextuales en los datos."
      ],
      "metadata": {
        "id": "2eofRtICPspg"
      },
      "id": "2eofRtICPspg"
    },
    {
      "cell_type": "markdown",
      "id": "48250605",
      "metadata": {
        "id": "48250605"
      },
      "source": [
        "# **Recursos adicionales**\n",
        "----\n",
        "* [*Fundamentals of Deep Learning – Introduction to Recurrent Neural Networks*](https://www.analyticsvidhya.com/blog/2017/12/introduction-to-recurrent-neural-networks/?utm_source=blog&utm_medium=best-papers-iclr-2019)\n",
        "* [*Understanding LSTM Networks*](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
        "* [*Generación de texto con un RNN - Construir el modelo*](https://www.tensorflow.org/tutorials/text/text_generation#build_the_model)\n",
        "\n",
        "* _Origen de los íconos_\n",
        "\n",
        "    - Generación de texto con un RNN. (n.d.). TensorFlow [Imagen] https://www.tensorflow.org/text/tutorials/images/text_generation_sampling.png?hl=es-419"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Créditos**\n",
        "---\n",
        "\n",
        "* **Profesor:** [Fabio Augusto Gonzalez](https://dis.unal.edu.co/~fgonza/)\n",
        "* **Asistentes docentes :**\n",
        "  * [Santiago Toledo Cortés](https://sites.google.com/unal.edu.co/santiagotoledo-cortes/)\n",
        "  * [Juan Sebastián Lara](https://http://juselara.com/)\n",
        "* **Diseño de imágenes:**\n",
        "    - [Mario Andres Rodriguez Triana](https://www.linkedin.com/in/mario-andres-rodriguez-triana-394806145/).\n",
        "* **Coordinador de virtualización:**\n",
        "    - [Edder Hernández Forero](https://www.linkedin.com/in/edder-hernandez-forero-28aa8b207/).\n",
        "\n",
        "**Universidad Nacional de Colombia** - *Facultad de Ingeniería*"
      ],
      "metadata": {
        "id": "cTodMhDf31Ch"
      },
      "id": "cTodMhDf31Ch"
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "-all",
      "main_language": "python",
      "notebook_metadata_filter": "-all"
    },
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}